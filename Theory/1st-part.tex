\documentclass[11pt,a4paper]{amsart}
\usepackage{amssymb,latexsym}
\usepackage{graphicx}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{axiom}{Axiom}
\newtheorem{proposition}{Proposition}
\usepackage{geometry}
\geometry{a4paper,left=2cm,right=2cm,top=1cm,bottom=1cm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\usepackage{ulem} % various underlines
\usepackage{hyperref} % to insert URL 
\usepackage{graphicx} % to insert illustration
\usepackage[mathscr]{eucal} % to express a collection of sets
\usepackage{bm} % bold font in equation environment
\usepackage{color} % color some text
\usepackage{framed} % to add a frame 
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=1pt] (char) {#1};}} % circled numbers
\usepackage{float}%do not auto repositioning
% $\uppercase\expandafter{\romannumeral1}$ Roman numeral
%	\begin{figure}[hbt]
%{\centering \includegraphics[scale=0.78]{ring_algebra_semi}}
%\caption{ring \& algebra \& semi-}\label{F:ring_algebra_semi}
%\end{figure}

\begin{document}
\title{A note for 704 Econometrics: PART 1}
\author{Jialiang Chen} 

\email{jlc99115@hotmail.com}
\urladdr{https://github.com/JLCHEN0115/Jialiang-notebook}
\date{\today}

\begin{abstract}
	This is a note for the first part of the master's econometrics course taught by Jack Porter at University of Wisconsin-Madison, 2022 spring.
\end{abstract}

\maketitle
\tableofcontents \newpage

\section{Some Preparation*}
		\begin{definition}
		We will use the following terms.
		\begin{enumerate}
			\item An \textbf{outcome} is a specific result.
			\item The \textbf{sample space} $S$ is the set of all possible outcomes.
			\item An \textbf{event} is a subset of outcomes in $S$.
			\item The sets $A_{1}, A_{2}, \dots$ are a \textbf{partition} of $S$ if they are mutually disjoint and their union is $S$. That is, for a collection of subsets of $S$, $\{A_{1}, A_{2}, \dots\}$, it is called a partition of $S$ if $A_{i} \cap A_{j} = \emptyset$ for all $i \ne j$ and $\cup_{i=1}^{\infty} A_{i} = S$.
		\end{enumerate}
	\end{definition}
	\begin{definition}\label{random variable}
		A \textbf{random variable} is a real-valued outcome; a function from the sample space $S$ to the real line $\mathbb{R}$.
	\end{definition}
	Sometimes, when necessarily, we denote the realization of random variable $X$ by the lowercase $x$.
	
	\begin{definition}\label{cdf}
		The Cumulative Distribution Function, CDF( sometimes called also \textbf{Probability Distribution Function} of a random variable), denoted by $F_{X}(x)$, is defined to be that function with domain the real line and co domain the interval $[0,1]$ which satisfies
		\[	F_{X}(x) = \mathbb{P}[X \leq x] = \mathbb{P}[\{ \omega \colon X(\omega) \leq x\}]		\]
		for every real number $x$.  
	\end{definition}
	A cumulative distribution function is uniquely defined for each random variable. If it is known, it can be used to find probabilities of events defined in terms of its corresponding random variable. The definition of cdf leads to the definition of probability mass function.
	
		\begin{definition}
		If $X \sim F(x)$ and $F_{X}(x)$ is continuous then $X$ is a \textbf{continuous random variable}. A random variable $X$ is discrete if $F_{X}(x)$ is a step function of $x$.
	\end{definition}
	
	\begin{definition}
			When $F(x)$ is differentiable, its \textbf{density function} is $f(x) = \frac{d}{dx} F(x)$.
	\end{definition}

	We have a similar definition when the random variable $X$ is discrete.
	
	\begin{definition}
	The \textbf{probability mass function} of a random variable is $ \pi(x) = P[X = x]$, the probability that $X$ equals the value $x$. 
	\end{definition}
	
	\begin{definition}\label{same distribution}
		Random variables $X$ and $Y$ are \textbf{equal in distribution (have the same distribution)}, denoted as $X \stackrel{d}{=} Y$, if they have the same distribution functions. That is, 
		\[	F_{X}(t) = F_{Y}(t) \quad \text{for all }t.	\]
	\end{definition}
	
	\begin{definition}\label{independence}
		We have the following concepts of independence for events.
		\begin{enumerate}
			\item Two events $A$ and $B$ are \textbf{independent} if and only if their joint probability equals the product of their probabilities. That is, 
		 \[	\mathbb{P}[A \cap B]  = \mathbb{P}[A]\mathbb{P}[B].	\]
		 	\item A finite set of events $\{A_{i}\}^{n}_{i=1}$ is \textbf{pairwise independent} if every pair of events is independent. That is, for all distinct pairs of indices $(m,k)$, 
		 	\[	\mathbb{P}[A_{m} \cap A_{k}]  = \mathbb{P}[A_{m}]\mathbb{P}[A_{k}].	\]
		 	\item A finite set of events is \textbf{mutually independent} if every event is independent of any intersection of the other events. (also see the theorem below).
		\end{enumerate}
	\end{definition}
	\begin{theorem}
		A set of events $\{A_{1}, \dots, A_{n}\}$ is mutually independent if and only if, for every subset of events, the probability of the intersection of those events is equal to the product of the probabilities of those events.
	\end{theorem}
	
	\begin{definition}\label{d: inde of random variables}
		We have the following concepts of independence for random variables.
		\begin{enumerate}
			\item Two random variables $X$ and $Y$ are  \textbf{independent} if for every $x$ and $y$, the events $\{X \leq x\}$ and $\{Y \leq y\}$ are independent events.
			\item A finite set of $n$ random variables $\{X_{1}, X_{2}, \dots, X_{n}\}$ is  \textbf{mutually independent} if for any sequence of numbers $\{x_{1}, \dots, x_{n}\}$, the events $\{X_{1} \leq x_{1}\}, \dots, \{X_{n} \leq x_{n}\}$ are mutually independent events.
		\end{enumerate}
	\end{definition}

	\begin{definition}
		For a pair of random variables $X, Y$ which are defined on the same sample space, the \textbf{joint cumulative distribution function (CDF)}, denoted as $F_{XY}$ is given by
		\[	F_{X,Y}(x,y) = \mathbb{P}[X \leq x, Y \leq y] = \mathbb{P}[\{x \colon X \leq x\} \cap  \{y \colon Y \leq y\}].	\]
	\end{definition}

	\begin{definition}
		For $N$ random variables $X_{1}, \dots, X_{N}$, the \textbf{joint cumulative distribution function (CDF)}, denoted as $F_{X_{1}, X_{2}, \dots, X_{N}}$ is given by
		\[	F_{X_{1}, X_{2}, \dots, X_{N}}(x_{1}, x_{2}, \dots, x_{N}) = \mathbb{P}[X_{1} \leq x_{1}, \dots, X_{N} \leq x_{N} ]	\]
	\end{definition}

	\begin{definition}
		The \textbf{joint probability mass function} of two discrete random variables $X, Y$ is 
	\[	P_{X,Y}(x,y) = P(X = x ~\text{and}~ Y = y).	\]
	\end{definition}

	\begin{definition}
		The \textbf{joint probability density function} $f_{X,Y}(x,y)$ for two continuous random variables is defined as the derivative of the joint cumulative distribution function
		\[	f_{X,Y}(x,y) = \frac{\partial F_{X,Y}(x,y)}{\partial x \partial y}.	\]
	\end{definition}

	\begin{definition}
		\textbf{Marginal probability mass function} Given a known joint distribution of two discrete random variables, say, $X$ and $Y$, the marginal distribution of $X$ is defined as 
		\[	p_{X}(x_{i}) = \sum_{j} p(x_{i}, y_{j})	\]
		where $p(x_{i}, y_{j})$ is the joint probability mass function.
	\end{definition}

	\begin{definition}
		Given two \textbf{continuous} random variables $X$ and $Y$ whose joint probability density function is known, then the \textbf{marginal probability density function} can be obtained by integrating the joint probability distribution, $f$, over $Y$, and vice versa. That is 
		\[	f_{X}(x) = \int_{c}^{d} f(x,y)dy,	\]
		and
		\[	f_{Y}(y) = \int_{a}^{b} f(x,y)dx	\]
		where $x \in [a,b]$ and $y \in [c,d]$.
	\end{definition}

	\begin{definition}
		For discrete random variables,\textbf{ the conditional probability mass function} of $Y$ given $X=x$ can be written according to its definition as: 
		\[	p_{Y | X} (y | x) := P(Y = y | X = x) = \frac{P(\{X = x\} \cap \{Y = y\})}{P(X = x)}.	\]
	\end{definition}

	\begin{definition}
		For the continuous  random variables, the \textbf{conditional probability function} of $Y$ given the occurrence of the value $x$ of $X$ can be written as 
		\[	f_{Y | X} (y | x) = \frac{f_{X,Y}(x,y)}{f_{X}(x)}	\]
		where $f_{X,Y}(x,y)$ is the joint density of $X$ and $T$, while $f_{X}(x)$ gives the marginal density for $X$ and in this case it is necessarily that $f_{X}(x) > 0$. 
	\end{definition}

	
	\begin{theorem}
		We have the following two theorems for independence of random variables.
		\begin{enumerate}\label{t: inde of random variables}
			\item Two random variables $X$ and $Y$ with cumulative distribution functions $F_{X}(x)$ and $F_{Y}(y)$ are independent if and only if the combined random variable $(X,Y)$ has a joint cumulative distribution function:
			\[	F_{X,Y} (x,y) = F_{X}(x)F_{Y}(y).	\]
			\item Two random variables $X$ and $Y$ with probability density functions $f_{X}(x)$ and $f_{Y}(y)$ and the joint probability density $f_{X,Y}(x,y)$ exist, then they are independent if and only if:
			\[	f_{X,Y} (x,y) = f{X}(x)f_{Y}(y).	\]
			\item A finite set of $n$ random variables $\{X_{1}, \dots, X_{n}\}$ is mutually independent if and only if 
			\[	F_{X_{1}, \dots, X_{n}} (x_{1}, \dots, x_{N}) = F_{X_{1}}(x_{1}) \cdot \dots \cdot F_{X_{n}}(x_{n})	\] for all $x_{1}, \dots, x_{n}$.
		\end{enumerate}
	\end{theorem}
	Notice that it is not necessary here to require that the probability distribution factorizes for all possible $k-$element subsets as in the case for $n$ events. 
	
	\begin{definition}
		A collection of random variables is \textbf{independent and identically distributed} if each random variable has the same probability distribution(See Definition \ref{same distribution}) as the others and all are mutually independent(See Definition \ref{d: inde of random variables} and equivalently ``or even better'', Theorem \ref{t: inde of random variables} ). That is, suppose a collection of random variables $X_{1}, X_{2}, \dots, X_{n}$, each is defined is assume values in $I \subset \mathbb{R}$. We say they that \textbf{i.i.d.} if 
		\[	F_{X_{1}}(x) = F_{X_{k}}(x), \qquad \forall x \in I	\]
		and 
		\[	F_{X_{1}, \dots , X_{n}}(x_{1}, \dots, x_{n}) = F_{X_{1}}(x_{1}) \cdot \dots \cdot F_{X_{n}}(x_{n}), \qquad \forall x_{1}, x_{2}, \dots, x_{n} \in I. 	\]
	\end{definition}
	
	\begin{definition}
		For a random variable $X$ having state space $S$, let $g(x)$ be a function of values of $X$. The \textbf{expected value} or \textbf{mean} of a random variable $g(X)$, denoted by $E g(X)$, is 
		\[ E g(X) = \begin{cases}
		\int_{-\infty}^{+\infty} g(x)f_{X}(x) dx,	\text{if $X$ is continuous}\\
		\sum_{x \in \mathscr{X}} g(x)f_{X}(x) = \sum_{x \in \mathscr{X}} g(x)P[X = x],	\text{if $X$ is discrete}
		\end{cases}
		\]
		where $f_{X}(x)$ is the probability density function of $X$ and $\mathscr{X}$ is the set of possible values of random variable $X$.
	\end{definition}

		\begin{definition}
		If $X$ and $Y$ are \textbf{discrete random variables}, the conditional expectation of $X$ given $Y$ is 
		\[	E(X | Y = y) = \sum_{x} xP(X = x | Y = y) = \sum_{x} x \frac{P(X = x, Y = y)}{P(Y = y)}	\]
		where $P(x = X, y = Y)$ is the joint probability mass function of $X$ and $Y$.
	\end{definition}
	
	\begin{definition}
		Let $X$ and $Y$ be continuous random variables with joint density $f_{X,Y}(x,y)$, $Y$'s marginal density function $f_{Y}(y)$, and conditional density $f_{X|Y}(x|y) = \frac{f_{X,Y}(x|y)}{f_{Y}(y)}$ if $X$ given the event $Y = y$. Then conditional expectation of $X$ given $Y = y$ is 
		\[	E(X | Y = y) = \int_{-\infty}^{+\infty} x f_{X|Y}(x|y)dx = \frac{1}{f_{Y}(y)} \int_{-\infty}^{\infty} x f_{X,Y}(x,y)dx.	\]
	\end{definition}
	
	The function $m(x) = E [Y | X = x]$ is not random. Rather, it is a feature of the joint distribution. Sometimes, however, it is useful to treat the conditional expectation as a random variable. To do so, we evaluate the function $m(x)$ at the random variable $X$ . This is $m(X) = E[Y|X]$.

\section{OLS}
\subsection{OLS Assumptions}
	For a linear equation system 
	\[	y_{i} = x^{'}_{i} \beta + \epsilon_{i} \qquad i = 1,2, \dots, n	\]
	where $y_{i}$ and $\epsilon_{i}$ are scalars, $x_{j}$ and $\beta$ are $k \times 1$ (column) vectors. Imagining there are $n$ agents indexed by $i$, each individual will have an associated variable $y_{i}$, sometimes called the ``outcome'', each individual will have a set of independent variables $x_{i}$ (might be characteristics of the individual). There would be a set of coefficients on the independent variables $\beta$, which will be our main interest. Most of the times we will assume $x_{i}$ contains an item $1$. We give observations on $y_{i}$'s and $x_{i}$'s. Linear model and OLS will be used for most empirical works even in top journals. This linear equation by itself does not have many content. We do not know $\beta$ and $\epsilon_{i}$'s. After all, there are too many unknowns in one equation. In order to have the linear equation tells us something, we need to make some assumptions. \par 
	There are different versions of assumptions, and we have the following four assumptions:
	\begin{enumerate}
		\item \textbf{(OLS $0$) $(y_{i}, x_{i})_{i = 1,2, \dots,n}$ is an i.i.d. sequence (and sufficient existence of moments);}\par 
		This requires us to get a random sample of $y_{i}$'s and $x_{i}$'s. Each individual is independently draw from the same distribution. This is not a very realistic situation.  But we will think about more complicated structures (e.g., time series, clustering) than that in the second part of the course. We will weaken this assumption to allow for other scenarios. But you want to think about the structure of your sampling data and as you change that, you will need to make different assumptions about the existence of moments, depending on what kind of results or properties you are looking for. So we are leaving that a little bit flexible at the moment.
		\item \textbf{(OLS $1$) $E(x_{i}x_{i}^{'})$ is finite and nonsingular;}\par 
		$x_{i}x_{i}^{'}$ is a $k$-by-$k$ matrix. The assumption here is those expectations exist and the matrix is nonsingular(invertible). We will touch on where does it come up soon. We can ask a question of whether it is restrictive or not. One thing it rules out is that you cannot have ``perfect multicollinearity''. Perfect multicollinearity happens when some of the variables are perfect linear combinations of each other. If we have the variables ``wage'' and ``$2$ wage'', this assumption will rules out this goofy case. But it also rules out something more important. For example, some machine learning cases or high dimensional covariance. So this assumption implies that $k$, the dimension of $x_{i}$'s, is smaller or equal to the sample size $n$. Most of the time this will not be a problem, but nowadays, sometimes you will get a lot of information about the individuals.  This will require some other approach, which will be discussed in the later part of the course. 
		\item \textbf{Orthogonality conditions(OLS $2$) $E(\epsilon_{i} | x_{i}) = 0$;}\par 
		This is kind of the key assumption. It tells us about the behavior of $\epsilon_{i}$'s in a way that pins down something about this equation. It tells us how we can interpret data in a statistical sense. That is a big assumption with two different versions. 
		\item \textbf{(OLS $2'$) $E(x_{i}\epsilon_{i}) = 0$;}
		\item \textbf{(OLS $3$) $E(\epsilon_{i}^{2}x_{i}x^{'}_{i})$ is finite and nonzero.}\par
		This assumption is just finite and corresponds to something we need to get the variances of the OLS estimate. This is not a very strong assumption and we will come back to this.
	\end{enumerate}

	These are the key assumptions for the linear model. In some sense, what are not clear here is ``What do we mean by the linear model'', ``What do we mean by $\beta$'' and ``Why we are interested in $\beta$''. There can be a lot of reasons why $\beta$ is of our interest. People will sometimes talk about $\beta$ tells you about the linear relationship of $y$ and $x$'s. Sometimes people tells you about the effect of $x$ on $y$. Those are kind of different objects usually. People use this model to estimate different things. Sometimes their estimating objectives have causal interpretation, as Harold will spend a chuck of time talking about. Sometimes they have descriptive contents, predictive information. But the thing I will say is the equation here does not change, so you cannot tell, just from this equation, what are the underlining objective that you want to use or how you want to interpret these coefficients. You can say something about the statistical content from just these assumption.  For instance, these two (OLS $2$ and OLS $2'$) assumptions, which are really critical to interpret data in a statistical sense. The thing is to notice that OLS $2$ implies OLS $2'$, but not vice versa. Let us take a closer look of it. Before that, we need a lemma that we will use a lot.
	
	\begin{theorem}
		\textbf{The Law of Iterated Expectations}.  
		\[	E(Z) = E[E(Z | W)].	\]
		\[	E(Z | W) = E[E(Z | U,W) | W].	\] 
	\end{theorem}
	Think of $W$ as measuring heights and $Z$ is some grouping or characteristic. If we want to figure out the average height in the room, there are two ways. One is just take everybody's height, and average them. Alternatively, say $W$ is the city born. $E(Z | W)$ is the average height of a single city. If I know the proportion of people in each city, then we can back to the average overall. And that is the right side of the equation. By $E[E(Z | W)]$, we take the average of the average heights over the distributions of cities by weighting each city the proportion of its people. It underlines so many of things and the very crucial concept which we will use over and over.
	
	\begin{lemma}
		For two random variables, we have $E(XY|X)=XE(Y |X)$.
	\end{lemma}

	\begin{theorem}
		\textbf{OLS $2$ implies $2'$} If OLS $2$ holds, then OLS $2'$ also holds.
	\end{theorem}
	\begin{proof}
		\begin{align*}
			E(x_{i}\epsilon_{i}) &= E(E(x_{i}\epsilon_{i} | x_{i})) &&\text{(By law of iterated expectation )} \\
			&= E(x_{i}E(\epsilon_{i} | x_{i})) &&\text{(pull $x_{i}$ out of expectation)} \\
			&= E(0) &&\text{(by OLS $2$)} \\
			&= 0.
		\end{align*}
	\end{proof}
	Why does it matter?\par 
	
	\begin{lemma}
		\textbf{Linearity of Expectation} The expected value operator (or expectation operator) $E[\cdot]$ is linear in the sense that, for any random variables $X$ and $Y$, and constants $a, b$, 
		\[	E[aX + bY] = aE[X] + bE[Y].	\]
	\end{lemma}

	\begin{theorem}\label{ols2 beta}
		OLS $2$ holds if and only if $E(y_{i} | x_{i}) = 0$.
	\end{theorem}
	\begin{proof}
		Given that OLS $(2)$: $E(\epsilon_{i} | x_{i}) = 0$ holds. Then \begin{align*}
			E(y_{i} | x_{i}) &= E(x_{i}' \beta + \epsilon_{i} | x_{i}) &&\text{(by replacing $y_{i} = x_{i}' \beta + \epsilon_{i}$)} \\
			&= E(x_{i}' \beta | x_{i}) + E(\epsilon_{i} | x_{i})  &&\text{(Linearity of expectation operator)} \\
			&= E(x_{i}' \beta | x_{i}) &&\text{($E(\epsilon_{i} | x_{i})  = 0$ under OLS $2$)} \\
			&= x_{i}' \beta  &&\text{(lemma above)}. 
		\end{align*}
	\end{proof}
	 That means the conditional expectation of $y_{i}$ given $x_{i}$ is linear. For example, let $x_{i}$ be the location of a city (longitude and latitude). Why would average heights be linear in the graphical location of earth? Do not know. Generally, you would not necessarily expect that although there are cases where it is true.  The key thing is OLS $2$ is making this assumption and it is not an assumption that holds in general. There are some counters to it. What it is saying is that $\beta$ is representing something important, which is the slope of this conditional expectation function $E(y_{i} | x_{i})$, the average of $y_{i}$ for every value of $x_{i}$. This conditional expectation lies on some hyperplane, whatever that $k$ dimensional space is. So this is an assumption that has a real content to it. You will see conditional expectation plays an important role in Harold's part, e.g., causality.\par 
	 The other assumption, OLS $2'$. is weaker. What does $E(x_{i} \epsilon_{i} ) = 0$ tells us? Notice that we have\par 
	 $(\textbf{OLS $2'$})  \qquad E(x_{i} \epsilon_{i}) = 0$ if and only if 
	 \begin{align*}
	 	 0 &= E(x_{i} (y_{i}-x_{i}' \beta))	&&\text{(by definition)} \\
	 	&= E(x_{i}y_{i}-x_{i} x_{i}' \beta) 	&&\text{(by distributivity)} \\
	 	&= E(x_{i}y_{i})-  E(x_{i} x_{i}')\beta 	&&\text{(by linearity of expectation )}
	 \end{align*}
 	Which is equivalent to 
 	\[  E(x_{i} x_{i}')\beta =	E(x_{i}y_{i}). 	\]
 	The dimension here: $x_{i} x_{i}'$ is $k$-by-$k$, and $\beta$ is $k$-by-$1$, and $x_{i}$ is $k$-by-$1$,and  $y_{i}$ is $1$-by-$1$. Notice that as a definition, the expectation of a matrix is just the matrix of expectations.\par 
 	Here, we get $k$ equations with $k$ unknowns by treating $\beta$ as a vector of unknowns. When we can solve this (a unique $\beta$) are exactly when we can invert the matrix $E(x_{i} x_{i}')$. If it is invertible, we can get
 	\begin{equation}\label{ols2' beta}
 	\beta =	E(x_{i} x_{i}') ^{-1} E(x_{i}y_{i}).
 	\end{equation}
  	\textbf{Notice that the condition $E(x_{i} x_{i}')$ is invertible is just OLS $1$.}\par 
 	This is saying we get an explicit formula for $\beta$ in terms of the underlying joint distribution between $x_{i}$'s and $y_{i}$'s. (That is what these expectation functions are, they are characterizations of the joint distribution.)  Here, what is interesting is this OLS $2'$ assumption, it really \textbf{defines} $\beta$ in terms of the joint distribution of $X$ and $Y$. You can think about it as an assumption, but it is really a definition! It really defines $\beta$. It tells what $\beta$ is.(Although you have to assume the expectation $E(x_{i} x_{i}')$ exists and the invertibility, i.e., OLS $1$, which is not a big deal).\par 
 	So have we impose assumptions about the joint distribution of $X$ and $Y$, other than the expectation existence and invertibility thing? NO! Take any joint distribution where the expectation exists and we have the invertibility of the square matrix $E(x_{i}x_{i}')$ and then $\beta$ will be well-defined out of it.\par 
 	How about OLS $2$? As we just discussed, OLS $2$ is a real assumption, and it may not hold! There are some joint distributions of $Y$ and $X$ where the conditional expectations of $Y$ given $X$, $E(Y | X)$ is not going to be linear! It is a real stringent assumption. It is an restriction on the underlying joint distribution of $Y$ and $X$. However, OLS $2'$ is much less stringent. Although you have to assume the existence and invertibility. But really, almost any joint distribution will have a $\beta$ associated with this OLS $2'$. What is that $\beta$? If OLS $2$ holds, then the $\beta$ defined by OLS $2'$ is exactly the same $\beta$ implied by OLS $2$ in Theorem (\ref{ols2 beta}), which is the slope of the linear expectation function. Since if OLS $2$ holds, then OLS $2'$ holds, and Theorem (\ref{ols2 beta}) and equation (\ref{ols2' beta}) holds. If OLS $2$ does not hold, but OLS $2'$ does. That would mean, the conditional expectation $E(y_{i} | x_{i})$ is not linear as in Theorem (\ref{ols2 beta}).\par 
 	Let us think about this classic labor case. 
 	We have $x$ axis the \textbf{education (years)}, the \textbf{$log$ wages} on the $y$ axis. See Figure (\ref{F:conditional expectation function})
 	Suppose for each of these education level in a certain population, we can have its average log wage. So the dots are telling about the expected log wage given education. If they are on a line, it is the case when OLS $2$ holds, and the $\beta$ represents the slope of the line. Do they have to be on the line? Of course not! For example, case $(b)$, $E(y_{i} | x_{i})$ is clearly not linear. This would be a violation of OLS $2$. In other words, OLS $2$ will rule out these cases. 
 		\begin{figure}[hbt]
 		{\centering \includegraphics[scale=0.28]{conditional expectation}}
 		\caption{Linear and nonlinear conditional expectation function}\label{F:conditional expectation function}
 		\end{figure}
 	
 	But OLS $2'$ would not.  OLS $2'$ would say, ah, it's fine if this conditional expectation function is not linear. Now, we want to see what the formula (\ref{ols2' beta}) sort of ``corresponds to''. The answer is $\beta$ corresponds to the line that is the closest to the curve. And $\beta$ is the slope of that line. What do we mean by ``the closet''? It is we want the difference $E(y_{i}|x_{i}) - x_{i}'b$, where $b$ is a $k$-by-$1$ vector.
 	We want to make the difference small. How do we do it? Well this is a random difference, it depends on $x$ and $x$ is a random variable. It's hard to make random variable small. So if we want to minimize this sort of difference, we are going to turn it into something that is nonrandom, so we take the square of the difference and then take the expectation.\par 
	  It is the following minimization problem:
	 \[	\min_{b} E[E(y_{i}|x_{i}) - x_{i}' b]^{2}. 	\]
	This is called the \textbf{mean squared error}. It is intended to make the difference between that curve we just drew and some line as small as we possibly can by making the square of errors have very small expectation. So we are trying to get this as possible as to $0$. \par
	\textbf{What is the solution to this minimization problem?}\par 
	\textbf{The solution is, what we called $\beta$ in Equation (\ref{ols2' beta}) before.}
	That is, for the problem
	\begin{equation}\label{mse}
		\min_{b} E[E(y_{i}|x_{i}) - x_{i}' b]^{2}.
	\end{equation}	
	the solution is 
	\[	b^{*} = \beta = E(x_{i} x_{i}') ^{-1} E(x_{i}y_{i}).	\]
	That is what we are saying. We know OLS $2'$, then what is $\beta$? $\beta$ is the slope of the line which is closet to the conditional expectation curve. So even if conditional expectation curve is not linear, that is fine.\par 
	There is another sense, in which you can try to think about. Defining another thing you can do is instead taking the square of the difference of the curve $y_{i}$ and $x_{i}' b$. This is a strategy sitting behind what you know as \textbf{ordinary least square}. 
	\begin{equation}\label{ols}
	\min_{b} E[(y_{i}-x_{i}' b)^{2}].
	\end{equation}
	It is trying to say why $x_{i}' b$ is a good fit in the sense that the error in the linear equation $\epsilon$ would be as small as possible. What is the solution of this minimization problem? It is the same solution as the minimization of mean squared error (and therefore, as the $\beta$ defined by OLS $2'$).\par 
	In conclusion, what OLS $2'$ is doing is telling us the explicit definition of $\beta$ in Equation (\ref{ols2' beta}), and saying that by the way, that definition of $\beta$ is the same thing as the solution to the minimization problem of mean square error (\ref{mse}) and the minimization  problem of ordinary least square (\ref{ols}). So it gives you a lot of nice interpretations for $\beta$. You can see why that is potentially more useful in a sort of more general way. When you coming along, you do not necessarily know which of these is true. Sometimes people have an argument or story why one or the other is true. What I am saying is OLS $2'$ is generally true. It is really pretty closed to a definition rather than a assumption. It is not actually ruling out a lot of joint distributions between $X$ and $Y$. It only rules out the ones where the expectations do not exists or the expectation is not invertible. Other than that, any joint distribution works. It turns out that, the interpretation is sort of easiest for $\beta$ when you have OLS $2$. It has a nice slope corresponding with the conditional expectation function and sometimes we will have nice causal interpretations but on the other hand, it might be a restrictive assumption. That is not surprising. The more assumptions of restrictions you place, the nicer the interpretations somehow. You get some benefits from that. The downside of that is that it may not hold. And if you made that assumption and it is actually not true, then you are probably actually sitting in front of the left line in Figure (\ref{F:conditional expectation function}) and you might have to think about any implications for that, which will lead to mistakes. \par 
	One other thing I want to address out is, OLS $2$ is essentially the same thing as saying the conditional expectation function $E(y_{i} | x_{i})$ is linear. If this thing is linear, then case $(a)$ in Figure (\ref{F:conditional expectation function}) is true, the dots sit on the line. If I want to learn about the slope, I can sample people with education years $16$ and $17$, then I will figure out the slope. Any two groups will be sufficient. I can put different weights on different education groups. One person goes out and collects data and get a lot of people with high education. Another person goes out and gets data from a group of people with low education. They both should be giving the estimating the same thing under OLS $2$. What happens if this is not a line? If I only have data from $16$ to $17$, I will use the data to make a line. If I only have data from $18$ and $20$, I will use the data to make a different line. Now, it is going to matter what weights I put on these $x$'s. How they are sample makes a difference on the result I get. That is another thing that we have to worry about. It also provide some evidence for OLS 2 fail that will help you when you do actual research. 
	
\subsection{Consistency and Asymptotic Normality}
	We will first do a quick review of some convergence ideas and a discussion on how we will be using these convergence properties. Then we will go back to our OLS story and do a little bit about robust standard errors. Harold will do more on robust standard error.\par 
	In probability theory, there exist several different notions of convergence of random variables. The convergence of sequences of random variables to some limit random variable is an important concept in probability theory, and its applications to statistics and stochastic processes. The same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behavior that is essentially unchanging when items far enough into the sequence are studied. The different possible notions of convergence relate to how such a behavior can be characterized: two readily understood behaviors are that the sequence eventually takes a constant value, and that values in the sequence continue to change but can be described by an unchanging probability distribution.\par 
	Convergence in distribution is in some sense the weakest type of convergence. All it says is that the CDF of $X_{n}$'s converges to the $CDF$ of $X$ as $n$ goes to infinity. It does not require any dependence between the $Xn$'s and $X$.\par
	Here is a formal definition of convergence in distribution.
	\begin{definition}
		Let $X_{1}, X_{2}, X_{3}, \dots$ be a sequence of random variables. The sequence $X_{1}, X_{2}, \dots$ \textbf{converges in distribution} to a random variable $Y$ means 
		\[	\lim_{n \to \infty} F_{X_{n}}(x) = F_{Y}(x)	\]
		for all continuous points $x$ of $F_{Y}(\cdot)$, where $F_{X_{n}}$ is the cumulative distribution function (c.d.f.) for $X_{n}$ and $F_{Y}$ is the c.d.f. for $Y$. If $X_{1}, X_{2}, \dots$ converges in distribution to $Y$, then we use  the notation 
		\[	X_{n} \xrightarrow{d} Y. \]
	\end{definition}
	\begin{framed}
		\underline{\textbf{Example}}  Let $X_{2}, X_{3}, X_{4}, \dots$ be a sequence of random variable such that 
		\[	F_{X_{n}}(x) = \begin{cases}
				1 - (1- \frac{1}{n})^{nx}, &x > 0; \\
				0 &\text{otherwise}.
		\end{cases}	\]
		\textbf{Show that $X_{n}$ converges in distribution to $Exponential(1)$.}\newline
		For reference, the cumulative distribution function of an expenential distribution is given by \[F_{X}(x, \lambda) = \begin{cases}
		1 - e^{-\lambda x}, &x > 0; \\
		0 &\text{otherwise}.
		\end{cases} \]
	\end{framed}
	The basic idea behind this type of convergence is that the probability of an “unusual” outcome becomes smaller and smaller as the sequence progresses.\par 
	The concept of convergence in probability is used very often in statistics. For example, an estimator is called consistent if it converges in probability to the quantity being estimated. Convergence in probability is also the type of convergence established by the weak law of large numbers.
	\begin{definition}
		Let $X_{1}, X_{2}, X_{3}, \dots$ be a sequence of random variables. The sequence $X_{1}, X_{2}, \dots$ \textbf{converges in probability} to a constant $\alpha$ means for all $\epsilon > 0$, 
		\[	\lim_{n \to \infty} Pr(|X_{n} - \alpha| > \epsilon) = 0.	\]
		We denote converges in probability as $X_{n} \xrightarrow{p} \alpha$.
	\end{definition}

	\begin{definition}
		A probability space or a probability triple $(\Omega ,\mathcal {F},P)$ where $\Omega$ is the sample space, $\mathcal {F}$ is a $\sigma-$algebra on $\Omega$, and $P$ is a probability function. It is a mathematical construct that provides a formal model of a random process or ``experiment''. 
	\end{definition}
	\begin{definition}
		When the sequence $X_{n}$and a random variable $Y$ live on the same probability space, then the notation $X_{n} \xrightarrow{p} Y$ simply means $X_{n} - Y \xrightarrow{p} 0$.
	\end{definition}
	We will be started by talking about the weak law of large numbers. It is actually a whole bunch of results with essentially the same conclusion and different assumptions.  
	\begin{theorem}
		\textbf{Weak Law of Large Numbers} Under some conditions (on dependence, heterogeneity and moments), 
		\[	\frac{1}{n} \sum_{i=1}^{n} Z_{i} - E(Z_{i}) \xrightarrow{p} 0.	\]
	\end{theorem}
	We will think about what will happen when we collect together (observe) a bunch of random variables, $n$ of them, and then take their average. This is the first object, which is our key object. When these $n$ random variables are identically distributed, they all have the same mean. Which means that if I get the same distribution function for each of them, then the expectation of the average will be the expectation of anyone of the $z$'s. That means the different between the average (first term) and the mean of $Z_{i}$ will converge in probability to $0$. Put it in another way, the average will converge in probability to the mean. Average does a good job of estimating expectation. Keep in mind that the expectations (means) are sometimes refer to population objects. They are the objects that depend on distribution of $Z$. The first term in WLLN is a sample object, which represent the random variables that we are getting observations on. What we are saying is that there is a close connection between the two items in WLLN: the difference between the two converges in probability to $0$. Therefore, the left side of the expression (which is a huge random variable) will be with in a little $\epsilon$ neighborhood of $0$ with very high probability nearly $1$. As the sample size grows, we will get closer and closer in that neighborhood of $0$ and we will have probability approaching to $1$ of containing the whole distribution. This neighborhood of $0$ will contain the distribution of the huge random variable with probability approaching to $1$ as $n$ (the sample size, the number of random variables we observe)  gets larger.
	
	\begin{figure}[hbt]
	{\centering \includegraphics[scale=0.12]{WLLN}}
	\caption{Weak Law of Large Numbers}\label{F:WLLN}
	\end{figure}

	Just to have something in you mind, let us think about three different distributions in Figure (\ref{F:WLLN}). Imagine that $Z$'s comes from a normal (density) distribution in case$(1)$. What is gonna happen is as I get lots of draws from this distribution, and I average them, then this average is going to get really close to $0$ with very high probability. The average $\frac{1}{n} \sum_{i=1}^{n} Z_{i}$ will have a density looks like the second row of case $(1)$. Almost all the objects mass around $0$. Similarly if we get some draws from a uniform distribution, the mean is $1/2$. The averages will pile up around $\frac{1}{2}$ like in case $(2)$. If I get draws from a discrete distribution with only two values with expectation $1$ as in case $(3)$, the averages will be right around the expectation value. Here is a formal presentation of a kind of weak law of large numbers. 
	\begin{theorem}
		\textbf{Weak Law of Large Numbers (WLLN):} Let $Z_{1}, Z_{2}, Z_{3}, \ldots$ be an i.i.d. (independent and identically distributed) sequence of random variables with finite mean $E\left(Z_{i}\right)$. Then,
	\[		\frac{1}{n} \sum_{i=1}^{n} Z_{i} \stackrel{p}{\longrightarrow} E\left(Z_{i}\right).\]
	\end{theorem}
	The central limit theorem is very similar.
	\begin{theorem}
		\textbf{Central Limit Theorem} Under some conditions (on dependence, heterogeneity and moments), we will be able to conclude that
		\[	\sqrt{n} \frac{1}{n} \sum_{i=1}^{n} |Z_{i} - E(Z_{i })| \stackrel{d}{\longrightarrow} N(0,V).	\]
	\end{theorem}
	This theorem is a statement about the behavior of an average. Notice that $ \frac{1}{n} \sum_{i=1}^{n} |Z_{i} - E(Z_{I})|$ is an average, the sum of $Z_{i}$ minus the expectation $E(Z_{i})$. What is this really about is $|Z_{i} - E(Z_{I})|$ is gonna take each random variable and subtract its  mean, basically shifting each random variable so we have the new random variable $|Z_{i} - E(Z_{I})|$ which will have mean $0$. We know what is going to happen to this average $ \frac{1}{n} \sum_{i=1}^{n} |Z_{i} - E(Z_{I})|$. That is what WLLN is about. WLLN tells us that this object converges in probability to $0$. It is like a stochastic version of getting close to $0$.\par 
	But the central limit theorem says if I look at that sequence of random variables that is getting closer and closer to zero in probability, and multiply it by a new sequence $\sqrt{n}$ (which goes to infinity as $n$ grows). We will then have two things that kind of balance each other. Neither one wins!\par 
	What if we did not start with normally distributed $Z$'s (even discrete)? It still goes to normal distribution.\par 
	Therefore, the weak of large numbers and the central limit theorem tell us about the behavior of averages. WLLN tells us that the average is close to the expectation, and we are going to use that intuition all the time. There is a lot of places where if we knew the expectation then we would be in business. The expectation is what we are going to be interested in, but usually we will have no access to the actual distribution of the random variables, so we do not have the actual expectation. Instead, we are given a sample from data and we will use that data to learn about the distributions of the random variables. We will use that data to learn about the means of the random variables through the averages often. So we know that averages are close to means.\par 
	And then the central limit theorem will say: oh, I know something even more and better. If I go over here, and I look at these crazy distributions, then pile out all of their mass at the mean. If I choose to shift this over and let it be centered around $0$, it then looks like a spike. If we spread out the spike a little bit, stretching out the (density) distribution by $\sqrt{n}$. It is no longer a spike, it turns out to be a nice, bell-shape curve.  WLLN tells us there is a spike, and CLT tells us we can make it a normal distribution. Following is a formal presentation of the central limit theorem.
	\begin{theorem}
		\textbf{Central Limit Theorem $(C L T)$ :} Let $Z_{1}, Z_{2}, Z_{3}, \ldots$ be an i.i.d. sequence of random variables with $0<\operatorname{Var}\left(Z_{i}\right)<\infty$. Then,
		\[
		\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \frac{Z_{i}-E\left(Z_{i}\right)}{\sqrt{\operatorname{Var}\left(Z_{i}\right)}} \stackrel{\mathrm{d}}{\longrightarrow} N(0,1) .
		\]
		Notice that this is a scalar case, and therefore we can also write it as 
			\[
		\frac{1}{\sqrt{n}} \sum_{i=1}^{n} [Z_{i}-E\left(Z_{i}\right)] \stackrel{\mathrm{d}}{\longrightarrow} N(0,\operatorname{Var}\left(Z_{i}\right)) .
		\]
		$V$ in the asymptotic normal distribution is $\operatorname{Var}\left(Z_{i}\right)$ only in the i.i.d. case. And to move the coefficient before the asymptotic normal distribution inside, we have to square it for the new variance in the scalar case. In the matrix case, we need to pre-multiply and post-multiply the transpose. See lemma (\ref{the matrix world constant}). \par 
		If the data is not i.i.d., then we must have $V = Var(	\frac{1}{\sqrt{n}} \sum_{i=1}^{n} [Z_{i}-E\left(Z_{i}\right)])$. See the Robust standard error section where we did this for the $n=2$ case. 
	\end{theorem}
	There are some other results that go hand and hand with these two that allows us to make use of CLM and WLLN for a lot of different problems or setups. One is the Slutsky theorem.
	\begin{theorem}\label{slutsky}
		\textbf{Slutsky Theorem.} If we have a sequence of random variables $W_{n}$ that converges in distribution to $W$; and a sequence of random variables $U_{n}$ that converges in probability to $\alpha$ where $\alpha$ is a constant, then 
		\[	W_{n} + U_{n} \stackrel{d}{\longrightarrow} w + \alpha;	\]
		\[	W_{n}U_{n} \stackrel{d}{\longrightarrow} w\alpha;	\]
		and 
		\[	W_{n}/U_{m} \stackrel{d}{\longrightarrow} w/\alpha \qquad \text{if}~ \alpha \ne 0.	\]
	\end{theorem}
	The conclusions are convergence in distribution rather than convergence in probability. Recall that convergence is distribution is weaker than convergence in probability but not vice versa. 
	\begin{theorem}
		\textbf{Continuous Mapping Theorem.} If $g(\cdot)$ is everywhere continuous, then 
		\[	U_{n} \xrightarrow{p} U \Rightarrow g(U_{n}) \xrightarrow{p} g(U)	\]
		and
		\[	W_{n} \xrightarrow{d} W \Rightarrow g(W_{n}) \xrightarrow{d} g(W).	\]
	\end{theorem}
	If we got a convergent in probability sequence, then we can take a nice continuous function of those random variables, and that would also converges in probability. Similarly for the case when we get a convergent in distribution sequence. This result also applies to random vectors or matrices, whatever you want to think about it, which is actually rather nice.\par 
	Another point to point out is that both the central limit theorem and the weak law of large numbers, they refer to a collection of results with essentially the same conclusion. That will happen under various conditions and there will be trade-offs in these different assumptions. The version of the setup for the random variables in the linear model assumed that $(y_{i}, x_{i})_{i}$ are i.i.d.. So we can make assumptions about the dependence, but we can also allow cases where there are dependent random variables. Perhaps we will allow for the cases where they do not have the same distribution. So we can allow for some heterogeneity. And if we are going to allow for dependence and heterogeneity, then typically we are going to make stronger assumptions about what are called the moments. The moments refer to usually the expectation of these random variables to some power. There will be some restrictions get imposed through the moments to allow for more flexibility on the dependence and heterogeneity over the sequence of random variables. We will talk a little bit about this as attached to the whole idea of getting standard errors that are robust to different situations. We will come back to this.\par 
	Now let us go back to where we ended up last time. We have a linear model
	\[	y_{i} = x_{i}' \beta + \epsilon_{i}, \qquad i = 1,2,3, \dots, n	\]
	and a bunch of assumptions:
	\begin{itemize}
		\item [OLS $0$] $(x_{i}, y_{i})_{i}$ are i.i.d. (some sort of moments existing);
		\item [OLS $1$] $E(x_{i}x_{i}')$ is finite and nonsingular;
		\item [OLS $2$] $E(\epsilon_{i} | x_{i}) = 0$;
		\item [OLS $2'$] $E(x_{i}\epsilon_{i}) = 0$;
		\item [OLS $3$] $E(\epsilon_{i}^{2}x_{i}x_{i}')$ is finite.
	\end{itemize}\par 
	I sometimes left [OLS $0$] fuzzy as some assumptions about the dependence and the moments existing. The reason is you might need different assumptions about the moment depending on whether you want to talk about the weak law of large numbers or the central limit theorem. Let us just assume that you got some moments existing enough to get the weak law of large numbers and the central limit theorem working.\par 
	Last time we found that:\par 
	 1) OLS $2$ is stronger than OLS $2'$.\par 
	 2) OLS $2'$ defines $\beta$. That is, with $E(x_{i}\epsilon_{i}) = E(x_{i}(y_{i}-x_{i}' \beta))$ solved out we get $\beta = E(x_{i}x_{i}')^{-1}E(x_{i}y_{i})$. It is also the optimal solution of the problem of ordinary least square 
	 \[	\min_{b} E[(y_{i}-x_{i}'b)^{2}].	\]
	 That got us to think about this object $\beta$, which is the coefficients on $x$ in this linear equation. This is our ultimate interest. These are the parameters that we do not know and you can see here that it is a function of the distribution of $x_{i}$ and $y_{i}$ as a very explicit formula. If we know the joint distribution of $X$ and $Y$, we could compute these expectations and then figure out what $\beta$ is. That would not be fun but we get things done. But in general, we do not know the joint distribution of $X$ and $Y$! Instead, we are going to get information about the joint distribution of $X$ and $Y$ from data. We will use data to learn the underlying distribution that $X$ and $Y$ comes from. So if you want to use your data and try to produce an estimate or guess of $\beta$, I think there are two natural things that comes from this formulation of $\beta$ under OLS $2'$. We do not know the expectations but what we said is that we will have sample averages. We can compute sample averages based on the data we receive, and those sample averages by the weak law of large numbers do a good job of estimating the corresponding expectations.\par 
	 Therefore, one thing we could do if we want to, is everywhere we see an expectation, let us just replace it by sample average. So we have
	 \begin{equation}\label{beta hat}
	 \hat{\beta} =  (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}(\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}),
	 \end{equation}
	 which is just two replacement of expectations in $\beta = E(x_{i}x_{i}')^{-1}E(x_{i}y_{i})$ with the corresponding averages. This is feasible and I can compute with. As long as those averages are close to the expectations, perhaps $\hat{\beta}$ should be close to $\beta$. We call $\hat{\beta}$ \textbf{the Ordinary least square estimator.}\par 
	 There is also something different we can do. Instead of solving 
	 \[	\min_{b} E[(y_{i}-x_{i}'b)^{2}]	\]
	 as in (\ref{ols}), we will just solving
	 \[	\min_{b} \frac{1}{n}\sum_{i=1}^{n} (y_{i}-x_{i}'b)^{2},	\]
	 which is also feasible and I can compute with. Hopefully this will give us a good estimate of $\beta$ because we are minimizing something close to the expectation. The optimal solution is nothing but $\hat{\beta}$ in Equation (\ref{beta hat}). This is why it is called ``the least squared''.\par 
	 Either way, we get to the same answer. So that $\hat{\beta}$ is probably a pretty decent estimator of $\beta$. No promises but looks promising to the extent that we have some intuition that expectation is well estimated by averages. Let us show it formally.
	 \begin{theorem}
	 	\textbf{Consistency.} If we make the assumptions OLS $0$, OLS $1$ and OLS $2'$, then 
	 	\[	\hat{\beta} \stackrel{p}{\longrightarrow} \beta.	\]
	 	Notice that $\hat{\beta}$ is just something we defined. We took the motivation from the expression of $\beta$ and replace the expectations with averages, with the hope that we are fine. 
	 \end{theorem}
 	Meaning: As the sample size gets large, the estimator $\hat{\beta}$ converges in probability to the true underlying parameter  $\beta$. Why is that true? Look at the piece of the formula (\ref{beta hat})
 	\[	\hat{\beta} =  (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}(\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}).	\]
 	\begin{itemize}\label{consistency pf}
 		\item $\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}'$ converges in probability to its corresponding expectation by the weak law of large numbers applied to the random matrix $x_{i}x_{i}'$. OLS $(0)$ ensures the i.i.d. condition and we assume that the expectation exists by OLS $(3)$.
 		\item $(\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}$ converges in probability to the inverse of $E(x_{i}x_{i}')$ by the continuous mapping theorem. The continuous function here is matrix inversion as long as it is in the invertible point. We have the nonsingularity of the expectation in OLS $(1)$. 
 		\item $(\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i})$ converges in probability to its corresponding expectation by the weak law of large numbers applied to the random matrix $x_{i}y_{i}$. We got the i.i.d. by OLS $(0)$, and as long as the expectation $E(x_{i}y_{i})$ exists, we are fine. Does it? Yes. Since $E(x_{i}y_{i}) = E(x_{i}(x_{i}' \beta + \epsilon_{i})) =  E(x_{i} x_{i}' \beta) + E(x_{i} \epsilon_{i}) = E(x_{i} x_{i}')\beta + 0 = E(x_{i} x_{i}')\beta$ by OLS $(2')$ and OLS $(1)$.
 	\end{itemize}
 	Therefore, if we put those things together, we get
 		\[	\hat{\beta} =  (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}(\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}) \stackrel{p}{\longrightarrow} E(x_{i}x_{i}')^{-1}E(x_{i}x_{i}') \beta = \beta.	\]
 	This is the rather informal proof of the consistency theorem.\par 
 	Two things to notice: 1) we will do a lot of this in class. 2) Other classes may have focused on different properties, but we are going to focus in asymptotic properties.  By asymptotic properties we mean properties that are true when the sample size becomes large. The reason for this is those properties are the ones that most portable. Those are the ones that tends to extend to other situations most easy. Consistency, and our next property, the asymptotic normality are the things that we are going to carry with us even for complicated models. They are general, robust property that hold under general conditions for OLS and also under general conditions for other models. So we are going to focus on those properties. And those get us really, pretty much what we need to ensure our estimator and procedure here are behaving nicely. They are approximation results in the sense that they converge in probability to $\beta$ for consistency. And in some sense it says the distribution of $\hat{\beta}$ is close to the point $\beta$. What does this mean by a distribution being closed to a point? Almost all the distributions are piled up close to $\beta$ as the sample size gets larger and larger. Remember, OLS $2$ implies OLS $2'$, so consistence holds as well if OLS $2$ is true.\par 
 	This first result is the consistency result. It parallels the law of large numbers. Although the estimator $\hat{\beta}$ is not exactly an average but a somewhat a little complicated combination of two averages.\par 
 	Notice that we have not used OLS $(3)$ yet, and we will use it in the next property. $(3)$ actually is not much an assumption, it says the expectation exists. It is not a big assumption. It could have been part of the moment existence assumption. And the Asymptotic Normality property is going to parallel to the central limit theorem. The central limit theorem says, the averages, if you normalize it properly and resend them around zero, and then scale them up by $\sqrt{n}$, they will converges to normal distributions. $\hat{\beta}$ is not just a simple average, it is a complicated random variable. But we will be able to look at the asymptotic normality of that object like when we were using the central limit theorem to look at the asymptotic normality of a simple average. 
 	\begin{theorem}\label{asymptotic normality}
 		\textbf{Asymptotic Normality.} Under the assumptions of OLS $(0)$, OLS $(1)$, OLS $(2')$ and OLS $(3)$, 
 		\[	\sqrt{n}(\hat{\beta} - \beta) \stackrel{d}{\longrightarrow} N(0,E(x_{i}x_{i}')^{-1}E(\epsilon_{i}^{2}x_{i}x_{i}')E(x_{i}x_{i}')^{-1}).	\]
 	\end{theorem}
 	The result is about $\hat{\beta} - \beta$, so let us let an expression of $\hat{\beta} - \beta$ first. Notice that by Equation (\ref{beta hat}), we know 
 	\[\begin{aligned}
 		 \hat{\beta} &=  (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}(\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}) \\
 		 &= (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}(\frac{1}{n}\sum_{i=1}^{n}x_{i}(x_{i}'\beta+\epsilon_{i}) ) &&\text{(By substitution)} \\
 		 &= (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\beta+\frac{1}{n} \sum_{i=1}^{n} x_{i}\epsilon_{i} ) &&\text{(By distributivity)} \\
 		 &= (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\beta+ (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{n}\sum_{i=1}^{n} x_{i}\epsilon_{i} &&\text{(By distributivity)} \\
 		  &= \beta+ (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{n} \sum_{i=1}^{n} x_{i}\epsilon_{i} &&\text{(Inverse matrix got cancelled)}.
 	\end{aligned}\]
 	Therefore, we have 
 	\[	 \hat{\beta} - \beta = (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{n} \sum_{i=1}^{n} x_{i}\epsilon_{i}.	\]
 	Our epectation is $\hat{\beta} - \beta $ should be converging to $0$ by the central limit theorem. Notice that 
 	\begin{itemize}
 		\item $\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}' \xrightarrow{p} E(x_{i}x_{i}')$;
 		\item $(\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}  \xrightarrow{p} E(x_{i}x_{i}')^{-1}$;
 		\item $ \frac{1}{n} \sum_{i=1}^{n} x_{i}\epsilon_{i} \xrightarrow{p} E(x_{i}\epsilon_{i}) = 0$ by OLS $(2')$. 
 	\end{itemize}
 	For a similar argument, see the trajectory of the proof of the consistency theorem by the bullet points (\ref{consistency pf}). Therefore, we know we have
 	\[	 \hat{\beta} - \beta = (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{n} \sum_{i=1}^{n} x_{i}\epsilon_{i} \stackrel{p}{\longrightarrow} E(x_{i}x_{i}')^{-1} \cdot 0 = 0. 	\]
 	Notice that this also shows the consistency. They are actually the same argument. Next, we want to stretch the random sequence  $ \hat{\beta} - \beta $ by $\sqrt{n}$, just as we did in the central limit theorem. We have 
 	 	\begin{equation}\label{asym normal pf}
 	 		\sqrt{n} (\hat{\beta} - \beta) = (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{\textcolor{blue}{\sqrt{n}}} \sum_{i=1}^{n} x_{i}\epsilon_{i}.
 	 	\end{equation} 
 	 	Notice that I multiplied the the $\sqrt{n}$ in the middle of the right side because this is the piece that goes to $0$. Therefore I am going to explode it up by $\sqrt{n}$ now.\par 
 	 	Take a look piece by pice. Notice that now, we already have the sequence of random variables $(x_{i}\epsilon_{i})$ with expectation $0$, so we do not need to shift it by its expectation. So by the central limit theorem, we must have 
 	 	\[  \frac{1}{\sqrt{n}} \sum_{i=1}^{n} x_{i}\epsilon_{i} =  \frac{1}{\sqrt{n}} \sum_{i=1}^{n} [x_{i}\epsilon_{i}-E(x_{i}\epsilon_{i})] \stackrel{d}{\longrightarrow} N(0,V)	\]
 	 	for some variance, and $V = Var(x_{i}\epsilon_{i})$ only in the i.i.d. case.\par 
 	 	And as for the first term $(\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}$, we know it converges in probability to $E(x_{i}x_{i}')^{-1}$ by the second bullet point above. $E(x_{i}x_{i}')^{-1}$ is a fixed value (actually, a matrix) by OLS 1. So we multiply two random variables together, in which one converges in probability and the other converges in distribution, then the product random variable will converges in distribution to the product of the constant(could be a matrix) and the distribution. This is the Slutsky theorem (\ref{slutsky}). Therefore, we have the formula in Equation (\ref{asym normal pf}) 
 	 	\[		\sqrt{n} (\hat{\beta} - \beta) = (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{\sqrt{n}} \sum_{i=1}^{n} x_{i}\epsilon_{i} \stackrel{d}{\longrightarrow} E(x_{i}x_{i}')^{-1} \cdot N(0,Var(x_{i}\epsilon_{i})) =  \cdot N(0,E(x_{i}x_{i}')^{-1}Var(x_{i}\epsilon_{i})E(x_{i}x_{i}')^{-1}) .	\]
 	 	(The last equality holds by Lemma (\ref{lc of normal}) and (\ref{the matrix world constant})).\par 
 	 	Then, by the Lemma (\ref{lemma cov}) below, we have 
 	 	\[	Var(x_{i}\epsilon_{i}) = E(x_{i}\epsilon_{i}(x_{i}\epsilon_{i})') - E(x_{i}\epsilon_{i})E(x_{i}\epsilon_{i})' = E(\epsilon_{i}^{2}x_{i}x_{i}')	\]
 	 	since $E(x_{i}\epsilon_{i}) = 0$ by OLS $2'$.\par 
 	 	 Our proof is done. Notice that OLS 3 is functioning here as the middle term in the variance of the asymptotic normal distribution. 
 	 	\begin{lemma}\label{lc of normal}
 	 		The linear combinations of normally distributed random variables(vectors) still have normal distributions. As you may curious, the sum of two independent normally distributed random variables is normal, with its mean being the sum of the two means, and its variance being the sum of the two variances.
 	 	\end{lemma}
  		\begin{lemma}\label{the matrix world constant}
  			If we have $E$ being a matrix, $N(0, V)$ being a normal distribution with the variance $V$, then 
  			\[	E N(0, V) = N(0, EVE').	\]
  			It is like the matrix world of the square of the constant(the matrix).\par  
  		 If we have $\alpha$ being a constant, then 
  			\[	\alpha N(0, V) = N(0, \alpha^{2} V).	\]
  		\end{lemma}
  		\begin{definition}
  			The \textbf{variance–covariance matrix (or simply the covariance matrix )} of a random vector $X$ is given by
  			\[	Cov(X) = Var(X) = E((X-E(X))(X-E(X))').	\]
  			For a random variable $x$, we have 
  			\[	Var(x) = E((x-E(x))^{2}).		\]
  		\end{definition}
  		\begin{lemma}\label{variance oepration}
  			If $Var(w) = \sigma^{2}$, then we have $Var(aw) = a^{2}\sigma^{2}$. If $w$ is no longer a scalar, but is now a vector, then the variance becomes this square matrix. That is, $Var(Aw) = A Var(w) A'$ where $w$ is a vector or matrix. Instead of squaring, you pre- and pose- multiply. It's squaring, it's just the matrix version of squaring.  
  		\end{lemma}
  		\begin{lemma}\label{lemma cov}
  			\[	Cov(X) = Var(X) = E(XX') - E(X)(E(X))'.	\]
  			For a one-dimensional random random variable $x$, we have the similar result
  			\[	Var(x) = E[x^{2}] - E[x]^{2}.	\]
   		\end{lemma}
  		\textbf{Why do we care about the result of this type?}\par 
  		In the consistency part, it is saying: hey our estimator is good in some sense. Basically, if we get lots of data, our estimator should do a good job. It should be with very high probability being very close to the true value of $\beta$. As we getting more data, your estimator's mass distribution is going to be collapsing right around $\beta$. \par 
  		As for the asymptotic normality, it tells us about the shape of that distribution ($\hat{\beta}$'s distribution around $\beta$). It tells us the shape of $\hat{\beta} - \beta$(it is a spike around $0$) and stretch it by $\sqrt{n}$. Why do we care with that shape? Because it enables us to form confidence intervals. And it allows us to do things like testing hypotheses stuffs.\par
  		So consistency is about estimation, our estimator is good. And the asymptotic normality says hey, we are going to be able to use that estimator to form a confidence interval. That is, we are going to be able to say, if we take $\hat{\beta}$ and add or subtract a little bit, then $\beta$ the true value, should fall into that interval with some fixed probability, usually $95$ percent. Or, this will allow us to say, if we have a predetermined value of $\beta$, we can test whether $\beta$ takes that value.  And we use this result to perform the hypothesis test of whether $\beta$ is actually $0$ or not. In order to do that, we are going to make use of this formula of variance in $(\ref{asymptotic normality})$. A lot of times you will see a formula looks like $\sigma^{2}(x_{i}x_{i}')^{-1}$ or $\sigma^{2}(x'x)^{-1}$. These are simpler. Why ours' is so complicated? It is because we did not make the simplifying assumption about $\epsilon_{i}$ what we call ``homoskedasticity''. It's possible that in a previous class, when you talked about ordinary least squares, you talked about the homoskedasticity case. The homoskedastic model is a special case of our model. The homoskedasticity is assuming 
  		\[	\sigma^{2} = E(\epsilon_{i}^{2}|x_{i}) = E(\epsilon_{i}^{2}).	\]
  		We do not assume it since it almost never happens in practice and economics. And in general, in economics, we would expect heteroskedasticity. We could let $\epsilon_{i}^{2}$ depending on $x_{i}$. Heteroskedasticity would be look like this:
  		\begin{figure}[hbt]
  			{\centering \includegraphics[scale=0.38]{hetero}}
  			\caption{Heteroskedasticity data}
  		\end{figure}
  		And as for homoskedaticity, the spread of $y$'s does not vary much with the value of $x$.\par 
  		We are allowing heteroskedasticity. That is a good thing in the sense that there is very few applications that we go to where homoskedasticity holds. Heteroskedasticity is the real world. 
\subsection{Confidence Intervals}
	Why do we care about a result like that. The reason is that we can use it to get confidence intervals. If we were to open up the latest issue of the American Economic Review or your favorite high level journal that has a lot of applied work in it. One thing you would find in the AER which I think is would be accepted as certainly a top five journal. If we were to open it up, one thing that we would discover is that ordinary least squares is used many times in that journal. You might think this is baby stuff. Who cares about this? It's used in the top journals, do the search, okay? So it's used all the time may not be in every article, but a high fraction of the articles would have some ordinary least squares estimates. And the way that those estimators would be presented are typically in tables. So that you've got this list of parameters that you're trying to estimate, and you get there the list of the estimates. And usually the estimators: you get your data, and you generate your beta hat, or you get your software to produce beta hat, a number or a vector of numbers. Typically, in that table, there's the estimates, and then underneath the estimates, there's something in parentheses. Those are the standard errors. Why are those there? They are to give you a sense of how accurate or how precise this estimator is. Because they help you to form confidence intervals. So that you cannot only have an estimate, you have some sense of whether the estimate is close or not to the parameter its estimating. It's not perfect. There's randomness. These are random variables, random vectors. But we can say things like with $95$ percent probability, this interval will contain the true value. Or if we wanted to, we can form different competent intervals. Let's talk about that. Where does that come from.\par 
	So we're gonna start with some parameter and the asymptotic normality result. We've already got one example OLS, but we'll show you more that look like this. For the moment. Just to make it simple, I'm gonna think about scalar data. Beta is only gonna have be real value, just for simple.\par 
	If that's true, then one thing that I can do is I can divide this quantity by the square root of variance $V$. This is what the scalar allows me to divide by a square root of $V$ instead of this being a matrix. Then we will have 
	\[		\frac{\sqrt{n}(\hat{\beta} - \beta)}{\sqrt{V}} \stackrel{d}{\longrightarrow} N(0,1).	\]
	This would converge in distribution to a normal with mean zero and variance one, a standard normal. What does this mean? It means as the simple size grows large, this random variable on the left (that whole thing) will have a distribution that's close to, in some sense, the standard normal distribution.\par 
	Now, here's a property of standard normal distributions. I should say, if I take my standard normal distribution, centered around zero. This is the density, the bell shaped curve. Then  the area under the curve between $1 . 96$ and $- 1 . 96$ is $. 95$, that means we have left $.025$ in each tail.\par 
	That means that the probability of $	\frac{\sqrt{n}(\hat{\beta} - \beta)}{\sqrt{V}} $ between $+/- 1.96$ is approximately $95$ percent. That is, 
	\[	0.95 \approx Pr[-1.96 \leq\frac{\sqrt{n}(\hat{\beta} - \beta)}{\sqrt{V}} \leq 1.96 ].\	\]
	Rearrange terms, we have 
	\[	\begin{aligned}
	0.95 &\approx Pr[-1.96 \leq\frac{\sqrt{n}(\hat{\beta} - \beta)}{\sqrt{V}} \leq 1.96 ] \\
	&= Pr[-1.96 \sqrt{\frac{V}{n}} \leq (\hat{\beta} - \beta)\leq 1.96 \sqrt{\frac{V}{n}} ] \\
	&= Pr[-1.96 \sqrt{\frac{V}{n}} \leq (\beta - \hat{\beta})\leq 1.96 \sqrt{\frac{V}{n}} ] \\
	&= Pr[ \hat{\beta}-1.96 \sqrt{\frac{V}{n}} \leq \beta \leq \hat{\beta} + 1.96 \sqrt{\frac{V}{n}} ].
	\end{aligned}
	\]
	What does this one here say? $\beta$ is a parameter. It's a value. It's not stochastic. It doesn't have a distribution. It's just a number. $V$ is a variance. It's also just a number in the scalar case. In the OLS case, it corresponds to this number in (\ref{asymptotic normality}). $n$ is just a number, $- 1 . 96$ is just number. $\hat{\beta}$ is different. It's a random variable. It's random. It's gonna depend on the data you get. When we think about its distribution before we get the data, it's gonna be like a random variable that has a distribution. What is this saying? It says if we take $\hat{\beta}$ and subtract this number and take $\hat{\beta}$ and add this number, that creates an interval, and that interval should be covering or including $\beta$ with probability $.95$ percent. $\beta$ is not moving, but $\hat{\beta}$ that's random. That comes from a distribution. As we get new different draws,  sometimes it would conclude it, sometimes it wouldn't. What this is saying is that interval is moving around. And $95$ percent of the time as that interval moves around, it includes the true $\beta$. So that would be a an approximate $95$ percent confidence interval, but people don't usually say the approximate, usually they just say $95$ percent with the understanding that people know it's not perfect. As the sample size gets large, this approximation gets better, an hopefully that probability gets closer and closer to $95$ percent.\par 
\subsection{White Robust Standard Errors Estimation} 	
  		But we still have a challenge to know something about the variance of this asymptotic normal distribution. You are probably thinking I know the trick, although I do not know the expectations, I know how to estimate them by average numbers. With the weak law of large numbers I am set. The problem here is we do not observe $\epsilon_{i}$ in the middle term $E(\epsilon_{i}^{2}x_{i}x_{i}')$, although we do observe $y_{i}$ and $x_{i}$. Of course we will know $\epsilon_{i}$ if we know $\beta$, but not knowing $\beta$ is kind of the whole reason why we are getting here. On the other hand, fortunately, we know how to estimate $\beta$, it follows then we will have a way to estimating $\epsilon_{i}$. We will come back and figure out how to get an approximation to this variance in Theorem (\ref{asymptotic normality}) by estimating the $\epsilon_{i}$. \par 
  		So our next topic today is robust standard error estimation, it is a big topic, and it is generally in a common econometrics. And as Harold mentioned on the first day, it's an active area of research. So this research has been ongoing from the first contribution over 40 years. But there's still contributions being made to this area that are affecting practice and how people practice a kind of econometrics now. We'll get into the issues surrounding robust standard errors. I will just overview and give you a little bit more detail on that heteroskedasticity case, and then a little detail on a couple of other cases. And then let Harold will take it from there. It's important enough that we kind of want to have it in your mind and thinking and spinning in the back of your mind so that it's a little easier when Harold gets over. Also it's a little bit unfair to  talk about these methods that you're probably familiar with ordinarily least square, IV and all of these things, but not also introduce robust standard error methods, because that is what's commonly use. These are the methods of people use in practice.\par 
		 Now there's a problem with this confidence interval, and that is: $\hat{\beta}$, at least in the OLS case, is something we have a formula for. But $V$ is not known. We know that we have 
		 \[	V = E(x_{i}x_{i}')^{-1}E(\epsilon_{i}^{2}x_{i}x_{i}')E(x_{i}x_{i}')^{-1}.	\]
		 in the OLS case. $V$ depends on these expectations. $V$ is not known in the OLS case, or more generally. So we also have to produce an estimator for $V$ if we want to produce the confidence interval. So the actual confidence interval that people would actually use is not this one. Usually we find an estimator $\hat{V}$ that converges in probability to $V$. And then use 
		 \[	Pr[ \hat{\beta}-1.96 \sqrt{\frac{\hat{V}}{n}} \leq \beta \leq \hat{\beta} + 1.96 \sqrt{\frac{\hat{V}}{n}}]	\]
		 instead, which is just replacing $V$ with $\hat{V}$. In the papers, in the AER where the tables are. There's the estimate of $\hat{\beta}$. Then there's another number, which is the \textbf{standard error}, which is $\sqrt{\frac{\hat{V}}{n}}$. Why is that the standard error? It corresponds to the the square root of the variance of $\hat{\beta}$. You can see that $V$ is not the variance of beta hat, is approximating the distribution of the variance of $\hat{\beta}$ times $\sqrt{n}$ (minus $\beta$ does not affect the variance).  \par 
		 If I were to divide both side to this equation (\ref{asymptotic normality}) by $\sqrt{n}$ then I would have a $\frac{1}{\sqrt{n}}$ on the right side. Because I have to square whatever the scalar entering to variances, we have 
		 \[	(\hat{\beta} - \beta) \stackrel{d}{\longrightarrow} \frac{1}{\sqrt{n}} N(0,V) = N(0,\frac{V}{n}).	\]
		 \textbf{And the square root of $\frac{V}{n}$ would be the standard error and we approximate that with the square root of $\frac{\hat{V}}{n}$. And once you have your estimate $\bar{V}$, then you have your standard error $\sqrt{\frac{\hat{V}}{n}}$.}\par 
		 You probably already do this. But I'll tell you, if you do any applied work, certainly in economics, but in most of social sciences. People immediately take those standard errors and in their head, they think about twice the standard error. Then they think about $\hat{\beta}$, adding and subtracting $2$ times the standard error. They don't use $1.96$ because you're just doing quick and easy. What is that confidence interval? And most of the time people have in their mind, the question of whether $\hat{\beta}$ is, what people will refer to as ``statistically significant''. That means, does the $95$ percent confidence interval contains zero or not.\par 
		 So the key thing pins down to the estimate of variance $V$. Now let's talk about this case $\hat{V}$, how we do it.
		 \[	V = E(x_{i}x_{i}')^{-1}E(\epsilon_{i}^{2}x_{i}x_{i}')E(x_{i}x_{i}')^{-1}.	\]
		 As we said last time, it gets a lot of objects that depend on underlying distributions, sometimes in the form of expectations, we don't know the expectations because we don't know the underlying distributions, but we usually get data that tells us about those distributions. And we can use that in a simple way, usually, to estimate expectations.\par 
		 So there are three terms here. Really, two of them are the same. It's really only two terms that we got to worry about. If we can estimate these three terms will be in business. We have already known how to estimate this $E(x_{i}x_{i}')$. It is actually part of the OLS formula. We have \par 
		 \[	\frac{1}{n} \sum_{i=1}^{n} x_{i}x_{i}' \stackrel{p}{\longrightarrow} E(x_{i}x_{i}')	\]
		 and 
		 \[	(\frac{1}{n} \sum_{i=1}^{n} x_{i}x_{i}')^{-1} \stackrel{p}{\longrightarrow} E(x_{i}x_{i}')^{-1},	\]
		  as long as this expectation $E(x_{i}x_{i}')$ is itself invertible, which would be implied by our OLS (1) assumption.\par 
		   What about the middle term? This middle term is also easy in the sense that it's an expectation. So you should just be able to replace the expectation with an average and estimate it. But unfortunately, it involves a random variable that's not directly observed. Our data in the OLS world is just $y_{i}$ and $x_{i}$ for $i = 1,2,3, \dots$.  Anything we want to use as an estimator has to be a function of those things. In other words, it's got to be something that's in our computer and that our software knows how to manipulate through some formula, create an estimate. But we do not have $\epsilon_{i}$. Keep in mind, we have 
		   \[	\epsilon_{i} = y_{i} - x_{i}' \beta.	\]
		   We don't know $\epsilon_{i}$. It is a function of things we observe, $y_{i}$ and $x_{i}$ but it's also a function of $\beta$, which we do not know. But there is a pretty natural way to estimate it. We've been the whole focus was estimating $\beta$. I've got $\hat{\beta}$ from OLS (\ref{beta hat}) so why don't we just use that to generate
		   \[	\hat{\epsilon_{i}} = y_{i} - x_{i}' \hat{\beta}.	\]
		  	And hopefully to have 
		  	\[	\frac{1}{n}\sum_{i=1}^{n} \hat{\epsilon_{i}}^{2} x_{i}x_{i}' \stackrel{? p}{\longrightarrow} E(\epsilon_{i}^{2}x_{i}x_{i}').	\]
		   That actually works. In general, and certainly under conditions, but certainly under the conditions that we talked about, you may need more moments to exist, in general, for this.\par 
		   If I put those pieces together, I can form an estimator of $\hat{V}$. The estimator would be 
		   \[	\hat{V}_{OLS} = (\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}')^{-1} (\frac{1}{n}\sum_{i=1}^{n} \hat{\epsilon_{i}}^{2} x_{i}x_{i}') (\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}')^{-1}.	\]
		   This is called the \textbf{White Heteroskedasticity Robust Variance Estimator}. It is robust to hetroskedasticity since we have never assumed $E(\epsilon_{i}^{2}|x_{i}) = E(\epsilon_{i}^{2})$ in our OLS assumptions. It is also the robust variant estimator that we are going to use when we have independence but not identically distributed data i.n.i.d.. But it is not robust to dependence, we need Newey-West robust standard error there.\par 
		    \emph{Once we get $\hat{V}$, we can divide it by $n$ and then take the square root to form robust standard error estimator. Then we can form the confidence interval.}\par 
	   		 Even when I write it in the chalkboard, it seems so simple.  How could someone not know this? But actually, it turns out that for many decades, this wasn't understood. The reason is actually that people were writing things in matrix form.  And they wrote the big variance that way, too. In matrix form, if they allow for herteroskedasticity, they will get things like 
	   		 \[	(X'X)^{-1}X' \Omega X (X'X)^{-1}	\]
	   		 where this $\Omega$ would be a diagonal matrix. The problem is this omega. It's actually $n-$by$-n$. Even the diagonal has $n$ elements. It seems like too many can have to make $n$ things with only $n$ data points. Too many things, we can't do a good job. So it seemed impossible.\par 
	   		 So why is this different? It's different because actually, you don't need an estimate of $\Omega$. Actually, what you need is an estimate of this $X' \Omega X$, which is only a $k-$by$-k$ matrix. So just rewriting it in a different way and that actually help the breakthrough.
	   		 
\section{Dependent Data and Newey-West}
	   		  When do we have independence? We have independence typically when we think the agents are individuals in our dataset are from some large micro data set where the agents are acting essentially independently. Also we want to think about the case when our data is not identically distributed. This section is mainly about the Newey-West robust standard error since it is robust to dependence (serial correlation) and for the independently in-identically distributed data, White's variance estimation is still robust. White just ignores the terms in the second parenthesis.\par 
	   		 Sometimes we don't have independent data, though. So we have to think about the cases of dependence as well. Let's take a sort of broad approach and talk about kind of the different forms in which we can deal with.
	   		\[ \sqrt{n} (\hat{\beta} - \beta) = (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{\sqrt{n}} \sum_{i=1}^{n} x_{i}\epsilon_{i}.	\]
	   		 This is the form in which we kind of transformed our least squares estimator to look like this. Last time we use this to derive the asymptotic normality (\ref{asym normal pf}). What I want to say is this is just a formula. There's something you're going to plug in. Now, Sometimes it might be useful to keep in your mind, at least to think about. Here i'm using the notation $i$. Usually we think of $i$ as individuals, if it's more helpful, sometimes you might think about using $t$ for time instead of $i$. And you get different observations across time. That would kind of give you the feeling that these are probably correlated. These are dependent on each other. And so we have to worry more about the dependent structure. The way in which the asymptotic normality work is the following. This average $\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}'$ converges in probability somewhere to the expectation matrix $M$. And we assumed $M$ is invertible by OLS 1. Our assumptions were
	   		 \begin{enumerate}
	   		 	\item (OSL 0) $(y_{i},x_{i})$ their distribution and dependence. How those observations comes to you.
	   		 	\item (OLS 1) $(\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')$ converges in probability to $M$ and $M$ is invertible. This is telling us that we could apply a weak law of large numbers. This assumption is going to control what's going on in $\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}'$ and $(\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1}$.  So this assumption is something just trying to say. This term is okay, nothing crazy is happening when we are trying to get consistency and the variance of asymptotic normal distribution.
	   		 	\item (OLS 2) $E(\epsilon_{i} | x_{i}) = 0$  or OLS (2') $E(x_{i}\epsilon_{i}) = 0$. This is telling us that for 
	   		 		\[ (\hat{\beta} - \beta) = (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{n} \sum_{i=1}^{n} x_{i}\epsilon_{i},	\]
	   		 		$\frac{1}{n} \sum_{i=1}^{n} x_{i}\epsilon_{i}$ is converging in probability to $0$(and also by the law of large numbers). That was the key thing. That's what that assumption was trying to do. Is trying to get the consistency. If I put OLS 1 and OLS 2' those two together, I get consistency. That's kind of the object. \emph{Notice that it doesn't matter what OLS $0$ is. I just got to get these things working like this.} 
	   		 	\item (OLS 3) $E(\epsilon_{i}^{2}x_{i}x_{i}')$ is finite and nonzero. It is trying to deal with the asymptotic normality issues. It is a little bit harder to see in the independent case. But basically, what I want OLS $3$ to do for it is to tell us that I can apply a central limit theorem on $\frac{1}{\sqrt{n}} \sum_{i=1}^{n} x_{i}\epsilon_{i}$, and this is gonna converge in distribution to a normal with mean zero and some variance. So the whole point of that assumption is just to make sure that when I go to show asymptotic normality, that term goes to a normal with mean zero and some variance. 
	   		 \end{enumerate}
   		 	Why am I talking about this kind of assumptions in a more general sense again? Because I want to change OLS 0. I don't want to always assume i.i.d. data. I might want to allow for data that has dependence in it, like time series data for macro. When I change that, I'm gonna have to change these other assumptions potentially. But what I want you to know is,  whatever I change them, I have to change them in a way that these convergences hold. Even if I don't have independent variables, it's still the case that I'm gonna be able to apply a different weak law of large numbers and different weak law of large numbers that applies to whatever this type of data I have got, and that will give me convergence in probability to something that's nice and invertible. I'm gonna have a a central limit theorem that applies to random variables that are not i.i.d. perhaps. I'm gonna say whatever the types of data that are, they're satisfied. So I can do this. That's gonna give me my consistency and the asymptotic normality.\par 
   		 	The key thing here is that what I'm gonna end up with is that this $\sqrt{n}(\hat{\beta}_{OLS} - \beta)$ converges in distribution to a normal with mean $0$ and variance $M^{-1}\Omega (M^{-1})'$ (See the proof of asymptotic normality in \ref{asym normal pf}). And $M$ inverse is just gonna be this expectation of $\frac{1}{n}\sum_{i=1}^{n} (x_{i}x_{i}')^{-1}$ in OLS 1. That's no big deal.  But the $\Omega$ part could vary. $\Omega$ is gonna correspond to the variance of the asymptotic normal distribution that approximates this re-scaled average $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}\epsilon_{i}$, that is to say, 
   		 	\[	\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}\epsilon_{i} \stackrel{d}{\longrightarrow} N(0,\Omega).	\] And that $\Omega$ is the middle term in 
   		 	\begin{equation}\label{normality general case}
	   		 	\sqrt{n} (\hat{\beta} - \beta) = (\frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}')^{-1} \frac{1}{\sqrt{n}} \sum_{i=1}^{n} x_{i}\epsilon_{i} \stackrel{d}{\longrightarrow} N(0,M^{-1}\Omega M^{-1}). 
   		 	\end{equation}
   		 	It's useful to think about what happens when $n$ is not so big for just a minute. Let us thing about the case when $n$ is two.  That's certainly not when this asymptotic normality will kick in. But it'll give you a little bit of a sense about what's going on with that $\Omega$. If $n = 2$, then 
   		 	\[	\begin{aligned}
	   		 	Var(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}\epsilon_{i}) &= \frac{1}{2} Var(x_{1}\epsilon_{1} + x_{2}\epsilon_{2}) \\
   		 	 &= \frac{1}{2} [E(\epsilon_{1}^{2}x_{1}x_{1}') + E(\epsilon_{2}^{2}x_{2}x_{2}') ] + \frac{1}{2}[E(\epsilon_{1} \epsilon_{2} x_{1}x_{2}') + E(\epsilon_{1} \epsilon_{2} x_{2}x_{1}')].
   		 	\end{aligned}	\]
   		 	These lemmas are used during the derivation, see also (\ref{variance oepration})
   		 	\begin{lemma}
   		 				\[	\operatorname{Var}(a x+b y)=a^{2} \operatorname{Var}(x)+b^{2} \operatorname{Var}(y)+2 a b \operatorname{Cov}(x, y).	\]
   		 				Notice that when calculate the variance of a random variable with a scalar multiplied, we need to square it when take it outside. This is the opposite to the rule of operation in calculating the variance of a normal distribution in lemma (\ref{the matrix world constant}) multiplied by a scalar, where we have to square it when bring the scalar inside. 
   		 	\end{lemma}
   		 	
   		 	 Variance is basically just taking a square, and then taking the expectation. This is what $\Omega$ is kind of approximating. This is what's kind of sitting behind $\Omega$. When $n$ is 1 million, There's way more terms here.\par 
   		 	 What happens when we have independence? When our data is independent of each other, so $x_{1}$ and $\epsilon_{1}$ are independent of $x_{2}$ and $\epsilon_{2}$, these cross terms in the later parentheses disappear. That is, we will have 
   		 	 \[	E(\epsilon_{1}\epsilon_{2}x_{1}x_{2}') = E(\epsilon_{1}x_{1})E(\epsilon_{2}x_{2}')	\] if $(x_{1},\epsilon_{1})$ is independent of $(x_{2}, \epsilon_{2})$.
   		 	 This is a special situation. Usually, the expectation of a product is not the product of the expectations except when they're independent. Then you can take the expectation of the product, and that's equal to the product of the expectation. Notice that these two terms are zero by OLS 2'. It doesn't seem like a huge simplification here, but it's an enormous simplification when $n$ gets large.\par 
   		 	 When we're in the identically distributed case, then $(x_{1},\epsilon_{1})$ has the same distribution with $(x_{2},\epsilon_{2})$. That means $E(\epsilon_{1}^{2}x_{1}x_{1}')= E(\epsilon_{2}^{2}x_{2}x_{2}')$.\par 
   		 	 We started out that our OLS data was i.i.d., i.e., independent and identically distributed. The independence kills these terms in the second parentheses. Nice. They identically distributed, says all these terms in the fist parentheses is really only one of them. That's why before I say all you have to estimate is $E(\epsilon_{i}^{2}x_{i}x_{i}')$. That's because it doesn't matter what $i$ is put in there. They're all the same.\par 
   		 	 So now let's talk about weakening these strong assumptions. \emph{First, let's talk about what would happen if we kept the independence.} But we got rid of the identically distributed part. What would happen is, instead of having one term there, we would have the average of all these terms in this first parenthesis. That would be the independent, not identically distributed case. That's usually referred to as i.n.i.d.. In that case, we have to estimate the sum of all these terms. But that's okay. \emph{That's actually what we were estimated to begin with. That is how White is estimating. He summed up over all these, essentially an estimate of each of these terms. So his estimator, in general, still works.} You have to control how much non-identality that you're allowed with, how much heterogeneity is here. But his heteroskedastic estimator.  estimator will still work.\par 
   		 	 \emph{Now let's think about what happens when we drop the independence. Let's drop the independence, but keep the identically distributed case.} There's dependence. But everything is identically distributed as we move across individuals. \emph{Now white's estimator isn't gonna work because his estimator really is just ignoring these terms in the second parentheses.} We're going to have to take account of these terms. One case where that comes up is the case of serial correlation, where you got dependent data is serially correlated andor strictly stationary (the dependent but identically distributed case). And in that case, we'll use what's called the Newey-West estimator.\par 
   		 	 A different case that comes up a lot nowadays is the clustered case.  The clustered case is a case where you've got clusters of observations that are dependent on each other. But each cluster is independent of each other clusters. And cluster standard errors is another form of robust standard errors that people deal with. And that does come up a lot.\par 
   		 	 I will back up one step and just say, essentially what happens. Started off saying we're gonna form these confidence intervals. They're gonna be based on our estimators and the standard error estimator, which is $\sqrt{\frac{\hat{V}}{n}}$. Here, $V$ is $M^{-1}\Omega M^{-1}$ in equation (\ref{normality general case}). What's gonna change, as we look at different types of data that we might get? $\Omega$. $M^{-1}$ is not changing. How to estimate $\Omega$. That's gonna depend on the dependent structure of our data. It's gonna depend on which things we can assume are zero and which we cannot. Could we just allow everybody to be arbitrarily correlated and dependent on everybody else in our data? No, if that is true, and if we're getting like $1$ million data points. Now, we'd have to like estimate this huge million by million variance matrix, variance covariance matrix. It's not $1$ million squared, because it's symmetric. But nearly that, right? Of that order of magnitude. So you can't do a good job of estimating that. It's just too much. You have to impose some structure on the dependence to get somewhere.  So those ideas that we were talking about our kind of middle grounds. One way in which you can control the dependence that you allow for is if I've got a time series data, the amount of dependence is gonna kind of decrease as we get further and further apart. So we're gonna have to control for data that are possibly close in time to each other. But if they're really far from each other in time we want, we think they are independent. That's one way of controlling the dependence. Then not all the observations are dependent on on each other.The other common way of controlling is the clustering idea.\par 
   		 	 Those are the two variations on how Halbert Whites' estimator they come up. The one that works for time dependent observations has been around since the mid $80$s. That's the Newey-West method. West is a Wisconsin economics departments faculty member. Then more recently, the clustering stuff that's really been around for a longer period of time, really more of a focus on how to do it in the right way only for over $10$ years. So that's not may seem like a long time ago, but that's pretty recent.  So I'll give you just a brief sense of that. 
   		 	 \begin{definition}
   		 	 	A sequence of random variables(vectors) is \textbf{strictly stationary} if $(z_{t}, \dots, z_{t+m}) \sim (z_{s}, \dots, z_{s+m})$ for all $t,s, m \geq 1$, $\sim$ means they have the same joint distribution.
   		 	 \end{definition}
   	 	 	 What's happening here is that I want to think about an assumption that weakens our i.i.d. assumption.  I want to allow for dependence. I don't want to impose independence, but i'm going to impose the sort of identical distribution still. This is one way to weaken that dependence, but not by that much. This assumption makes a strong assumption. It says I if I observe some macro variables in $2000$ and I observe them again in $2005$, they should be drawn from the same distribution. It's too strong of an assumption, but it's actually weaker than the assumption we've been working with, which is i.i.d.. We can allow for dependence and allow for more heterogeneity at which would allow for more variation across time.\par 
   	 	 	 We could rewrite our os estimator like this using a different subscript.
   	 	 	\begin{equation}\label{beta hat minus beta,sc}
   	 	 	\hat{\beta}_{OLS} - \beta = (\frac{1}{T}\sum_{t=1}^{T}x_{t}x_{t}')^{-1}\frac{1}{T}\sum_{t=1}^{T}x_{t}\epsilon_{t}.
   	 	 	\end{equation}
   	 	 	 So we are now allowing for serial correlation. 
   	 	 	 \begin{enumerate}
   	 	 	 	\item \textbf{(SC $0$).} $(y_{t},x_{t})$ is strictly stationary and some moment exists.
   	 	 	 	\item \textbf{(SC $1$).} There are some conditions under which $(x_{t}x_{t}')$ satisfies the weak law of large numbers and $E(x_{t}x_{t}')$ is finite and non singular.\par 
   	 	 	 	This is no longer a an i.i.d. sequence. It's actually a dependent sequence, but the weak laws of large numbers exist for dependent sequences as well, especially for strictly stationary ones. They are fairly straightforward.  The intention of this first assumption is to control this first term $\frac{1}{T}\sum_{t=1}^{T}(x_{t}x_{t}')^{-1}$. Therefore we have 
   	 	 	 	\[	(\frac{1}{T}\sum_{t=1}^{T}x_{t}x_{t}')^{-1} \stackrel{p}{\longrightarrow} E(x_{t}x_{t}')^{-1}.	\]
   	 	 	 That's true by SC $1$ and the whole purpose of SC $1$ is to make sure this term is okay. Nothing crazy is happening. 
   	 	 	 	\item \textbf{(SC $2'$).} $\{x_{t}\epsilon_{t}\}$ satisfies Weak Law of Large Numbers and $E(x_{t}\epsilon_{t}) = 0$.\par 
   	 	 	 	The purpose of SC $2'$ is to get consistency. Kind of taking a step back and think about the big picture of how we might set up assumptions to get consistency and asymptotic normality properties for situations other than the i.i.d. case. And even situations other than the strictly stationary case. That's kind of the idea behind them go to a different independent structure and leave these alone. 
   	 	 	 	\item \textbf{(SC $3$).} $\{x_{t}\epsilon_{t}\}$ satisfies the Central Limit Theorem. \par 
   	 	 	 	SC $3$ to make sure that we get the Central Limit Theorem. So if it's okay, I'm gonna multiply both  sides of the equation (\ref{beta hat minus beta,sc}) above by $\sqrt{T}$, which gives us 
   	 	 	 	\begin{equation}\label{sc, sqrt T}
   	 	 	 	\frac{1}{\sqrt{T}}(\hat{\beta}_{OLS} - \beta) = (\frac{1}{T}\sum_{t=1}^{T}x_{t}x_{t}')^{-1} \frac{1}{\sqrt{T}}\sum_{t=1}^{T}x_{t}\epsilon_{t}.
   	 	 	 	\end{equation}
   	 	 	 	And by this SC $3$, we know $\frac{1}{\sqrt{T}}\sum_{t=1}^{T}x_{t}\epsilon_{t}$ is going to converge in distribution to a normal with mean zero and some variance. That is, 
   	 	 	 	\begin{equation}\label{sc 3 object}
   	 	 	 	\frac{1}{\sqrt{T}}\sum_{t=1}^{T}x_{t}\epsilon_{t} \stackrel{d}{\longrightarrow} N(0, \Omega).
   	 	 	 	\end{equation}
   	 	 	 	If you want, we can just make equation (\ref{sc 3 object}) the assumption.
   	 	 	 \end{enumerate}
    	 	 
    	 	 What we learned last time was that if we want to put these two pieces in equation (\ref{sc, sqrt T}) together, what we end up with is an asymptotic distribution of the least estimator:
    	 	 \[		\frac{1}{\sqrt{T}}(\hat{\beta}_{OLS} - \beta) \stackrel{d}{\longrightarrow} N(0, E(x_{t}x_{t}')^{-1} \Omega E(x_{t}x_{t}')^{-1}).	\]
    	 	 It has three terms, the first in the last term are the same and estimating it is easy. We just use SC $(1)$ and use its average version. The whole game is $\Omega$.\par 
    	 	 We did an example last time where I just took $T$ to be equal to two.  In that case, when I looked at the variance of this thing, there were four terms.  They're kind of the two square terms, and there were the cross terms. When we have independence, the cross terms in the second parenthesis are zero. And that's when we can use Halbert Whites' method. But when we don't have independence, the cross terms might matter.\par 
    	 	  So under strict stationary assumption, $\Omega$ will look like this:
    	 	  \[	\Omega = E(\epsilon_{t}^{2}x_{t}x_{t}') + \sum_{ l = 1}^{\infty}[E(\epsilon_{t} \epsilon_{t-l} x_{t} x_{t-l}') + E(\epsilon_{t} \epsilon_{t-l} x_{t-l}x_{t}') ].	\]
    	 	  $E(\epsilon_{t}^{2}x_{t}x_{t}')$ is all the square terms and $\sum_{ l = 1}^{\infty}[E(\epsilon_{t} \epsilon_{t-l} x_{t} x_{t-l}') + E(\epsilon_{t} \epsilon_{t-l} x_{t-l}x_{t}') ]$ is all the cross terms that have to do with dependence between what's happening at time $t$ and what's happening at some other time $t - l$. If these were independent, then these expectations $E(\epsilon_{t} \epsilon_{t-l} x_{t} x_{t-l}')$ and $E(\epsilon_{t} \epsilon_{t-l} x_{t-l}x_{t}')$ would break apart into two expectations, which would both be zero. But if they're dependent, then it may not. And so then we have to worry about these terms when we go to estimate $\Omega$. So under the i.i.d. assumption(also when only the independence assumption), we just have to estimate this $E(\epsilon_{t}^{2}x_{t}x_{t}')$ and that's the White heteroskedastic Robust variance estimator. We replace this expectation with an average and replace the $\epsilon$ with a hat and we're done. That's what we did last time. The Newey-West method is meant to deal with situations like this related ones and even less restrictive ones. We are meant to capture these terms $\sum_{ l = 1}^{\infty}[E(\epsilon_{t} \epsilon_{t-l} x_{t} x_{t-l}') + E(\epsilon_{t} \epsilon_{t-l} x_{t-l}x_{t}') ]$. But there's kind of a new problem over here. The new problem is there's an infinite number of these terms. We only have a finite amount of data, so we probably are not gonna be able to do a good job of estimating an infinite number of terms. Especially when each of these terms sort of represents something about the dependence of $x_{t}\epsilon_{t}$ and $x_{t-l}\epsilon_{t-l}$. That is observations that are $l$ periods apart. If your data has $100$ years of observations in it, it probably doesn't do a good job of estimating dependence across $1$ million years, right? You can't estimate all of these reasonably.\par 
    	 	  But on the other hand, if this is actually a convergent sum, the expectations that are way out in the tail got to be getting smaller. So what Newey and West say is they say all right, I'm gonna estimate this term just like white. And then I'm gonna estimate \emph{some of} these cross terms. I'm gonna promise that \emph{as my sample size $T$ grows, I will estimate more of them}. So eventually I'm gonna be getting it right. The hope is that in a finite sample with a finite amount of $t$. I'll get enough of them to make it reasonable.\par 
    	 	  \textbf{How do we estimate $\Omega$?}\par 
    	 	  \begin{enumerate}
    	 	  	\item \textbf{(Choose a lag length.)} That means we can't estimate all infinity values of $l$. We're only going to estimate $l$ as it goes from $1$ to some value $G$. That's gonna be the maximum value of $l$ that we will consider. And we're gonna assume that this lag length grows approximately with $T^{\alpha}$ for $0 < \alpha < 1/4$. That is, $G = O(T^{\alpha})$. 
    	 	  	\item \textbf{(Form estimators for each of these terms that we're going to estimate.)} Form 
    	 	  	\[	\hat{\Gamma}_{l} = \frac{1}{T}\sum_{t=l+1}^{T}\hat{\epsilon_{t}}\hat{\epsilon_{t-l}}x_{t}x_{t-l}'	\]
    	 	  	where $\hat{\epsilon_{t}} = y_{t} - x_{t}' \hat{\beta}_{OLS}$.
    	 	  	\item \textbf{(Form Newey-West Variance Estimation.)} 
    	 	  	\[	\hat{\Omega}_{NW} = \hat{\Gamma}_{0} + \sum_{ l = 1}^{G-1} (\frac{G-l}{G})(\hat{\Gamma}_{l} + \hat{\Gamma}_{l}').	\]
    	 	  	Here's how they say to estimate. You take your estimate of each term, for each $l$ up to $G-1$, you estimate this expectation in step (2) and add the terms in, but you down-weight it by a factor $(\frac{G-l}{G})$ which is depending on the upper bound of $l$. I'm not gonna try to justify it here. You can kind of derive it, but I'm not gonna talk about that. I will tell you why they do it, though. So this is the estimator. \emph{Notice that if you just have $\hat{\Gamma}_{0} $, this is Whites estimator and we already had that.} 
    	 	  \end{enumerate}\par 
     	  	$\sum_{ l = 1}^{G-1} (\frac{G-l}{G})(\hat{\Gamma}_{l} + \hat{\Gamma}_{l}')$ are the new terms. They deal with these additional terms over here (they are the second parenthesis terms). The hope is that as $G$ grows, they would do a better and better job of estimating all these terms.\par 
     	   And they choose that $G$ to be growing at the proportion of $T$  to something less than a quarter. It doesn't even have to be less than one fourth. In practice, what do people do? They use two or three. If they have seasonality and they have quarterly data, they'll use four just to include a whole year. They think about the economics and they think about how far out that correlation they think might be significant. So it kind of depends on the frequency with which your observations are drawn. This also works for non time series situations where you just have dependent data, like you could have dependent data observations by state or region, and there could be dependencies and you could be have dependencies on how far away states are from each other, for instance.\par 
     	   But in this idea, what Newey and West show is two things.\par 
			\begin{enumerate}
				\item  \textbf{First, this estimator converges to the right place.}
				\[	\hat{\Omega}_{NW} \stackrel{p}{\longrightarrow} \Omega.	\]
				It's a consistent estimate of $\Omega$ as $T$ grows. That was kind of the objective. You might think that was the only objective. It turns out before them there were other methods that could do this. The thing that was magic about their method and the thing that got people actually use it is the other property. 
				\item \textbf{This estimator is $\hat{\Omega}_{NW}$ is positive semi-definite.}\par 
				What it means is, in essence, that when you go to estimate this matrix, you're not gonna get negative estimates of variances. The problem with the old method is sometimes you estimated a variance and your estimate was negative. We know variances are not negative. And people don't like it when they get negative variance estimates, because they don't know what to do with that. They want to create a standard error. They want to take a square root. They know they can't take a square root. They know it's not right to set it equal to zero that there's no sampling variation. It's just a bad estimate. The key thing here that Newey-West is they created a method that's non-negative.  And actually it's this little weighting thing here $(\frac{G-l}{G})$ that generates the positive semi-definiteness. That's the trick they use. And that gives them an estimator that's positive semi-definite, which means that when you go to pull off variances from the diagonal, at least they won't be negative. Semi-definite means possibly they could be zero but they won't be negative.
			\end{enumerate}
		So when you take Newey-West estimator of $\Omega$, and you put it together with our estimate is of $E(x_{t}x_{t}')^{-1}$ and then you get an estimate of the asymptotic variance of the OLS estimator. That omega piece is the hard piece and that's what happens under Newey-West. Harold will tell you more about the details of that.\par 
		The last one that I'm not actually gonna walk through carefully because Harold is gonna spend time on is clustering. What I'll tell you about clustering is that clustering takes care of a different dependent structure than this strict stationary.\par 
		Clustering is the dependent structure where you've got your dataset, and you kind of broken it up into these clusters where within a cluster you might have some dependence, but across clusters you have independence. That structure of dependence, as it's a mixture of dependence, some observations are dependent on each other in quite general ways. And then some observations are completely independent of each other. 
		And clustering is another way to estimate the sort of thing that corresponds to $\Omega$ in that world. It won't look like this. The $\Omega$ will look different.  You'll be able to assume some have zero covariance, and then for some you'll have to allow quite general forms. And clustering standard errors will allow you to do that.\par
		So that's a quick introduction to robust standard errors.
		
\section{Endogeneity}
	 	We will talk about some different situations that lead to what we'll call endogeneity. It is a failure of our OLS $2'$ or SC $2'$ assumption. In particular, that is when $E(x_{t}\epsilon_{t}) \ne 0$. How might that happen? Let's talk about a couple of examples where that assumption could fail. And then we'll talk about what we should do with that fails.  As a general proposition, that assumption could fail quite frequently in economic applications.
\subsection{Omitted Variable}
	 	One reason for having an endogeneity problem is if you're missing variables. \par 
	 	I'm gonna take my right hand side independent variables and partition them into two sets of variables, $x$'s and $w$'s. 
	 	\[	y_{i} = x_{i}' \beta + w_{i}' \gamma + \epsilon_{i}.	\]
	 	The reason I do that is I'm going to assume eventually that I observe $y_{i}$'s and the $x_{i}$'s, but I don't get to see the $w_{i}$'s. But before I do that,  let's pretend there's a world where maybe I have that $w_{i}$'s. This is what I want to do. I'd like to run the regression of $y$ on $x$ and $w$.\par 
	 	I will assume that, just for simplicity,  we've got i.i.d. random variables. I'm going to make an assumption about the finiteness and non-singularity only of $x_{i}$ because I'm only getting observations of $x_{i}$ in the end. And I'm going to assume that $x$ and $w$ are both orthogonal to $\epsilon$ in the OLS $2'$ setting. So we have 
	 	\begin{enumerate}
	 		\item \textbf{(OLS $0$) $(y_{i}, x_{i}, w_{i})$ is an i.i.d. sequence;}
	 		\item \textbf{(OLS $1$) $E(x_{i}x_{i}')$ is finite and nonsingular;}
	 		\item \textbf{(OLS $2'$) $E(x_{i}\epsilon_{i}) = 0$ and $E(w_{i}\epsilon_{i}) = 0$.}
	 	\end{enumerate} 
 		That's our setup. This is a set-up where if I do least squares regression of $y$ on $x$ and $w$ get consistent estimates of $\theta$ and $\gamma$. In general,  if I could run this OLS regression of $y$ on $x$ and $w$, I would do it. But sometimes there are cases where I don't get the $w$. So $w_{i}$ is the omitted variable. And so instead I end up having to run a regression of $y$ on $x$ omitting $w$. What do I get?\par 
 		Here's the definition of beta hat in ordinary least squares:
 		\[	\begin{aligned}
 				\hat{\beta}_{OLS} &= (\frac{1}{n} \sum_{i=1}^{n}x_{i}x_{i}')^{-1} \frac{1}{n}\sum_{i=1}^{n} x_{i}y_{i} \\
 				&=(\frac{1}{n} \sum_{i=1}^{n}x_{i}x_{i}')^{-1} \frac{1}{n}\sum_{i=1}^{n} x_{i}(x_{i}' \beta + \underbrace{w_{i}' \gamma + \epsilon_{i}}_{u_{i}}) \\
 				&= (\frac{1}{n} \sum_{i=1}^{n}x_{i}x_{i}')^{-1} \frac{1}{n}\sum_{i=1}^{n} x_{i}x_{i}' \beta + (\frac{1}{n} \sum_{i=1}^{n}x_{i}x_{i}')^{-1}\frac{1}{n}\sum_{i=1}^{n} x_{i}(w_{i}' \gamma + \epsilon_{i}) &&\textbf{(First term cancelled.)} 	\\
 				&= \beta + (\frac{1}{n} \sum_{i=1}^{n}x_{i}x_{i}')^{-1}[(\frac{1}{n}\sum_{i=1}^{n} x_{i}w_{i}')\gamma + \frac{1}{n}\sum_{i=1}^{n}x_{i}\epsilon_{i}]. 
 		\end{aligned}
 		\]
 		Notice that I don't observe $w_{i}$ and $\epsilon_{i}$, so the truth is we can now call $u_{i}$ being our error term.\par 
 		Now, apply the WLLN, we have 
 		\[		\hat{\beta}_{OLS}  \stackrel{p}{\longrightarrow} \beta + E(x_{i}x_{i}')^{-1} E(x_{i}w_{i}')\gamma + E(x_{i}x_{i}')^{-1} E(x_{i}\epsilon_{i}) = \beta + E(x_{i}x_{i}')^{-1} E(x_{i}w_{i}')\gamma  	\]
 		since $E(x_{i}\epsilon_{i}) = 0$ by OLS $2'$.\par 
 		But we would like $\hat{\beta}$ to be estimating $\beta$. This term here $ E(x_{i}x_{i}')^{-1} E(x_{i}w_{i}')\gamma $ is usually referred to as \textbf{the omitted variables bias}. It's not exactly what we typically mean by a bias. But it's the reason we don't have consistency potentially. If this term is not equal to zero,then OLS is not consistent for $\beta$. If it is equal to zero, we're consistent with $\beta$. We just have to look at this term and see when it's zero and when it is not.\par 
 		This first part $E(x_{i}x_{i}')^{-1}$ is a nice invertible matrix. That's non-zero. So the omitted variable bias is going to be zero if and only if $\gamma$ is equal to zero or $E(x_{i}w_{i}')$ is zero.\par 
 		Think about the example we'll put here. In our example. What is $\gamma$? $\gamma$ is the coefficient on ability in describing log wages. So the question would be is $\gamma$ equal to zero or not. In general, we would expect that it would not be equal to zero. In general, we imagine whatever ability is, it is probably correlated with wages. We should call it spunkyness. How spunky you are.\par 
 		So we look at the second possible way in which we don't have omitted variable bias is looking at $E(x_{i}w_{i}')$. It is not exactly the covariance and usually people kind of think about it as the covariance. Could kind of refer to that as correlation. Would that expectation be zero? Think about what's in $x$ and what's in $w$. In $x$ we have $1, edu, exper$ and in $w$ we have $ability$. The key piece that people are typically worry about is that education and ability would be correlated with each other and that would cause the expectation to be non zero.\par 
 		 In this example of log wages with missing ability, we would expect both of these conditions to fail, so we'd be worried about it. Therefore, the ordinary least squares estimator may not be a consistent estimator for the coefficient on education, experience and intersect. We could have some limited variables bias.\par 
 		 So when that happens, we call that an endogeneity problem. The reason we call it an endogeneity problem is that it kind of looks like our OLS $2'$ assumptions are holding. But in fact these are kind of the wrong OLS 2' assumptions. The reason for that is the regression we're running is not a regression of $y$ on $x$ and $w$ it's actually just the OLS 2' assumption for the regression of $y$ on $x$. So the orthogonality that would be needed for this OLS to work is the orthogonality between $x$ and $u$, i.e., $E(x_{i}u_{i}') = 0$, not $x$ and $\epsilon$ since $w$ is now in the error term too. So we could ask the question does is $x$ is orthogonal to $u$? There is a potential issue because we know that OLS is not necessarily consistent. If we look at $E(x_{i}'u_{i})$, we have 
 		 \[	E(x_{i}u_{i}')  = E(x_{i}(w_{i}\gamma + \epsilon_{i})^{'}) = E(x_{i}w_{i}’\gamma +x_{i} \epsilon_{i}) = E(x_{i}w_{i}')\gamma + E(x_{i} \epsilon_{i}) =  \gamma E(x_{i}w_{i}').	\]
 		 It is the same thing that will determine the omitted variable bias. So when we have omitted variables bias, this is not equal to zero. Then we've got a failure of the orthogonality condition, i.e., $E(x_{i}u_{i}') \ne 0$. If we have no omitted variables bias, then we would say that we have orthogonality and we can use OLS.  That's an example where failure of OLS $2'$ can occur. A common situation.
\subsection{Simultaneity} 
 		 Let's consider a different one.\par 
 		 Levitt(1997). So we will use this paper to talk about the motivation for why there can be endogeneity.  What Levitt was thinking about in his paper is the effect of increasing the police force on crime. The idea is to try to understand how much the marginal addition of a police officer helps lower crime.\par 
 		 How much should I increase my police force to lower crime. Here are our variables. $c_{it}$, $p_{it}$ where $i = city$ and $t = year$. The regression that people were essentially running was this one. 
 		 \[	c_{it} = \alpha_{0} + \alpha_{1}p_{it} + \alpha_{2}p_{i,t-1} + \alpha_{3}'x_{it} + \epsilon_{it}.	\]
 		 The regression that corresponds to this equation. Doing ordinary least square regression of crime on intercept, the number of police in a city in some year and the number of police in the city in the year before, just because there might be some lag influence of the number of police on crime rates and a bunch of other things characteristics of the city in $x_{it}$.\par 
 		 They were interested in this sort of $\alpha_{1}$ and $\alpha_{2}$ or maybe the sum of those two things. What they're interested in is if I increase police today by $10$ percent, how much will it decrease crime? How negative is $\alpha_{1}$ and $\alpha_{2}$. But here's what they found. When they estimate, these were positive. So what's going on there? They're looking at crime statistics across different cities. At least in the united states using the data from 70s and 80s. I'm sure this has been done more recently. It turns out that smaller cities like Madison don't have that much crime, at least compared to larger cities like New York city or something, and also have much less police even in a proportional sense. So cities like Madison have low police, low crime, cities like New York, maybe high police, high crime. So when you go to run the regression in level terms, you actually get a positive correlation. More police looks like leads to more crime because of that variation from city to city.\par 
 		 So an issue in that literature was how to deal with something that they knew was sort of problematic. Somehow that didn't represent their intuition. Here is the idea.\par 
 		 When you think about this equation, think about it as a crime production function. It is resulting from a bunch of individual decisions about committing crimes. The criminals are making these decisions and they're making these decisions based on the number of police that are out there. Presumably more police makes them less more likely to get caught and less likely to try to commit a crime. So more police should result in lower crime. And all sorts of other stuff about these individuals, their personal circumstances.\par 
 		 Then, think about another equation:
 		 \[	 p_{it} = \beta_{0} + \beta_{1}c_{it} + \beta_{2}c_{i,t-1} + \beta_{3}' w_{it} + u_{it}.	\]
 		 It's a different equation that's the political process for how do we end up with the number police. How many police are there in a city? Well, that's largely a political question and that's something that's determined by local governments and state governments for most for the most problem. Well, presumably it could depend on all sorts of things. One thing that it might depend on is the current and lag crime rate. If there's more crime, maybe a city will move more resources into producing or hiring more police. It might depend on all sorts of other things, such as the budgets and whatever else.\par 
 		 You can kind of think about there being two equations. One is the crime production equation and the other is the police production equation. And where we end up with the amount of crime and police is sort of the intersection of these two equations. It's very analogous to demand and supply.\par 
 		 In the crime production function. We expect $\alpha_{1}$ and $\alpha_{2}$ to be negative. But in the police production, we actually expect more crime to lead to more police. So we expect $\beta_{1}$ and $\beta_{2}$ to be positive. They are kind of working against each other, kind of like demand and supply, right? Having completely different slopes. And so the idea is that if we just pretend like the crime production function is the only equation, then probably we will not get consistent estimates of $\alpha_{1}$ and $\alpha_{2}$. We want know whether the number of police is correlated or is orthogonal to $\epsilon_{it}$.  Is it?\par 
 		 \emph{I know that crime is certainly not orthogonal to $\epsilon$, it has a direct influence on crime by the first equation. And I know the crime has a direct influence on police by the police production production. So that means very likely $\epsilon$ has a indirect influence on police by influencing $c_{it}$ first directly and then $p_{it}$ through the second equation. That means police, the number of police is actually probably correlated with $\epsilon$. That's the simultaneity problem.} \par 
 		 Think about when you have the demand and supply curve, and if you wanted to estimate the demand and supply curve, it's hard. The reason is what you observe is a price and a quantity at the intersection of the two curves. Maybe you move to a different market or a different time period. And you see another intersection and another intersection and another intersection. See another price and a quantity, each one sort of represents this intersection of demand and supply. So you just see all of these intersections. It's hard to just trace out. What does work actually, it's kind of if you could hold demand fixed and have supply moving around. Then you could figure out demand. So you have to think about things that might move supply, but are sort of orthogonal to demand. Those are things that we usually refer to as things like instruments.\par 
 		 In this case, Levitt had to think about things that maybe are sitting in here that move the police production function around to be able to trace out what the crime production function is. He's really trying to think, are there things in the police production equation but are not in the crime production function. Are the things that determine the number of police in this police production, political process, whatever calling it, that are not part of the crime production process. That's the hope. That's the challenge. So that's why you had a cool paper, although It had some problems.\par 
 		 So what I'm saying is OLS $2'$ can fail. $E(x_{i}'\epsilon_{i}) \ne 0$ because of this simultaneity , the fact that there's another equation that in which crime is having the reverse and another influence directly on police.
\subsection{Instrumental Variable}
 		 What do we do in these situations? Just to be clear.  What is the problem? The problem is a failure of OLS $2'$. If assumption fails, ordinary least square regression of $y$ on $x$ may not be consistent, may not have good properties at all, so it may not be useful.  Now we got to go to something else. We can't use ordinary least square.\par 
 		 And there's lots of possible fixes to this problem. What we do is we go to our bag of tricks. And one of the main tricks for economists is \textbf{instrumental variables}. It is especially fun to talk about it this year since this is the topic that won a Nobel prize this year, pretty excited.  It's a big leap from the OLS set-up in the sense that it involves new variables.\par 
 		 What we're going to assume is that we're still interested in this crime production equation and we're gonna assume that OLS $2'$ fails. We're going to look for another variable or set of collection of variables that have a couple of properties. The $x$ is not orthogonal to $\epsilon$, so we're gonna look for variables that are orthogonal to $\epsilon$ and have relation with $x$. This kind of seems abstract at the moment, but we'll talk about an actual applications here.\par 
 		Levitt was saying this is my setup for crime and police, I would like to estimate the $\alpha_{1}$ and $\alpha_{2}$ here. But i'm worried that police are correlated with this $\epsilon$. So I need some variable that's uncorrelated with this $\epsilon$, but is correlated with the police. So he looks in the police production function. Whatever these $w$'s are, as long as their coefficients $\beta$ are not zero, they are definitely going to affect police (thus the correlation requirement is satisfied). But he has to find something that's uncorrelated with $\epsilon$. What is $\epsilon$? $\epsilon$ is everything that's left over in that crime production function. So crime is explained by all sorts of things, number of police and some characteristics potentially of the city. Then all the other stuff that goes into that crime production. So it's kind of hard to think about something that has a direct effect on police, but somehow is orthogonal to the error in this crime production function.\par 
 		 His idea was to use the timing of elections, mayoral elections and gubernatorial elections, governors of state. The idea is that crime is an issue during elections, at least in the United States. Most of the time, what you find is that politicians, when they want to be elected, they want to appear as politicians who care about crime, they're doing something about crime. So what do they do when they're about to have an election they increase the number of police. And that's their show of being tough on crime and try to get re-elected. The idea is if you use a variable that kept track of when elections are, the years in which there are elections, there tend to be increases in the number of police. But does the election year have a direct influence on crime production? He has to argue that it doesn't. It's orthogonal to all these other factors that affect crime, that a criminal making a decision about whether to commit a crime doesn't really care whether it's in the election year or not. That's not part of their production function. \par 
 		 That's the idea in behind. Finding a variable. It's outside of this regression we want to run, and it has this property of orthogonality, but also correlated with the endogenous variable that we're trying to fix. That's the idea behind instrumental variables methods. We're going find variables like that, then help us to deal with the endogeneity by themselves being exogenous or having this orthogonality and  themselves having some explanatory power towards the variable $x$ it is designed for.

 		 \textbf{(Instrumental Variables)} 
 		 \[	y_{i} = x_{i}' \beta + \epsilon_{i}, \qquad i= 1,2,3, \dots, n.	\]
 		 \begin{enumerate}
 		 	\item \textbf{(IV $0$)} $(y_{i}, x_{i}, z_{i})$ i.i.d. and sufficient moment existence;\par 
 		 	Just for simplicity
 		 	\item \textbf{(IV $1$)} $E(z_{i}z_{i}')$ is finite and non-singular, $E(z_{i}x_{i}')$ is full column rank;\par 
 		 	New assumption: $E(z_{i}x_{i}')$ is full column rank.
 		 	\item \textbf{(IV $2$)} $E(\epsilon_{i} | z_{i}) = 0$;
 		 		\item \textbf{(IV $2'$)} $E( z_{i}\epsilon_{i}) = 0$;
 		 	\item \textbf{(IV $3$)} $E(\epsilon_{i}^{2}z_{i}z_{i}')$ is finite;
 		 \end{enumerate}
 	 So these are these are going to be our IV assumptions, and they're very parallel to what we had with the OLS assumptions. $z$ is for the most part going to play the role of what $x$ played before. In particular, $z$ is going to be orthogonal to $\epsilon_{i}$. And they are going be some variables that are outside of this regression. Actually they can overlap with some of the $x$'s, but not all of them, so at least somewhat different. We call these $z$'s the \textbf{instrumental variables}.\par 
 	 Notice that $x_{i}$ is $k-$by$-1$ and $\epsilon_{i}$ is a scalar, so $x_{i}\epsilon_{i}$ is a  $k-$by$-1$ vector. When we say this expectation is not equal to zero. All we mean is that at least one element is non zero, then the whole vector is considered not equal to zero. There might be some elements that are that are orthogonal. There might be some of the $k$ elements of $x$ that are orthogonal. And those we can include in our instruments $z_{i}$ because they're orthogonal. They satisfy this condition. They are going to play a rule when we talk about weak IV. But if there's at least one that element that has the expectation of $x_{i}$ times $\epsilon$ not equal to zero then our OLS orthogonality fails. And we have to do something else. \par 
 	 ``$E(z_{i}x_{i}')$ is full column rank;'' is new and we'll come back to why we need this. It is a sort of interaction, if you will, between $z$ and $x$ and basically the underlying idea is going to be that we're gonna have to make sure that somehow, these two are related, there's some correlation between them. There's a more formal notion that has to be fulfilled here, but just as an example of what full column rank would imply. One thing that it would imply, but not sufficient for full column rankness would be that the number of variables in $z$ is at least as large as the number of variables in $x$. That is, if $k$ is the dimension of $x$ and $l$ is the dimension of $z$, then we must have $l \geq k$.  You've got to have at least as many instruments as you've got variables in $x$.  This $E(z_{i}x_{i}')$ full column rank implies more than that. But that's one of the things that implies. \par 
 	 They're gonna put you in good shape. So just to show you why IV is gonna help you, look at the IV 2' assumption. Remember, just like with the OLS, IV 2 is a stronger assumption than IV 2'.  If we assume IV 2, we know IV 2' holds. We only assume IV 2' here. 
 	 Notice that we have
 	 \[	0 = E(z_{i}\epsilon_{i}) = E[z_{i}(y_{i}-x^{'}_{i} \beta)] = E(z_{i}y_{i}) - E(z_{i}x^{'}_{i}) \beta.	\] 
 	 So IV 2' is the same thing as saying 
 	 \[	E(z_{i}y_{i}) =  E(z_{i}x^{'}_{i}) \beta.	\] 
	\emph{Why is that so crucial to to saying that we can use this data on $y$ and $x$ and $z$ to learn about $\beta$?} What does that mean? There's a condition. Sometimes we talk about consistency. We talk about asymptotic morality. But before we talk about any of those things, there's a notion, a concept called \textbf{identification}. So what is identification? If I have enough information from this data on $y$ and $x$ and $z$ then can I learn what $\beta$ is. What do I mean by enough information? Suppose that my dataset was not sample size $n$. Suppose it was infinite. Suppose in fact that I knew the distributions of $y$ and $x$ and $z$ and then I can figured out $\beta$. I knew that much information about $y$ and $x$ if I have these observable variables and I know their joint distribution. Could I figure out $\beta$? That's identification. If you can, then $\beta$ is identified, if you can't, then $\beta$ is not identified. That means even if you had all the information from the state in the world, it wouldn't help you. Right? So you need identification as a starting point.  Could we figure out $\beta$ here? If we knew the distributions of $y$ and $x$ and $z$s, if we knew their joint distribution, then we could compute this expectation. Notice that we have $z_{i}y_{i}$ a $l-$by$-1$ matrix, $z_{i}x^{'}_{i}$ a $l-$by$-k$ matrix and $\beta$ be a $k-$by$-1$ matrix. So we would have $l$ equation, each row corresponds to a different equation and $k$ unknowns. Could we figure out what $\beta$ is equal to? What does this new assumption say? It says that this matrix $z_{i}x^{'}_{i}$ is full column rank. Full column rank is kind is basically saying, yes, this system of equations has enough information to figure out what $\beta$ is.  Basically, in the case, when think about this, when $l$ is equal to $k$ this becomes a square matrix. And can we solve for beta? This linear system of linear equations has a unique solution as long as this $E(z_{i}x^{'}_{i}) $ is invertible. And when the dimension of $z$ is equal to the dimension of $x$, full column rank is the same thing as being invertible. When $z$ has a dimension $l$ that's greater than $k$, the full column rank condition just ensures that there's enough variation to have $k$ linearly independent columns. And that's enough to get us being able to solve for $\beta$. Actually if $l$ is greater than $k$, then we could actually throw out some of there of the $l$ variables. And we get back to a square matrix. And we could use it to solve for $\beta$. \par 
	Basically, IV 2', combined with this new assumption in IV 1(b),  tells us that there's a solution for $\beta$. It is identified.  It's just a linear system of equations, $l$ equations and $k$ unknowns. And because of that, we refer to these two cases differently. When $l$ is equal to $k$ we usually call that the \textbf{just identified case}. That's just enough information to possibly identify what $\beta$ is. It is to figure out $k$ equations with $k$ unknowns, right?  If  $l$ is greater than $k$ , then more equations than unknowns. We have more information than we need to solve. We have the ability to possibly throw away some of the information and still solve them. So that's referred to as the \textbf{over-identified case}. That's the terminology that gets used to.
\subsection{Two Stage Least Squares Estimation}
	So that gives us some hope that these assumptions are going to be sufficient to get information about this parameter $\beta$ that we're interested in. That wasn't necessarily obvious but it turns out good. \par 
	If I was given a data set of $n$ observations, and I have observations on $y$ and $x$ and $z$, how could I use them to learn about $\beta$? And then we want to show that estimate or estimator has nice properties like consistency and asymptotic normality. \par 
	There's a famous estimator called the \textbf{Two Stage Least Squares Estimator}. And basically, this is how it works.
		\[ \textbf{(structual equation)} \qquad	y_{i} = x^{'}_{i} \beta + \epsilon_{i},	\]
		\begin{equation}\label{1st stage equation}
			\textbf{(1st stage equation)} \qquad x^{'}_{i} = z^{'}_{i} \pi + v^{'}_{i}
		\end{equation}
		where $ \quad i = 1,2,3, \dots,n.$.
	Here's the intuition about what's going on . This can be a little fuzzy, but we'll be formal in a second. $x$ varies in a way that makes it hard to figure out how variation in $x$ is going to, how exogenous variation $x$ is gonna influence $y$ because of the endogeneity problem. That creates an issue that $\epsilon$ sort of tends to move systematically  with $x$ and make it hard to figure out what $\beta$ is. That's the intuition. What we'd like to do is we'd like to decompose the variation in $x$ into two pieces. We'd like to decompose it into a part that is sort of explained by $z$, this new instrument variable and everything else. So the hope is here in the second equation.  There's this part, $z' \pi$. That's the variation that is explained by $z$ and this $u^{'}_{i}$ is just everything else. Now, the variation in $x$ that comes from the variation at $z$ which is the good part since $z$ is orthogonal to $\epsilon$. So that's the part of the variation in $x$ that's exogenous to $\epsilon$. Then all the parts of variations of $x$ that causes the endogeneity problem are dumped into the $v_{i}$ part. We'd like to just use that good piece of the variation and leave the rest aside. But we can't, because we don't know $\pi$. What is $\pi$? $\pi$ is what you get when you just project $x$ onto $z$ in a linear way. It's the coefficient from the least squares projection. You estimate this first stage equation (\ref{1st stage equation}), which is a new linear equation and get $\hat{\pi}_{OLS}$. And  if we want to define $\pi$, we can define $\pi$ by the orthogonality condition for $z$ and $v$ here. \par 
	Let's look at our dimensions here. $x^{'}$ is $1-$by$-k$. $z'$ is $1-$by$-l$, and that must mean $\pi$ is $l-$by$-k$ and $v$ is $k-$by$-1$.\par 
	 So if we have $0 = E(z_{i}v^{'}_{i})$ (analogous to OLS 2', \emph{notice that this assumption is NOT in IV assumptions}), then that would in essence define $\pi$. We've done this before. This is now just a least squares. This is a collection of least squares equations since now we have a $1-$by$-k$ vector on the left side of the regression equation. \newline
	 We have 
	 \[	0 = E(z_{i}v^{'}_{i}) = E(z_{i}(x^{'}_{i}-z^{'}_{i}\pi)) = E(z_{i}x^{'}_{i}) - E(z_{i}z^{'}_{i})\pi,	\]
	 which is 
	 \[	E(z_{i}x^{'}_{i}) = E(z_{i}z^{'}_{i})\pi.	\]
	 If $E(z_{i}z^{'}_{i})$ is nonsingular(imposed by IV 1), then we have 
	 \[		\pi = E(z_{i}z^{'}_{i})^{-1}E(z_{i}x^{'}_{i}).	\]
	 So making this assumption $0 = E(z_{i}v^{'}_{i})$, the OLS $2'$ assumption. We kind of argued that it's kind of an assumption but it's also kind of a definition. It's really defining what $\pi$ is. So when we say this is true, we're defining $\pi$ to be this guy. How do we estimate that guy? Ordinary least squares regression of $x$ on $z$. What we're going to do here is we know we can't regress $y$ on $x$, but what we'd like to do is regress $y$ on the part of $x$ that is explained by $z$. In essence, we'd like to regress it on $z' \pi$.  But we don't know $\pi$. Well, go and estimate $\pi$ and then regress it on $z' \hat{\pi}$. That's what we do, and that's called \textbf{two stage least square}. Let us be a bit more formal here.
	 \begin{enumerate}
	 	\item \textbf{The first stage} is OLS of $x_{i}^{'}$ on $z_{i}^{'}$, it gives me $\hat{\pi}$, which is equal to 
	 	\begin{equation}\label{1stage, pi hat}
	 		\hat{\pi} = \left(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i}\right)^{-1} \frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i}.
	 	\end{equation}
	 	So this is actually, it's really $k$ different equations here. And I'm just doing OLS on each one. I can put them all together if I want to. Or I can do them one by one, for each variable in $x_{i}$, $\dots$. But this just stacks them together and puts it all into one expression. 
	 	\item After we've got $\hat{\pi}$. We're going to form $z_{i}' \hat{\pi}$, and call it $\hat{x_{i}}'$:
	 	\[	\hat{x_{i}}' = z_{i}^{'} \hat{\pi}.	\] 
	 	\item In the second stage. I'm going to do OLS of $y_{i}$ on $\hat{x_{i}}$. That's going to give me $\hat{\beta}_{2SLS}$. Just use the least squares formulas, we have
	 	\[	\begin{aligned}
		 	\hat{\beta}_{2SLS} 	&=\left(\frac{1}{n}\sum_{i=1}^{n}\hat{x_{i}}\hat{x_{i}}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}\hat{x_{i}}y_{i} \\
		 	&= \left(\frac{1}{n}\sum_{i=1}^{n}\hat{\pi}'z_{i} z_{i}^{'} \hat{\pi}\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}\hat{\pi}'z_{i}y_{i} \\
		 	&=  \left(\hat{\pi}'(\frac{1}{n}\sum_{i=1}^{n}z_{i} z_{i}^{'}) \hat{\pi}\right)^{-1}\hat{\pi}'(\frac{1}{n}\sum_{i=1}^{n}z_{i}y_{i}) &&\text{[replace  $ \hat{\pi}$ from (\ref{1stage, pi hat})]} \\
		 	&= \left[(\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i})\right]^{-1} (\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}\frac{1}{n}\sum_{i=1}^{n}z_{i}y_{i}
	 	\end{aligned}
	 	\]
	 	That's the expression rewritten in terms of their original variables, $y$, $x$ and $z$ and that's the original data you got. This is the formula that you can plug in your software, or your software has internalized that. One thing to just quickly notice is that this is the formula for two states least squares. It's okay, conceptually to think about it as a two stage thing. First, you do this, then you do this, but you don't have to.  It's really just like anything. Any estimator is a transformation of your data. We can put both those stages all into one big transformation, here it is. It is not necessary to only think of it through these two states. Just pointing that out. 
	 \end{enumerate}
 		So that's our estimator. And it was sort of motivated by some intuition about decomposing variation of $x$ into the piece that has to do is explained by $z$ and everything else.  But now we have to ask the question, is it a good estimate? Was that intuition correct? 
 	\subsubsection{Consistency}
 		\emph{The first thing we'll think about is consistency.} That means as the sample size grows large, does this estimator converge in probability to $\beta$ or not. And you can kind of see from the formula. The things might look good. What I mean by that is, if I look at this formula, six terms, but look at the last term, $\frac{1}{n}\sum_{i=1}^{n}z_{i}y_{i}$. What is $y_{i}$? $y_{i}$ is just $x^{'}_{i} \beta + \epsilon_{i}$. For a moment, for just one second, think about what would happen if instead of $y_{i}$, we had $x' \beta$ in there? If I replaced $y_{i}$ by $x^{'}_{i} \beta$, what would happen? We have the three terms in the square brackets nearly identical to the later three terms except for the $\beta$. There's an inverse here of the square brackets. They would all cancel, and I would just end up with $\beta$. That's probably a good sign that if I plug in $x' \beta$ that I would get $\beta$. That's probably a good thing. I don't have $x' \beta$ unfortunately. If I had beta, we wouldn't be doing any of this.  But I have $y_{i}$ which is $x' \beta + \epsilon_{i}$. If I substitute in for $x' \beta + \epsilon_{i}$, what I get is, the first term is $\beta$. The $\beta$ we just looked at. That is, 
 		\[	=	\beta + \left[(\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i})\right]^{-1} (\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}.	\]
 		But the thing is I want to show consistency. That means I need to show that this big expression converges in probability to $\beta$. That is, to show the later big expression converges in probability to zero. There are a lot of stuffs going on here, but look at the last term, if the weak law of large numbers applies, this will converge in probability to its expectation. That is, $E(z_{i}\epsilon_{i})$, which is zero by IV 2'. That's the key assumption. That's the whole thing about $z$ and that's what makes $z$ valuable.\par 
 		How do we get consistency? This goes to zero, and everything else just has to be nice. We're just multiplying together six different things. One of them goes to zero. As long as the other ones don't shoot off to infinity, they just behave nicely, we will have consistency. They behave nicely. There are five terms here, but you can see it's really only two terms, there's the $x_{i}z^{'}_{i}$, then there's the transpose $z_{i}x^{'}_{i}$ and the $z_{i}z_{i}'$. Their expectations exist by IV 1. So we can apply a weak law of large numbers and say, they converge in probability to their expectations. So we have 
 	\[	(\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i}) \stackrel{p}{\longrightarrow} E(x_{i}z^{'}_{i}),	\]
 	\[	(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1} \stackrel{p}{\longrightarrow} E(z_{i}z^{'}_{i})^{-1},	\]
 	\[	(\frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i})  \stackrel{p}{\longrightarrow} E(z_{i}x^{'}_{i}).	\]
 	Does the whole thing $\left[(\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i})\right]^{-1}$ converts to the inverse of the product of expectations? It does. Why? Because what we've got in the middle is non-singular. And the right side matrix $E(z_{i}x^{'}_{i})$ is full column rank, which means the left side matrix $E(x_{i}z^{'}_{i})$ is full row rank. And that full column rankness ensures that what we end up with this whole thing being invertible. You put it all together and you've got $0$ times nice stuff, and that gives you zero. That's consistency. So  IV 0, 1, and 2' give us consistency.\par 
 	 So that is a good estimator. In that sense, that's a good property. The property that says you can get enough data. Eventually, our estimator will be distributed, right in a teeny little neighborhood around $\beta$. That's good.
\subsubsection{Asymptotic Normality}
 	 Now we want to show asymptotic normality. To show this, we will start with 
 	 \[	\begin{aligned}
	 	  \hat{\beta}_{2SLS}  &=	\beta + \left[(\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i})\right]^{-1} (\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}.
 	 \end{aligned}
 		\]
 		Which gives us 
 			 \[	\begin{aligned}
 		\hat{\beta}_{2SLS} - \beta &= \left[(\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i})\right]^{-1} (\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}.
 		\end{aligned}
 		\]
 		I know that $\hat{\beta}_{2SLS} - \beta $ converges in probability to zero. It's not normal. But if I multiply by the square root of $n$, we get 
 			 \[	\begin{aligned}
 		\sqrt{n}(\hat{\beta}_{2SLS} - \beta) &= \left[(\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}x^{'}_{i})\right]^{-1} (\frac{1}{n}\sum_{i=1}^{n}x_{i}z^{'}_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}z^{'}_{i})^{-1}\textcolor{blue}{\frac{1}{\sqrt{n}}}\sum_{i=1}^{n}z_{i}\epsilon_{i}.
 		\end{aligned}
 		\]
 		So when we want asymptotic normality, we look at $\sqrt{n}(\hat{\beta}_{2SLS} - \beta) $. And for that product of these six term, they multiplies by the square root of $n$. We only have to do that one place. It turns out it's good to multiply the term that goes to zero by the square root of $n$. All the other terms, nothing changes. They still converge in probability to their probability limits. Is just this one term, the final term. This is the critical one. \par 
 		Now, this is gonna obey a central limit theorem. Not a weak law large numbers. It's not gonna converge in probability. It's gonna converge in distribution. And it's gonna converge to a normal distribution with mean zero since $E(z_{i}\epsilon_{i}) = 0$ and some variance. And in the i.i.d. cases, the variance of this just the variance of the term in the sum, which is to say, $Var(z_{i}\epsilon_{i})$. They all have the same variance. And that's actually equal to $E(\epsilon_{i}^{2}z_{i}z_{i}')$ by lemma (\ref{lemma cov}), which is exactly what IV 3 says is finite. So we can apply the central limit there. The variance exists. So everything is nice. I was just about to put all those together. I was just about thinking I was gonna write it there. And now I realize I'd better not. It's actually a pretty big expression.  The reason it's a big expression is that we get five terms multiplying a normal. And when you just take a normal, you take linear combinations of a multivariate normal. It's just a new normal distribution. They are just rescaling a normal.  What is the new normal? You take whatever you multiply by, and you just multiply the mean by that. So in this case, multiply all these by zero, and you'll get zero. It'll be a normal with mean zero. What will be the variance? We kind of have to square all the things that you're multiplying by to get the new variance. But in matrix terms that's pre- and pose- multiplication, you're gonna take this variance. See the lemma (\ref{the matrix world constant}).\par 
 		Rather than writing it on the chalk board, I will count the the terms here in the slide.  Let's not make too big of a deal about it. \emph{But you can cancel out most of them in the just identified case, but cannot cancel them in the over identified case.} The point is you get asymptotic normality. And just like with OLS but it's a bigger uglier expression than OLS. The thing about these 11 terms is that they're easy terms to estimate. Just like with OLS there's a middle term there. That's the hard one, because it involves $\epsilon$. An $\epsilon$ is not a variable that we observe directly. We don't get data on $\epsilon$. That's the only one that's tricky. All these other ones. We know how to estimate the expectations (just with averages). It turns out that all the things we learned about robust variance estimation and all that stuff that we talked about with OLS it all will apply in the exactly the same way for IV too. You can apply all of those methods in IV, it's just gonna be that instead of $\hat{\epsilon_{i}}x$ you're going to, have $\hat{\epsilon_{i}}z$, that's it. You're basically trying to estimate the variance of $\epsilon_{i}$. So it works the same way. You can do heteroscastic(we never assumed homoskedasticity yet), non-identical distribution, dependent data, cluster data and all of those good things.\par 
 		What the conclusion here says is that we've got  consistency and asymptotic normality. And the asymptotic normality holds under IV 0, 1, 2' and 3 now.\par 
 		 One thing I really want you to do is do not allow the fact that instead of two terms, there are more terms. Now suddenly things are really hard. It is no harder. It's just more terms. And really, there's only one. There's only a key term  $\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}$, that's the thing that's driving the boat here. It's either convergence in probability to zero, or it converges in distribution to a normal. All the other terms are just nice averages that go to nice places, convergent probability. \par 
 		 Those are the results and the properties of two stages least squares under the assumptions that we made beginning. 
\subsection{Levitt(1997)}
 		 Let's go back to our example. We were talking about simultaneity as a situation in which we could end up with endogenity. We'd like to run a regression of crime on police and see what the coefficients on police are. Here, we are thinking about a situation where we're gonna have observations on crime that vary by city and by year and vary by time, in two dimensions. So we're gonna have these police variables. And there's a classic finding here, which is basically that you don't get what you expect. You're sort of expecting to see how increases in the police decreases crime.  But in fact, what we find is that higher crime and higher police sort of go hand in hand. Levitt explains that by saying that's because there's actually two different equations. One is the crime production function and another is police or political production function. But the number of police that are out there could easily depend on how much crime is there. And that was the tricky thing that creates the connection between police and $\epsilon$. That's the endogeneity problem.\par 
 		  Levitt comes along with a very clever idea. Finding an instrument is a tricky thing. We're talking about it in econometrics class, but finding an instrument is not really an econometric exercises.  It really is about the economics. It's about the data you have, and it's about the economics of the problem. Way more than econometrics. The econometrics can kind of tell you what are the properties that have to hold. But that doesn't telling which $z$ to choose. That's not very helpful in a lot of ways.  You really have to think through what is $\epsilon$ representing. What are the variables that I might get observations on that will satisfy orthogonality. Levitt has this idea. This idea comes really from the demand and supply world. You'd like to estimate this equation, the crime production function. Because what we actually observe in the data is where the crime production function intersects the police supply function. The way to do that, is essentially to get variation in the police production function and hold the crime production function, and that will trace out the crime production curve. \par 
 		  Now, it's not possible to make it not move, but what you can do is you can get movements in the other curve that are orthogonal to movements in this curve. That's what you're trying to do. When we look in here in this equation, there's these $w$'s in the police supply function curve. And those end up being the the things that we can see and we're going to observe that move this police supply function curve. Let's think about what's in $w$. He thought of the idea of electoral cycles, the timing of election. It is why this paper immediately became a somewhat famous paper was published in one of the best journals, one of the top line journals, journal political economy, I think.  And the reason for all of that is, to some extent, because of the numbers that he finds in the empirical results. But a lot of it is because of the idea behind the instrument, finding the instrument itself is a really key part of the research. One that's really compelling and that people suddenly go, whoa, I hadn't thought about that, but that really breaks through this endogeneity problem. And I believe the answers. That's what was compelling here.\par 
 		  What are the two properties that an instrument has to satisfy? It's really that second IV 1 property $E(z_{i}x^{'}_{i})$ full column rank for relevance and the IV 2' property $E(z_{i}\epsilon_{i}) = 0$ for orthogonality. Those are the two key ones. Here is how people think about those properties sort of intuitively, informally.  It's not exactly a restatement of the properties. This is not a formal thing, but this is how they think about it. They usually think that orthogonality condition, they refer to that as uncorrelated. So, like okay, I need my instrument to be uncorrelated with $\epsilon$. What is $\epsilon$? $\epsilon$, in this case, are all the determinants of crime that are not observed.  Not on the right side set of variables. Everything else that determines the crime rate in these cities that's not observed. So it's the un-observable drivers of the crime. So would we believe that orthogonality?
 		  We're gonna have that the timing of mayor, mayoral elections, and gubernatorial elections for governors are orthogonal are uncorrelated with these other determinants of crime. We'll have to talk about that. That's one piece. The other property is that full column rankness, that really just gets summarized here in a really informal way, as the following. We need the variation in our instrument to explain something about the variation in the endogenous variables.  The endogenous variable, the ones we're worried about are the police variables. And so the whole the question is, do we believe that elections, the timing of elections has something to do with the number of police? That's something that he can provide evidence about directly. \par 
 		  He says, hey, let's just check it out. Here's what i'm gonna do. I get all this information about the number of police in a given city in a given here. I'm gonna take a log, and then I'm gonna take a first difference.  This basically gives you the  percentage change in the number of police per 10000 person. And he says, I'm gonna look at that variable in the gubernatorial election years. Those are for governors. What's the average?  It is .02 That means that on average, in a gubernatorial election year, there's a 2 percent increase in police forces in his data. How about in mayoral election years? There's a 2 percent increase, on average in mayoral election years. How about in years in which there is no election? zero. That's a first pass. It's not everything and it's not exactly what you want, but it's pretty good first pass. Okay, maybe I believe the story that in election years, the number of police are increased. That's one piece of evidence.\par 
 		  What is the other story before I show you this is the bottom one? The other side of what the thing that an instrument has to satisfy is orthogonality. I would need $w$ denote my instrument by year of election, governor or mayor. I need that to be orthogonal to $\epsilon$. Now, he's got a problem. And that is he can think of things that might affect crime, but could be correlated with the election years. So if mayors wanna be reelected, they increase their number of police. What else do they do to try to get elected. Maybe that affects their budget. They've got to balance budget, so they have to decrease spending in other areas.  Maybe some of those other areas are things that could affect whether people are committing crimes.  It could affect people's socioeconomic circumstances.  You can look back at all the variables he's got, there are lots of things that mayors and governors could do. What he says is ok, here's what I've got to do then.  I need to try to observe as many of those variables as I can and include them up in my $x$'s in the structural equation. So they're not in the unobserved $\epsilon$.  If I think there are variables or aspects of the decision made by a crime that would be influenced by mayors or governors being in the election years, I need to include those things in my regression, my structural equation and I need to observe that. The hope is  what's leftover in the $\epsilon$ is orthogonal with elections.\par 
 		  Let's take a look at what he finds is here's what he's doing in Figure (\ref{F:levitt}). 
 		  	\begin{figure}[hbt]
 		  {\centering \includegraphics[scale=0.34]{levitt}}
 		  \caption{Levitt}\label{F:levitt}
 		  \end{figure}
 		  He's going to regress crime on the number of police, and he's actually gonna regress on police this year and police lag 1 year. He's going to take the two coefficients on police is going add them together. Okay, that's the total effective police. And he's going to report that in the top row. What he does in column number one and column number two is OLS ordinary least squares. We claimed that's probably not a good idea, but nonetheless, that's what people have been doing in these literature all the time. So he wants to first say, is my data like what people in the rest of the literature? Again, do I get the same kinds of numbers people have gotten before? It's not a trick of my data that I'm going to get new results. The first column he's doing is called a level regression. That means he's looking at the level of crime and the level of the police. That just the crime and the police. He does actually everything in logs. The reason it does everything in logs is that then you can interpret these coefficients as elasticity. The changes in logs are like percentage changes, and you get like a ratio of percentages to get some nice interpretation. So the first column are levels. The second column is first differences. In the second column, what he's doing is he's gonna take the difference in the crime rate and the difference in the police force from the previous year.  So instead of comparing the number of police who are in New York city to the number of police in Madison, now we're talking about percentage changes. That's a more comparable object. And put them on a more equal plane. \par 
 		  He does these two OLS regressions. The first regression he gets, really, the key row is the top one. You can see the first estimate that gets in levels, that the coefficient estimate is positive. That's this idea that, hey, if you add more police, the amount of crime should go down. But in fact, people were getting positive coefficients and finding that more police leads to more crime. But that was happening because we were comparing New York to Madison, where there's a lot of crime and a lot of police in New York, not much crime and not much police in Madison. You got that sort of positive correlation, right? There's the simultaneity was driving too much of that. So one idea is to take differences. That's column number two, and that seems to clean things up a little bit. Maybe that helps the endogeneinity problem. He's gonna claim that it's not perfect. Then in columns 3, 4, and 5, and you don't have to worry about 6. It's just another IV estimator. He's going to do IV estimates,  that use the two stages least squares estimators. And in each one, he changes a little bit about the instruments. In one case, he just uses elections. And then in another one, he multiplies the election times the size of the city you're in. So it gets more instrument. Another ones, he multiplies elections by region. So he's got a whole bunch more instruments. But the point is, the basic idea is that the year of election is the main instrument that's driving the results in columns 3, 4, and 5.\par 
 		   You can see that there's two big things that happen when he runs these regressions. One is that the size of the coefficient, the coefficient changes quite a bit. You go from $.28$ to $-.27$, and then it all the way jumps to $-1.39$ and  that is supposed to be interpreted as the elasticity. In the elasticity of $-1 . 39$ means a 1 percent increase in the police force causes a  $1 . 39$ percent decrease in crime. Pretty huge. But on the other hand, there's another thing that happens. The standard errors also get big. And that is something that when you would expect when doing instrumental variables. You're only getting part of the variation in $x$, the part that has to do with $z$. What you need to figure out, the coefficient on $x$ is, a lot of variation in $x$ and what instrumental variables does is it only picks up part of the variation. And by reducing down the variation, it's harder to pin down or identify what the effect of that variation is. So That's why you see intuitively why you see larger standard errors.  We're not gonna be able to estimate that $\beta$ coefficient precisely when we use instrumental variables. But you can see they gone significantly. They got quite a bit bigger here. But the main finding here, from his perspective, is that he found strong negative effects of increasing the police force or strong effects of increasing the police force on decreasing crime. You kind of see pretty significant differences as we go from columns 3 to 4 to 5. These are not all the same, although it's hard to maybe distinguish between them, because the standard errors are big. There is an idea that if you just increase the number of instruments that you typically will start to bias your results towards the ordinary least squares. And you kind of see that back here. That's table three. \par 
	    	Here's table four, which is nearly the same kind of table, same kinds of regressions and everything, except for one thing, the dependent variable is different. Now we're not looking at violent crime, we're looking at property crime, so no person is hurt. But there's some sort of crime having to do with the property, usually stealing things. And the effects it's actually quite different here. But for this column three in table 4, for instance, here you can't distinguish the estimate from zero statistically. You take the ratio of the estimate to the standard error.  It's less than two in absolute value. So we would say that's not statistically significantly different from zero. So it's hard to find the effective police on on the property, right? He does one more thing in table 5, which is he starts to break down those crime categories, got violent crime and property crime and each of them have little categories within it. When you do this, there is one interesting thing, which is that murder. It just kind of dominate things in terms of the magnitude. It's got much bigger effects. \par 
			 And in a way, this is just him being honest about the breakdown of where these violent crime and property crime results come from.  But a in a way, I don't know you could have different reactions to this. For the fact that a lot of the movement comes from the murder rate rather than just the general piling crime rate,  I don't know if that's perfectly believable or not. Before I showed you these results, if I said i'm gonna increase the police force by 10 percent, do you think that's gonna have a bigger effect on how many cars are stolen or on the murder rate? That's hard to know. But a lot of times, at least people might argue that murder is not something that people are calculating with. Usually it's something that happens maybe in the heat of a conflict between two people often that know each other. They're not really thinking about the police, but at the motor vehicle theft, car theft they are. Maybe that's a more planned out thing. You would at least think and then, the criminal crime production function, these people would calculate. So it's a little bit surprising, I think. If the results that murder is the one with the biggest effect is a little bit surprising, maybe even worry. That's how this the main story here seems to be hanging on the murder rate and the influence on murder. \par 
			 Maybe this is too much into the details. The point is that when you read the paper, you should be able to have the corresponding idea of what the regressions, what OLS estimates, where did those come from? Where did the two stage least square estimates come from and so on? Let's be specific. I won't write out every single detail. And I  gonna present it on slides instead of the chalkboard, because lots of this is hard to follow. So here's the idea. The last table we saw which was table five had to do with breaking down by different crime categories, murder, robbery. So they were just from regressions on a different dependent variable. One was the murder rate, one was the robbery rate. I'm not putting all seven here, but i'm putting two just as an example. \par 
			I'm gonna tell you about some other independent variable. So to get an intercept, usually what you do is you just include a column of ones. You include an $x$ variable, that's a one. Then the coefficient on that will be an intercept.  He doesn't do exactly that. He wants to have a different intercept for each city, call those \textbf{city effects}. Each city gets their own special intercept. That sort of means that each city is allowed to have kind of a different level. That's what the intercept does is, just kind of moves everything up or down. And so instead of having just a vector of ones, he actually has a vector of ones for Atlanta and then zero for the other cities. The first column here sort of corresponds to an intercept for Atlanta. Then this would create an intercept for Boston, and so on. Each city gets their own intercept.\par
			And then, lastly, collect together what we call the instruments $Z^{A}$. The $X^{A}$, those were the police variables. Those were the ones we were worried about being endogenous. We're gonna create instruments that sort of correspond to them. Try to help us identify part of their variation through something exogenous like elections. And this is $Z^{A}$ gives me an election variable for mayors and governors.  So mayors of cities have elections possibly on different years than governors, maybe sometimes the same, sometimes different. These are just gonna be indicator variables that tell me whether in Atlanta in 1971, was there in election for mayors. If yes then it's a one, if no it's a zero. This is gonna be the lag, because he always does the today and the lag in his endogenous variable and same thing for elections for governors. Those are going to be his instruments. \par 
			And what I want to say, just to be super clear, here, my objective here is the following. Why am I giving you these huge matrix? The idea is that if I give you the matrix for $X_{n \times k} = (x_{1}', x_{2}', \dots, x_{n}')'$. The first row is that is the transpose of $x_{1}$. Last row is transpose of $x_{n}$. And the vector for $y_{n \times 1}$, I tell you here's what it looks like. Then what I hope you will know is, okay, if i've got this, I know how to compute, say, OLS. I take
			\[	\hat{\beta} = (\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}')^{-1} \frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}.	\]
			Once I specify the big matrix of all the data, you have enough to do OLS. If I show you the big matrix of the instruments, you have enough to do the two stage least squares. That's why I'm showing you the big matrix, but also it's kind of helpful to look at these rather than observation by observation, $i$ by $i$ but at the whole matrix, it turns out it's a little bit useful to do that.  \par 
			 So there are multiple routines. I'm first gonna show you two different methods of estimation that are not in Levitt. But you'll see why I show that to you. They're related to ones that are in Levitt. I'm gonna take those those matrices $X^{A}, X^{B}$ and $X^{C}$, I'm gonna set them right beside each other. I'm gonna create one huge matrix, which I'll call $X^{\circled{1}} = (X^{A}, X^{B}, X^{C})$. $X^{\circled{1}}$ just contains all those right hand side co-variants. So I can do a regression of the murder rates, the murder dependent variables, or robbery on those right inside variables.  So those I can do OLS. That's what I mean by regress $y$ on $x$. $y$ murder, on $X^{\circled{1}}$, that gives you some of OLS estimates. There's another thing you could do, which is you could take the murder dependent variables, the robbery dependent variables, then the other ones too in the violent crime category, you could stack them on top of each other. This is a big vector, and this is a big vector, and even creates bigger vector when I stack them on top of each other. I can put them in this way into a new $X$ variable called $X^{\circled{2}}$. 
			 \[	X^{\circled{2}} =  \begin{pmatrix}
				 X^{\circled{1}} & 0 \\
				 0 & X^{\circled{1}}
			 \end{pmatrix} = \begin{pmatrix}
			 X^{A} & X^{B} & X^{C} & 0 & 0 & 0 \\
			 0 & 0 & 0 & X^{A} & X^{B} & X^{C} 
			 \end{pmatrix}.	\]
			 What would happen if I run OLS of this dependent variable, both murder and robbery together, all stack together on these independent variables $X^{\circled{2}}$,  I would actually get coefficients stacked over. It turns out either regressing murder on $X^{\circled{1}}$ and robbery on $X^{\circled{1}}$. Or stacking them together like this. They give you the same thing. Numerically, identical, we have $\hat{\beta}^{\circled{1}}_{mur} = \hat{\beta}^{\circled{2}}_{mur}$ and $\hat{\beta}^{\circled{1}}_{rob} = \hat{\beta}^{\circled{2}}_{rob}$.\par 
			  In other words, we can run separate regressions for each dependent variable, for the murder dependent variable and for the robbery dependent variable. Or alternatively, we could stack them all up and do them together and get we would get exactly the same answer. But for the independents, we do not just put $X^{\circled{1}}$ one on top of $X^{\circled{1}}$, we have to put them in this large block diagonal way, so that each crime category gets its own set of coefficients $X^{\circled{1}}$. \par 
			  These turns out not what Levitt does. But these are called the I call these \textbf{the unrestricted}. What happens is, essentially I'm allowing for a completely separate coefficient in $\beta$ for murder and property. Two separate sets of coefficients, kept them completely separated. You can kind of see that by the structure of this covariant matrix $X^{\circled{2}}$. \par 
			  Here's what Levitt does, though. He opposes some restriction. He does the stack version. He poses a little bit of restrictions. You can see he's gonna impose the following restriction
			  \[	X^{\circled{3}}  = \begin{pmatrix}
			  X^{A} & X^{B} & 0 & 0 & X^{C} \\
			  0 & 0  & X^{A} & X^{B} & X^{C} 
			  \end{pmatrix}.		\]
			   The coefficients matrix $X^{C} $ those are the city effects. Those intercepts for each city, he forces those to be the same for murder and robbery and other crime categories in the violent crime. He allows the coefficients on all the other things to be different across crime category, he is gonna impose that the city effects are the same. That's where he gets the coefficient estimates in table 5. The last table we looked at had it breakdown by murder, robbery, motor vehicle, theft, blah, blah. That's where he gets them from a regression like this. \par 
			   How does he get the earlier ones? Tables three and four. In tables three and four he doesn't have a separate, effective police on murder and robbery. He just has one for all the violent crime and one for all the property crime. How does he do that? He imposes a restriction that the coefficient on police, that's the $X^{A}$ is the same for robbery and for murder. That's how he gets the results in tables three and four. This is like the table three version:
			   \[	X^{\circled{4}}  = \begin{pmatrix}
			   X^{A} & X^{B} & 0 & X^{C} \\
			   X^{A}  & 0 & X^{B} & X^{C} 
			   \end{pmatrix}.	\]
			   That's how he does the OLS. The IV or the two stage least squares actually works very similarly. He just has to set up instruments for each of those regressions. And the way in what he sets up as the instruments is very parallel to how the co-variants are set up. The way $Z$'s looks mimics exactly the way $X$'s are defined, he impose the same restrictions and so on. That's how he actually does those regressions. And you wouldn't necessarily know it unless you read really carefully. Exactly what he does in there. \par 
			   Right now, it turns out there's even more of a twist to this. Because even what I just described as the OLS and two stages least square results for these different specifications aren't exactly what he does. Actually, he doesn't do  the OLS and two stages least square. He does something called \textbf{weighted the OLS} and \textbf{the weighted two stages least squares.}
\subsection{The Weighted OLS/2SLS} 
			   First, let's talk about weighted least squares. It's a modification of ordinary least squares. Here are weighted ordinary least squares estimation and weighted two stage least square for beta. 
			 \[  \hat{\beta}_{W L S}=\left(\frac{1}{n} \sum_{i} w\left(x_{i}\right) x_{i} x_{i}^{\prime}\right)^{-1} \frac{1}{n} \sum_{i} w\left(x_{i}\right) x_{i} y_{i}. \]
			 \[\begin{aligned}
			 \hat{\beta}_{W 2 S L S}=& {\left[\left(\frac{1}{n} \sum_{i=1}^{n} w\left(z_{i}\right) x_{i} z_{i}^{\prime}\right)\left(\frac{1}{n} \sum_{i=1}^{n} w\left(z_{i}\right) z_{i} z_{i}^{\prime}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} w\left(z_{i}\right) z_{i} x_{i}^{\prime}\right)\right]^{-1} } \\
			 & \cdot\left(\frac{1}{n} \sum_{i=1}^{n} w\left(z_{i}\right) x_{i} z_{i}^{\prime}\right)\left(\frac{1}{n} \sum_{i=1}^{n} w\left(z_{i}\right) z_{i} z_{i}^{\prime}\right)^{-1} \frac{1}{n} \sum_{i=1}^{n} w\left(z_{i}\right) z_{i} y_{i}
			 \end{aligned}\]
			   And you can see the only difference is I've added in a little weight function here to each of these. Is weighted least squares a ok thing to do? Well, you can see, for one thing, the weights here are only a function of the $x$'s, they're not allowed to be a function of the $y$'s. \par 
			   Will this produce a consistent estimate? All the answer is it depends. We know that we could actually rewrite this. If I substituted $y_{i}$ with $x_{i}'\beta + \epsilon_{i}$,  I could rewrite this thing as $\hat{\beta}_{WOLS} - \beta$. Then I could get the same expression, but with an $\epsilon_{i}$ here. If I could get that average to go to zero, convergent probability to zero, I would have consistency. That's our trick. We've done it many times now. So basically, you're saying that in the last average, if I replace the $y_{i}$ with an $\epsilon_{i}$, 
			   \[	E\left[w\left(x_{i}\right) x_{i} \varepsilon_{i}\right]=0	\]
			   would be the orthogonality condition that would be required to get consistency. Is that orthogonality condition implied by our usual orthogonality conditions?  It turns out it's implied by OLS 2, but not implied by OLS 2'. Now, suddenly, there's a difference. Which one of these assumptions we makes a difference here. It determines whether weighted least squares is something that we can do. \par 
			   If you make the stronger assumption, OLS 2, then this will be equal to zero. You can use the law of iterated expectations to show that. If we assume OLS 2', then there's no guarantee. \par 
			   So actually, now we can sort of see that Levitt must happen his mind OLS 2. That's what he's doing as weighted least squares and he must be thinking he's got the stronger assumption. Otherwise, he's producing an estimate of something else. \par
				What about two stage least square? A parallel story. We've got the two stage least squares estimator. I just add into each of these sums, a weight that depends, again on the exogenous variable $z_{i}$, the one that satisfies some orthogonality. It'll only depend on the instruments. Each sum is exactly the same, except of just carrying a weight function, the same weight function in each one.  When is that gonna be consistent? Same kind of story. \par 
				He's gonna use the weighted two stage least squares. So he must be believing in the stronger OLS 2 assumption and IV 2 assumption. Why doesn't he just use the regular old OLS and regular old two stage least squares? There's got to be a reason. It turns out, if you look at those results that we have in the table 5 the two stage least squares estimates, for getting statistical significance, it's a close call. He needs some accurate, precise estimates. And it turns out that if you have heterosckedasticity, you need to worry about robust variance estimation for instance. Then OLS and two stage least squares under the stronger OLS 2 and IV 2 assumptions will not be efficient estimators. There will be some way to weight different observations that gets you a more efficient estimator. \par 
				That kind of makes sense. If you have heteroskedasticity, that means that in essence, some of the observations have quite different variances associated with them than other ones.  What you want to do is you'd like to put more weight on the observations to have small variance, those are more precise. You'd like to take your weight away from the observations to have big, lots of noises, lots of variables. And that's what he has in mind. He's trying to get statistical significance, and it's a close call. So we don't get to see what the regular old OLS and two stage least square results are here. But he needs every bit of precision in his estimates that he can get. \par 
				Okay. So what does he do? He goes and uses weighted two stage least squares and the weighted OLS. What kind of weights does he? It turns out that there's a very natural idea here.  It turns out that the best weights to apply are like the inverse of the variance of the errors in your structural equation, your OLS equation.  That's what you want to do. So observations with large variance. They get small weight because you use the inverse of the variance. There's a lot of noise, a lot of variance.  If it's small variance, you get a lot of weight. \par 
				So what you should be doing, what he has in mind is he's gonna go and somewhere in the background, there's a two step process. First he goes and does the regular OLS, And he gets those errors. He estimates the residuals. Then he estimates for each crime category, what the variance is, and then he inverts it. He plugged it in as a weight and does the whole thing again. The weight are things like $\frac{1}{\sigma^{2}_{mur}}, \frac{1}{\sigma^{2}_{rob}}, \frac{1}{\sigma^{2}_{i}}, \dots$.\par 
				Now, here's the sad part of this story. He made a mistake. In his programming, there's an error. For some of these results. Instead of using one over the  the standard deviation, he uses just the standard deviation. So he should have been inverting and squaring. He use the wrong way. He or maybe his RA. So he used the wrong weighting. Then you will get a less efficient estimator, most likely. But it turns out have some tough implications for him. Someone else was trying to re-do some of this work and realize this.\par 
				And what happens is when we look at the murder category, I noted that its coefficient coefficient on murder was much larger than other rows in Levitt's TABLE 5. We already talked about that. But there's another thing that's kind of interesting and weird here, and that is the standard errors. So compare across the columns in this row 2SLS, these standard errors, it turns out that in this world (2SLS), the standard errors for  murder is small. It's the most precisely estimated((0.91). Look at what happens in OLS.  There the standard error on murder is the largest instead of the smallest compared to these other crime category. It's the opposite. So it's a little bit tricky, wouldn't necessarily have noticed it when you're looking at this table. In fact, most people didn't notice this for a while. But that's actually very unusual. It's not what you would expect to have this order. Suddenly, it's the least precisely estimated under OLS but then under two stage least squares, it's the most precisely estimated. Maybe you could sort of justify it, because you could say, well, actually, if we're thinking about the effect of this instrument that actually this instrument changes, the number of police is much more responsive to the murder rate in this political system, than to other crime category. Maybe you can put some explanation, but it's a little bit odd. But this actually comes, unfortunately, from this mistake in the in, the coding.\par 
				So here's what McCrary does. He goes and gets the data or nearly the same data. And he tries to replicate what Levitt does. What he's got? Refers to McCrary's paper.  It turns out it's a little bit hard to exactly get it exactly right, but it's pretty close. You can see that this column two and four, those are pretty close to each other. They're pretty close standard errors and estimates, those are the OLS columns. And it turns out that if you look at the columns, three and five here, and here, not quite as close. It turns out, in particular, there's a distinction in standard errors. The reason is that when McCrary do it,  he's trying to correct it. He's trying to use the correct weights. This is what you would have gotten. If you use the correct weighting, the inverse variance weighting. And you can see now, murder actually has the largest standard error associated with it. Which is more in line with what you would expect, because it was the largest standard error for OLS, too.\par 
				 It's a little bit unformed, because what happened is the estimate it's still big and different from what it was before big in magnitude. But the standard error has increased quite a bit.  But the magnitude is decreased so now you don't get the statistical significance that you have before. You don't get that overall statistically significant conclusion for all violent crimes, which is what was driving some of the excitement about this paper.\par 
				 Now, this is kind of unfortunate. So what's kind of going on in the background here? What's the important thing to take away from this.  One of the things is that he's really combining, instead of having separate coefficients for each of these sub categories, he puts all four of those together to get this one. Problem is the effect of police on the robbery looks like it's probably quite different than the effect of police on murder. There might be some heterogeneity there when he imposes the restriction that they're all the same, what's happening is that by imposing that restriction, he's kind of changing what's leftover in the error. Another way to put it is by pulling these altogether, he's actually, in essence, directly violating our OLS 2 or our IV 2 type of assumption. It's probably violating the orthogonality. In violating the orthogonality, what happens is the weighted ordinary least squares, or the weighted two stage least squares is giving you a consistent estimate.  Basically, what he's doing is he had a way of weighting things up so that he got a lot of weight put on $-3$, up there the coefficient on murder. When you put a lot of weight on that, then he ended up with something that was big in magnitude. When you put less weight on that, you don't get as much. That's all that was happening. So these weights really can make a quite a bit of difference in the results that you get. And that probably is coming from the fact that it may not be a good idea to assume that IV 2 or OLS 2 are holding that these weighted versions are actually reasonable to do. That's the unfortunate potential conclusion. \par 
				 As a concluding comment about this, I will say this happened in the way that it should, which is, people make mistakes. Researchers made mistakes. Every paper has mistakes. This one was somewhat significant mistake. It was caught. It was unfortunate. He got kind of made a big deal about, because there's a new publication that's focused on just this mistake. But Steven Levitt, he did the right thing.  He sort of said yah I made a mistake. He tried to look at some other things, he has a reply. He's still trying to say if you look at it, look at looks at a little different cuts of the data.  He is still trying to defend his position, but also admits the mistake. That's kind of how we hope the process work.  It doesn't always work that way, but that these guys kind of did it. The right way in the sense is, I think is a compliment to both of them.  That's how research should function. It's okay to make mistakes. It's not great, but it definitely happens. It could be fun. It's about best thing to do and say, yeah, it's a mistake. \par
				 Questions or comments about this stuff. ME: ``So why does different ways could sort of improve our estimation?'' Jack ``Yeah. So it has the potential for giving you better estimates. The funny thing is that it turned out that the way that he by applying the incorrect weights, he actually ended up with something that had a smaller standard error. The idea of applying these inverse variance ways is actually to get a smaller standard error. But what happens, unfortunately, you're not guaranteed to get a smaller standard error by applying the sort of efficient weights.  But in the case when you shouldn't be applying the weights where IV 2 doesn't hold, you really aren't guaranteed anything that this is a more efficient estimator. It's just an estimator of a different thing. In essence, you're kind of putting together these four of numbers. You could just giving them different weights and you can see how you could get quite different answers, depending on how on the different weights you get. And then by putting, depending on the way to put on these different things, your standard error could be quite different, too, may not. This isn't necessarily efficient. It's not even consistent. ME:``So it doesn't mean the if we put more weights on the observations with small variance, then our estimator will be more efficient.'' Jack`` It will be more efficient if the OLS 2 or IV 2, depending on which one you're using holds. But if it doesn't hold, you're just getting an estimate of the a different thing. It's not even consistent. '' So that's another little indication that there might be problem. \par 
				 So the next thing that I want to talk about is weak instruments.  What I'll say is that what I'm trying to describe a bigger picture, the motivation behind a lot of this stuff is to try to put together sort of how econometrics works and what's happening in practice. And to get you think hard about how those two fit together. It turns out there's another thing. Even the results that we got on OLS and 2SLS where there are consistency and asymptotic normality have some potential to be not great.  So there's occasions where the 2SLS that we got asymptotic normality says the estimator is approximately normally distributed with a certain variance. Sometimes it may not be. So in general, it works well, these approximation work well. But you have to be really careful about exactly where they work well. And that leads us to a different investigation, which is what happens when we have \textbf{weak instruments}. We'll figure out how to fix up that next time.
\section{Weak Instrumental Variable} 
	We have developed the two stage least square method.
	\[	y_{i} = x_{i}' \beta + \epsilon_{i}, \qquad i= 1,2,3, \dots, n.	\]
	\begin{enumerate}
		\item \textbf{(IV $0$)} $(y_{i}, x_{i}, z_{i})$ i.i.d. and sufficient moment existence;\par 
		Just for simplicity
		\item \textbf{(IV $1$)} $E(z_{i}z_{i}')$ is finite and non-singular, $E(z_{i}x_{i}')$ is full column rank;\par 
		New assumption: $E(z_{i}x_{i}')$ is full column rank.
		\item \textbf{(IV $2$)} $E(\epsilon_{i} | z_{i}) = 0$;
		\item \textbf{(IV $2'$)} $E( z_{i}\epsilon_{i}) = 0$;
		\item \textbf{(IV $3$)} $E(\epsilon_{i}^{2}z_{i}z_{i}')$ is finite;
	\end{enumerate}
	Let us focus on IV (1b) and IV 2' for the moment:
	\begin{framed}
		IV (1b) $E(z_{i}x_{i}')$ is full column rank;\par 
		IV (2')  $E( z_{i}\epsilon_{i}) = 0$;
	\end{framed}
		We want to think about this in a more specific case, i.e., when $z_{i}$ and $x_{i}$ are scalars. Remember that we have $x_{i}$ being a $k \times 1$ variable and $z_{i}$ being an $l \times 1$ variable, so in that case, we have $l = k = 1$. It is a simple case but useful to build intuition and explain something. \par 
		In this case, our assumption of IV (1b) implies that $E(z_{i}x_{i})$ is nonzero. That is to say, $z_{i}$ and $x_{i}$ are not orthogonal. \par 
		For the first stage equation, we have 
		\[	x_{i} = z_{i} \pi + v_{i}.	\]
		Notice that we do not need to bother with transpose here. Keep in mind that the first stage equation comes hand in hand with an assumption/definition, that is, the usual OLS 2' $E(z_{i}v_{i}) = 0$. Without this. the first stage equation barely has any meaning. It is a definition as well as an assumption since it really defines $\pi$, to see this, notice 
		\[	\begin{aligned}
			E(z_{i}x_{i}') &= E(z_{i}(z_{i}'\pi + v_{i})) \\
								&=  E(z_{i}z_{i}')\pi + E(z_{i}v_{i}). \\
		\end{aligned}	\]
		So we have $\pi = E(z_{i}z_{i}')^{-1}E(z_{i}x_{i}')$ if and only if $E(z_{i}v_{i}) = 0$. (Also assumed IV 1, but not the key here.) So the orthogonality condition between $z_{i}$ and $v_{i}$ is actually a definition of our best liner predictor $\pi$. \par 
		In our case, where $k = l = 1$, we have 
		\[	E(z_{i}x_{i}) = E(z_{i}(z_{i}\pi + v_{i})) = E(z_{i}^{2})\pi + E(z_{i}v_{i}) = E(z_{i}^{2})\pi . 	\]  
		By IV (1b), we cannot have $E(z_{i}x_{i}) = 0$, so that is equivalent to requiring $E(z_{i}^{2})\pi \ne 0$. \par 
		Notice that we must have $E(z_{i}^{2}) \geq 0$, and $E(z_{i}^{2}) = 0$ if and only if $z_{i}^{2} = 0$. However, the case $z_{i}^{2} = 0$ is excluded by assumption IV (1a)' non-singular requirement. An instrument of all zeros will be very boring. So, the requirement that $E(z_{i}^{2})\pi \ne 0$ now pins down to $\pi \ne 0$. \par
		That is to say, the new full column rank assumption IV (1b) is actually saying there is some relation here between $z$ and $x$ that is useful, i.e., $\pi \ne 0$. If $\pi = 0$, then our first stage equation becomes $x_{i} = z_{i}\pi + v_{i} = v_{i}$, we are decomposing the variation in $x_{i}$ all to $v_{i}$, which is not useful at all.\par 
		But weak instrument is worrying about a different situation, not so much as $\pi = 0$, but what if $\pi$ is really close to zero. \par 
		In the two stage least square regression of our simple case, we have 
		\begin{enumerate}
			\item \textbf{(The structural equation )} $y_{i} = x_{i}\beta + \epsilon_{i}$;
			\item \textbf{(The $1^{st}$ stage equation)} $x_{i} = z_{i}\pi + v_{i}$;
			\item \textbf{(The reduced form equation)} $y_{i} = (z_{i}\pi + v_{i})\beta + \epsilon_{i} =z_{i}\pi\beta + v_{i}\beta + \epsilon_{i}  $.
		\end{enumerate}
		Now, define $\pi\beta = \alpha$ and $v_{i}\beta + \epsilon_{i}=u_{i}$, then 
		\[	y_{i} = z_{i}\alpha + u_{i}.	\]
		We are interested in the first stage equation, which tells us how $x_{i}$ varies with $z_{i}$ and the reduced form equation, which tells us how $y_{i}$ varies with $z_{i}$. One question is, do we get consistent estimate of $\pi$? The answer is yes since we have $E(z_{i}v_{i}) = 0$ in the first stage least square. The other problem is do we get consistent estimate of $\alpha$? The answer is again yes since we have $E(z_{i}u_{i}) = E(z_{i}(v_{i}\beta + \epsilon_{i})) = E(z_{i}v_{i})\beta + E(z_{i}\epsilon_{i}) = 0$ by IV 2' and first stage least square assumption. The reason why we are is we can get $\beta$ by $\alpha$ and $\pi$, remember we defined $\alpha = \pi \beta $, and then $\beta = \frac{\alpha}{\pi}$. \par 
		If, however, $\pi = 0$, then we are in trouble. We get $\beta = \frac{0}{0}$, two stage least square method is fails. Fortunately, IV (1b) help us exclude this.\par 
		Now, we want to do a simulation to see whether the 95 percent confidence interval generated by IV estimation indeed contain true $\beta$ of 95 percentage of times. Here I choose $z_{i}$ from a standard normal distribution independently and $(\epsilon_{i},v_{i})$ jointly from a multi-variate normal distribution with mean zero. Notice that the correlation of $\epsilon_{i}, v_{i}$ has some meaning. Consider the first stage equation $x_{i} = z_{i}\pi + v_{i}$. Here $x_{i}$ is the endogenous variable that has the endogeneity problem, and $z_{i}$ is the instrumental variable we choose, which is exogeneous. Therefore, the reason for endogeneity must come from $v_{i}$, it is the part of $x_{i}$ that has some correlation with $\epsilon_{i}$. So the covariance of $\epsilon_{i}$ and $v_{i}$ represents the level of endogeneity. If we choose $\rho = 0$, then we could just do OLS. Although there is no problem to do IV, but there is no reason we would begin. \par 
		Now, if we change the $\rho$ and $\pi$, you can see when $\pi$ is close to $0$, we may need a different method for forming confidence intervals. Also, sometimes we are just not sure, so we need to find a method to protect us.\par 
		Let us try to understand what is behind this weak instrument issue.\par 
		You could argue that as our sample size gets larger, we will finally get better coverage close to 95 percent. However, first off, we cannot choose our simple size; secondly, the real issue is no matter what sample size given, there is always an instrument weakness where the approximation is poor if $\pi$ is small enough or $\rho$, the severity of indogeneity is high enough. Remember when $k = l = 1$, we have 
		\[	\begin{aligned}
		\hat{\beta}_{2SLS} &= \frac{(\frac{1}{n}\sum_{i=1}^{n}x_{i}z_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}^{2})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}y_{i})}{(\frac{1}{n}\sum_{i=1}^{n}x_{i}z_{i})(\frac{1}{n}\sum_{i=1}^{n}z_{i}^{2})^{-1}(\frac{1}{n}\sum_{i=1}^{n}z_{i}x_{i})}\\
		&= \frac{\frac{1}{n}\sum_{i=1}^{n}z_{i}y_{i}}{\frac{1}{n}\sum_{i=1}^{n}z_{i}x_{i}}\\
		&= \frac{\frac{1}{n}\sum_{i=1}^{n}z_{i}(x_{i}\beta + \epsilon_{i})}{\frac{1}{n}\sum_{i=1}^{n}z_{i}x_{i}}\\
		&= \beta + \frac{\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}}{\frac{1}{n}\sum_{i=1}^{n}z_{i}x_{i}} \\
		&= \beta + \frac{\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}}{\frac{1}{n}\sum_{i=1}^{n}z_{i}(z_{i}\pi + v_{i})} \\
		&= \beta + \frac{\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}}{(\frac{1}{n}\sum_{i=1}^{n}z_{i}^{2}\pi) + (\frac{1}{n}\sum_{i=1}^{n}z_{i}v_{i})}.
		\end{aligned}	\]
		Our consistency require that the last term converges in probability to zero. \par 
		Our two stage least square method says, okay, since $\pi \ne 0$, which is guaranteed by my IV 2' assumption, the dominant term in the denominator is just $(\frac{1}{n}\sum_{i=1}^{n}z_{i}^{2}\pi)$, which converges in probability to a nonzero number (by IV 1a) times $\pi$. We just ignore the second term in the denominator since it converge in probability to zero. The nominator converges to zero, too. Therefore we have consistency.\par 
		Let us look at another extreme, where $\pi = 0$. Then, 
		\[		\hat{\beta}_{2SLS} = \beta + \frac{\frac{1}{n}\sum_{i=1}^{n}z_{i}\epsilon_{i}}{0 + (\frac{1}{n}\sum_{i=1}^{n}z_{i}v_{i})}.	\]
		The numerator and denominator both converges to $0$. Then we have 
		\[	\hat{\beta}_{2SLS} - \beta \stackrel{d}{\longrightarrow} Cauchy.	\]
		That is, we no long have a nice convergence in probability property. Now, $\hat{\beta}_{2SLS} - \beta $ converges in distribution to a Cauchy distribution. And a Cauchy distribution has the ``normal bell shape'', but it has very fat tails such that it does not even have mean and variance exist. It does not have sufficient moments exist. Notice that we do not have $\sqrt{n}$ on $\hat{\beta}_{2SLS} - \beta $  here. It just take too much weight far from $0$, which is a bad situation.\par 
		Now, we can take a look of what happens if $\beta$ is very close to zero, an intermediate case in some sense. In that case, the point is actually we cannot just ignore the second term in the denominator. For any given sample size, we can always find a $\pi$ small enough to make $\frac{1}{n}\sum_{i=1}^{n}z_{i}v_{i}$ plays a role. If it starts to dominate, we are moving to the convergence in Cauchy distribution side, and we are in trouble. \par 
		So if $\pi$ is large and the term $(\frac{1}{n}\sum_{i=1}^{n}z_{i}^{2}\pi)$ dominates, the asymptotic normal does a really good job. But if the term  $\frac{1}{n}\sum_{i=1}^{n}z_{i}v_{i}$ dominates, we are probably in trouble. If $\pi = 0$, i.e., $\frac{1}{n}\sum_{i=1}^{n}z_{i}v_{i}$ dominates completely, we are doing a very poor job.  The problem is whether or not the approximation of $95$ percent confidence interval we construct is good enough \emph{given a sample size}. We know when $\pi$ is small, we should worry about it. But we do not always know $\pi$. Usually we could estimate $\pi$ by doing the first stage regression, but we should try to protect us.\par 
		The conclusion is that the confidence interval method we use is quite unstable. That is to say, it is sensitive to $\pi$. It is accurate for certain $\pi$'s, i.e., the ones far from zero. And it is poor for other small $\pi$'s. We need a different inference method that is robust to $\pi$. \par 
		 Next time, we will take a small step backward and think about the connection between hypothesis testing and confidence intervals. Confidence interval is about rejecting or accepting a hypothesis, and confidence interval is about finding the set of $\beta$'s that are somehow consistent with what the data gives us. We will see there is a duality relation between these two objects, i.e., given one thing we can construct the other. Then we will use this duality relation to generate a new hypothesis testing method which will be robust to the value of $\pi$, and then use that new confidence interval to generate a new confidence interval which is also robust to the value of $\pi$. 
\subsection{Test Inversion}
	Today we want to find ways to fix weak instrument variable problem. Specifically, we will develop the classic Anderson-Rudin test. Before that, let me show you something more general, that is, the test inversion, which is using a dual relation between hypothesis test and confidence interval to form confidence interval. \par 
	We will consider a particular type of hypothesis testing: two-sided hypothesis testing. In many ways, hypothesis testing is a much simpler statistical procedure than the ones we have been talking about, producing estimates, standard errors and confidence intervals, in the sense that it has only one of the two possible outcomes: acceptance or rejection. \par 
	We have the \textbf{null hypothesis}
	\[	H_{0} : \beta = \beta_{0}	\]
	where $\beta_{0}$ is a user specified value and $\beta$ is the true value we do not know; we also have the \textbf{alternative hypothesis}
	\[	H_{1} : \beta \ne \beta_{0}.	\]
	When combined with our test result, we have 
	\begin{table}[H]
		\centering 
		\begin{tabular}{ | l | c | c | } 
			\hline 
				& Accept $H_{0}$ & Reject $H_{0}$  \\ \hline 
			$H_{0}$ true & $\surd$ & Type 1 error \\ \hline 
			$H_{1}$ true & Type 2 error & $\surd$ \\ \hline 
		\end{tabular} 
	\end{table}
	Classical hypothesis testing regard the control of Type 1 error as the first priority. That is not saying Type 2 error is not important, but we turn to that only after the control of Type 1 error. We want to put a cap on the maximum allowable probability of Type 1 error, which leads to the definition of \textbf{significance level}:
	\begin{definition}
		\textbf{Significance level} is the maximum allowable probability of Type 1 error, it takes a value of $(0,1)$, and is usually chosen to be $0.5$. 
	\end{definition} 
	Here is usually what happens in a hypothesis testing. For a any given data set $Data_{n}$, where the subscript $n$ indicates the sample size, we form a \textbf{test statistic} $T_{n}$, which could be thought as a function of our data so $T_{n} = T_{n}(Data_{n})$. Then we choose the corresponding \textbf{critical value} $c_{\alpha}$ w.r.t. the $\alpha$ significance level. Then
	\begin{enumerate}
		\item If $T_{n} \geq c_{\alpha}$, we reject $H_{0}$;
		\item If $T_{n} < c_{\alpha}$, we accept $H_{0}$.
	\end{enumerate}
	The whole thing of creating hypothesis testing is what test statistic should I use. The critical value here $c_{\alpha}$ is usually(but not has to be) a constant. It is chosen so that the significance level holds. That is, we must have 
	\[	\alpha \geq Pr[reject~ H_{0} | H_{0}~ is ~true]	= Pr[T_{n} \geq c_{\alpha} |  H_{0}~ is ~true], \]
	where $\alpha$ is the maximum Type 1 error probability cap we chosen beforehand. Notice that $Pr[T_{n} \geq c_{\alpha} |  H_{0}~ is ~true]$ is the probability we make a Type 1 error.\par 
	Some people make a big deal about arguments like ``accept $H_{0}$'' or ``do not have evidence to reject $H_{0}$''. My claim is, there are only two possible results, whatever you call.\par 
	What hypothesis test really does is that if we start with a specified $\beta_{0}$, we can get a result of acceptance or rejection for any data given to me. It is ready to go in the sense that for any possible data set, we immediately have one and only one of the results. That is how statistical procedure works. That is to say, if we think about the space of all possible data, we can actually form a partition of that space into two sets. One set for acceptance and another for rejection. That leads to the definition of \textbf{acceptance region}. 
	\begin{definition}
		For a given $\beta_{0}$, the \textbf{acceptance region} $A(\beta_{0})$ is the subset of the space of  data such that the hypothesis testing gives an acceptance of the null hypothesis. In other words, 
		\[	A(\beta_{0}) = \{Data_{n} \colon T_{n}(Data_{n}) \leq c_{\alpha}\}.	\]
		As you may notice, it is a mapping from the parameter space to the space of subsets of data space.
	\end{definition}   
	This is basically in essence what hypothesis testing is doing. The definition of \textbf{confidence set/interval} is in some sense the inverse of the definition of acceptance region.
	\begin{definition}
		For a given data $Data_{n}$, the \textbf{confidence interval(confidence set)} $CI(Data_{n})$ is the set of all parameters such that its acceptance region includes our given data. That is, 
		\[	CI(Data_{n}) = \{\beta_{0} \colon Data_{n} \in A(\beta_{0})\}.	\]
		In other words, it is the set of all $\beta_{0}$ that will be accepted as a null hypothesis at significance $\alpha$. 
	\end{definition}
	Most of the procedures we have been talking about are about how to take data and move it into a parameter, confidence interval or standard error, the statics that have some properties. Then we use hypothesis test to test the coverage situation. \par 
	Therefore, once we have specified a hypothesis testing, we can do the test inversion and get a confidence interval. It can be shown that the confidence is determined by $\alpha$, the behavior of our test under null hypothesis. 
	\begin{definition}
		The \textbf{confidence interval} is $1-\alpha$, where $\alpha$ is the significance level specified in the hypothesis testing procedure before test inversion. It is the minimum probability of coverage of the true $\beta$ by our confidence interval.
	\end{definition}
	It may be quite hard to follow at this level of generality, we are going to have a example of test inversion for IV to create an Anderson-Rubin confidence interval, which is the Anderson-Rubin test. \par 
	In our previous process of two stage least square regression, we are trying to deal with the case when OLS 2' $E(x_{i}\epsilon_{i}) = 0$ fails. But notice that this condition/definition requires all the elements in the vector be zero. Actually, there could be some nonzero variables even if when $E(x_{i}\epsilon_{i}) \ne 0$. We want to separate our original $x_{i}'$ into two variable vectors:
	\begin{enumerate}
		\item $x_{i}'$: $E(x_{ij}\epsilon_{i}) \ne 0$ for all $j$, these are the variables causing the endogeneity;
		\item $w_{i}'$: $E(w_{I}\epsilon_{i}) = 0$, these are the variables that satisfy the orthogonality.
	\end{enumerate}
		Now, our structural equation is 
	\[	y_{i} = x_{i}' \beta + w_{i}' \gamma + \epsilon_{i}.	\] 
	Also, we want to put $z_{i}$ into the first stage equation:
	\[	x_{i}' = z_{i}'\pi + w_{i}'\alpha + v_{i}. 	\]
	Now, terms in $x_{i}$ are called the \textbf{included endogenous variables}; terms in $w_{i}$ are called the \textbf{included endogenous variables}; terms in $z_{i}$ are called the \textbf{excluded exogenous variables}. The key thing here is still $\pi$. \par 
	Here is what Anderson and Rubin do. First they come up with a hypothesis test:
	\[	H_{0} : \beta = \beta_{0}, \qquad H_{1} : \beta \ne \beta_{0}.	\]
	Then, subtract $x_{i}'\beta_{0}$ on both sides of the structural equation, we get 
	\[	\begin{aligned}
			y_{i} - x_{i}'\beta &= x_{i}'(\beta - \beta_{0}) + w_{i}' \gamma + \epsilon_{i} \\
			&= (z_{i}'\pi + w_{i}'\alpha + v_{i})(\beta - \beta_{0}) + w_{i}' \gamma + \epsilon_{i}\\
			&= z_{i}'\pi(\beta - \beta_{0}) + w_{i}'(\gamma + \alpha(\beta - \beta_{0})) + (\epsilon_{i} + v_{i}(\beta - \beta_{0}))\\
			&= z_{i}'\theta + w_{i}'\lambda + u_{i}
	\end{aligned}	\]
	where we denote $\pi(\beta - \beta_{0}) = \theta$, $(\gamma + \alpha(\beta - \beta_{0})) = \lambda$ and $(\epsilon_{i} + v_{i}(\beta - \beta_{0})) = u_{i}$. \par 
	We want to run this new regression. We know that we could get consistency result since $z_{i}'$ and $w_{i}$ are exogenous with $u_{i}$. Specifically, they are orthogonal with $v_{i}$, which is an assumption comes hand in hand with the first stage regression; in addition, they are orthogonal with $\epsilon_{i}$, by IV 2' assumption. This is similar with the reduced form equation we have before. \par 
	The reason why we care about $\theta$ and $\lambda$ is that if $\beta_{0} = \beta$, then $\theta = 0$. The converse holds if $\pi \ne 0$, which is implied by IV (1b) assumption. So we are going to run the regression and then test whether $\theta = 0$ or not. Our new test is now 
		\[	H_{0} : \theta = 0, \qquad H_{1} : \theta \ne 0.	\]
	Then, we use that hypothesis test to form confidence interval using test inversion introduced before. Below are some details about this, it is a little bit vague but it actually depends on how deep you want to explore. 
	\begin{framed}
		In this new regression, we do OLS regression of $(y_{i}-x_{i}'\beta_{0})$ on $z$ and $w$. The estimate of our coefficients is given by
		\[	\begin{pmatrix}
		\hat{\theta} \\
		\hat{\lambda}
		\end{pmatrix}
		=	\left[ \begin{matrix}
			\frac{1}{n} \sum_{i=1}^{n} 
			\begin{pmatrix}
			z_{i} \\
			w_{i}
			\end{pmatrix}
			\begin{pmatrix}
			z_{i} \\
			w_{i}
			\end{pmatrix}'
		\end{matrix} \right]^{-1} \frac{1}{n}\sum_{i=1}^{n} 
			\begin{pmatrix}
		z_{i} \\
		w_{i}
		\end{pmatrix}(y_{i}-x_{i}'\beta_{0}). 	\]
		By the asymptotic normality property, we have 
		\[	\sqrt{n} 
		\begin{pmatrix}
		\hat{\theta} - \theta \\
		\hat{\lambda} - \lambda
		\end{pmatrix} \stackrel{d}{\longrightarrow} N(0, \begin{pmatrix}
		V_{\theta} & C_{\theta\lambda}	\\
		C_{\theta\lambda}	 & V_{\theta} 
		\end{pmatrix}).	\]
	 	Then we can pull off the $\hat{\theta}$ part, get 
	 		\[	\sqrt{n}(\hat{\theta} - \theta) \stackrel{d}{\longrightarrow} N(0,V_{\theta}). 	\]
	 	For a scalar case, we can get
	 	\[	V_{\theta}^{-1/2}\sqrt{n}(\hat{\theta}-\theta) \stackrel{d}{\longrightarrow} N(0,I). 	\]
	 	By squaring the terms (inner product) on the left hand side, we have 
	 	\[	n(\hat{\theta} - \theta)' V_{\theta}^{-1} (\hat{\theta} - \theta)  \stackrel{d}{\longrightarrow}  \chi^{2}_{lz}	\]
	 	where $lz = \dim(z) = \dim(\theta)$ is the degree of freedom.\par 
	 	We want to test whether $\theta = 0$, we can set $\theta = 0$ and then 
	 	\[	T_{AR} = n\hat{\theta}' V_{\theta}^{-1} \hat{\theta}  \stackrel{d}{\longrightarrow} \chi^{2}_{lz}	\]
	 	under the null hypothesis $H_{0}$.\par 
	 	Then the critical value 
	 	\[	c_{\alpha} = (1-\alpha)~quantile~of~\chi^{2}_{lz}.	\]
	 	And then,
	 	\[	Pr_{H_{0}} (T_{AR} \geq c_{\alpha}) \rightarrow \alpha.	\]
	 	Our confidence set for Anderson-Rubin is 
	 	\[	CI_{AR} = \{\beta_{0} \colon Data_{n} \in A(\beta_{0})\}.	\]
	\end{framed} 
	By the way, in practice you do not need to do the inverse, there are software doing these works for you.\par  
	There are two points of showing this:\par 
	\begin{enumerate}
		\item There are other ways to form confidence interval;
		\item Anderson-Rubin is a confidence interval method that is robust to $\pi$. The significance level $\alpha$ here is determined by the behavior under the null hypothesis, and that in turn determines the confidence interval $1-\alpha$. Notice that under the null hypothesis, if $\beta = \beta_{0}$, then $\theta = 0$, it does not matter what $\pi$ is. The behavior under the null is not dependent on the value of $\pi$. That is the intuition. Of course $\pi$ being close or not matters, but is does not affect the significant level now, it is affecting the probability of Type 2 error under the alternative hypothesis.
	\end{enumerate}
	Also, if $\pi = 0$, then we cannot reject any $\beta$, our confidence interval is everything. This is useless. If $\pi$ is close to zero, then we will have significantly larger confidence interval, which reflects it is a weak instrument. \par 
	In practice, people first estimate the coefficient in the first stage regression. They also try to use the F statistics and compare that with 10. If F is greater than 10, maybe  we can tolerant the 2SLS result with the usual confidence interval. But if F is less than 10, we might want to treat weak instrument seriously. For the just identified case, Anderson-Rubin works very well. But for the over-identified case, it turns out to be not very effective. \par 
	The problem here is that these methods are proposed under the situation of homoskedasticity. It is not a great assumption. People have proposed a new F statistics that works with heteroskedastic standard error. For the just identified case, we also have AR with robust test and another method that I am involved. For the over-identified case, people are still trying to find a optimal approach.
\subsection{Angrist and Krueger (1991)}
	This is where people start to consider about weak IV seriously. I think there is a story about Angrist and Krueger presenting this paper at a conference, people were fascinated by this paper. It is a clever instrument variable. But people are concerned with small correlation of education and the birth season. Another concern is when you try to chop up the 180 instruments, the influence of each instrument tends to be really small. In this sense, people start to think that these instruments are very weak. Each individual has  very limited explanatory power towards the endogenous variable. Since then, there is a ton of discussion. \par 
	When they try replicate  the Angrist and Krueger result. The first thing they do is if we can essentially replicate the result. Then they follow the suggestion by Krueger. If you are worry about these weak instruments (notice that this is before people start to analyze weak instruments like we have been in class), how about this. What if you take the instruments we have (the quarter of birth), and you replace them by a random quarter of birth. That random number should not be correlated with anything, it is just a random number. Then I am going to treat it like the real instruments and see what happens. Table 3 shows the result. The fist two columns are IV with interaction of year of birth, which gives us 30 IVs, then run the IV regression. What you see is the estimates look a lot like OLS in table 1 and 2. This is actually not that surprising. If we regress schooling on our random IVs, then if our IVs be enough, we will pick up some variations. But those variations will be a lot like the variation in schooling itself. So it is a kind of noisy version of OLS. They are claiming that their choice of IVs is not random. Maybe too many weak IVs Angrist and Krueger gets are quite biased to OLS. That is an issue. Then they say let us look at the extreme case. But we would be able to tell through the standard errors. The the standard errors in the random IVs case end up of being very similar to the true instruments. The point is the standard errors do not tell you directly you should avoid this. They did not explode up and looks very nice. The bottom line is to the extent that the estimates Angrist and Krueger get were quite biased towards OLS, you would not be about to detect it necessarily with the standard error. They bottom line is, there could be a problem there, but we would not be able to know at that time. This is a clever way to show it. \par 
\subsection{L-M-M-P}
	It is interesting to show you something I am doing. Some of the motivation of this research is because of teaching in this class in the past several years. I have been telling people for many year, if you have just identified case instrument variable case, which is really common in empirical work, you should use Anderson-Rubin. And econometricians  always agree on this, Anderson-Rubin is a great method for constructing confidence intervals. But the reality is A-R rarely used! (3 surveyed AER papers). Everybody just ignores it. We take a step back and think about it, what can we do about the situation. One of the method people usually use is a method that has problem associated with it. It is not only just an inefficient method, but it does not have correct coverage e.g., the usual confidence interval method. So how can we convince people. First we asked the problem why people do not like Anderson-Rubin. One of that reasons is A-R forms a confidence interval that is not centered at 2SLS. Not the case that 2SLS plus or minus something. They like 2SLS be in the middle of the confidence interval. They also like the simple thing even computer will do it for them.\par 
	What many people do is a two-step method. In the first stage, they look at the F statistic associated with the first stage regression. That is the regression of endogenous or included endogenous variables on instruments and look at the coefficients on the excluded instruments $\hat{\pi}$'s. They form the F statistic by $F=\frac{\hat{\pi}^{2}}{\hat{V}(\hat{\pi})}$. If the F statistic is big, then we reject the null hypothesis; when it is small, we accept the null. There is a method that Stock-Yogo come up with says, we need to compare our F statistic with a threshold $\bar{F}$. If it is bigger than the threshold, we form confidence interval like you love to do. If it is small, just report an infinite interval. People do not usually report this infinite interval but that is what they are supposed to do. People usually choose the threshold $\bar{F}$ to be $10$. An when your F is greater than 10, you form your confidence interval by adding and subtracting 1.96 standard error. You will get a confidence interval that covers true $\beta$ at $.887$ percent. That does not sound too bad. It is not that far from $95$ percent, but it is also not $95$ percent. One reason people do not think this is bad is that if we do not have weak instrument, then if we add and  subtract 1.64 we get $90$ percent interval. Not that a big deal. But it turns out, if we want to use $1.96$ and get actual 95 percent coverage, what is the threshold is required. The answer is you need a $\bar{F} = 104.7$. You have to change it a lot. So we want to tell people that is not fine. It is way off fine. That our try to get attention. If you are less than $104$, you should use infinite confidence intervals. People do not like that. To get a 1 percent, you cannot even find a threshold. \par 
	Our idea is maybe people would like to adjust how much to add and subtract the standard error(what is multiplied by) smoothly depends on the F statistic they get in the first stage. That is the whole idea. Last comment is if you compare the confidence interval from Anderson-Rubin and the method I just show you, there is an interesting thing is expected length of AR confidence interval is inﬁnity but finite in this method. That is possible one favor of this new method. But Anderson-Rubin is great, this is just get people to be able to do something else.
\section{Binary Choice}
	We will now look at the case which the dependent variable only takes two values.\par 
	In principle, our linear model still work. However, there is a potential problem. Notice that OLS 2 assumption implies that $E(y_{i}|x_{i}) = x_{i}' \beta$. And since now $y_{i}$ only takes $0$ and $1$, we can compute the expectation:
	\[	E(y_{i}|x_{i}) = 1 \cdot Pr(1|x_{i}) + 0 \cdot Pr(0 | x_{i}) = Pr(1|x_{i}) .	\]
	Based on our probability function axiom, we must have 
	\[	 0  \leq x_{i}' \beta = Pr(1|x_{i}) \leq  1.	\]
	However, since the slope is constant $\beta$, we can easily contradict this. We have no prediction power for the $x_{i}$'s such that $x_{i}' \beta > 1$ or $x_{i}' \beta < 0$. We never restricted on the values of $y_{i}$ it can take, but people are skeptical about this. It has very limited use. This is called \textbf{the linear probability model}.\par 
	Now, we want to think the nonlinear probability model. What people do is actually use economics to model binary choice. It is behind the idea of \textbf{random utility formulation}. The idea is that there is going to have a random utility for each choice and each person will make a choice between $1$ and $0$. And that $y_{i}^{*}$($u_{i}^{*}$ in the slide) represents the difference between the two random utilities.  People are going to make the choice with higher utility. So if $y_{i}^{*} > 0$, people will choose $1$ and if it is less than $0$, people choose $0$. The levels of these two utility do not matter. Now we want to somehow relate this random utility formulation to the co-variants. We want to see how change in $x_{i}'$ affects the latent random utility. \par 
	In order to identify $\beta$. We want to say something about $\epsilon_{i}$. It turns out that if we make the OLS 2 assumption, that would not be enough to pin down $\beta$. What we need is a probability distribution of $\epsilon_{i}$ given $x_{i}$.
	\[ (\textbf{The Probit Assumption})	 \qquad \epsilon_{i} | x_{i} \sim N(0,1)	\]
	 Recalled that the probability density function is just the derivative of cumulative distribution. We denote the CDF of standard normal distribution as $\Phi$ and its probability density function as $\phi$.  $\phi$ is a famous one with the bell shape and $\phi = \frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}$.\par 
	 The probit assumption tells us 
	 \[	E(y_{i}|x_{i}) = Pr(y_{i} = 1 | x_{i}) = Pr(-\epsilon_{i} \leq x_{i}' \beta | x_{i}) = \Phi(x_{i}'\beta).	\]
	The first equality holds by the result before. The second equality holds by definition of $y_{i} = 1$. The third equality holds since if $\epsilon_{i} | x_{i}$ is a standard normal, then we must have $-\epsilon_{i}$ being a standard normal since the standard normal is symmetric about 0. This is a nice connection between $ Pr(y_{i} = 1 | x_{i}) $ and $\beta$ and $x_{i}$. This is the key equation in the probit model. \par 
	Me: ``We need the orthogonality condition for the first equation to be hold, do we have orthogonal conditions here?''\par 
	Jack:``There is a certain kind of orthogonality we assumed here. Where does that comes from. When we assumed $\epsilon_{i}|x_{i}$ is a standard normal, what we assumed is if you tell me some $x_{i}$, I can tell you the distribution of $\epsilon_{i}$. But you notice that nothing in the distribution depends on $x_{i}$. That means, $\epsilon_{i}$ is independent of $x_{i}$. Even if I say conditional on $x_{i}$, there are really two assumptions: $\epsilon_{i}$ is a standard normal and $\epsilon_{i}$ is conditional on $x_{i}$. That is a strong from of orthogonality. In the linear model, we have $E(x_{i}' \beta) = x_{i}' \beta \Leftrightarrow E(\epsilon_{i}|x_{i}) = 0$. In the binary choice, we have $E(y_{i}|x_{i}) = \Phi (x_{i}' \beta)$. In this case, we have $E(u_{i} | x_{i}) = 0$ where $y_{i} = \Phi(x_{i}' \beta) + u$. Notice that this $u_{i}$ is not the $\epsilon_{i}$ in the random utility formulation. And $y$ is the observable dependent binary variable. So we have orthogonality condition running around here, but it is an orthogonality about a different error term. ''\par 
	Before we go into this estimation, I want to make two comments on the parameters of the Probit model.\par 
	\begin{enumerate}
		\item \textbf{Always include a constant term.} Include a $1$ in you $x_{i}$'s. Suppose we have $x_{i} = (1 ~ w_{i})'$ and $\beta = (\beta_{0} ~ \beta_{w})$ where $w$ stands for everything else. Then we have 
		\[	E(y_{i} | x_{i}) = Pr(y_{i}=1 | x_{i}) = \Phi(-\epsilon_{i} \leq x_{i}'\beta | x_{i}) = \Phi(\beta_{0} + x_{w}' \beta_{w}).	\]
		I want to look at what happens when $w_{i} = 0$. Then we have 
		\[	Pr(y_{i} = 1 | w_{i} = 0, x_{i}) = \Phi(\beta_{0}).	\]
		This is what happens when you include the constant. What does that equals to? It could be anything, it depends on $\beta_{0}$. Our model would be flexible in this sense. But if you don't, here is what happens
		\[			Pr(y_{i} = 1 | x_{i} = 0) = \Phi(0) = 1/2.		\]
		Is that a good model?  I have no idea. It depends on your application.  It is a huge constrain. If we do not include constant, we are making this assumption that if $x_{i} = 0$, then the success probability is a half. In general, we do not want this.
		\item \textbf{The scale of $\beta$.} Remember we have 
		\[	y_{i} = \begin{cases}
		1 &\text{if $y^{*}_{i} \geq 0$} \\
		0 &\text{if $y^{*}_{i} < 0$} 
		\end{cases}	=
		\begin{cases}
		1 &\text{if $cy^{*}_{i} \geq 0$} \\
		0 &\text{if $cy^{*}_{i} < 0$} 
		\end{cases} =
		\begin{cases}
		1 &\text{if $c(x^{'}_{i}\beta + \epsilon_{i})\geq 0$} \\
		0 &\text{if $c(x^{'}_{i}\beta + \epsilon_{i}) < 0$} 
		\end{cases}=
		\begin{cases}
		1 &\text{if $x^{'}_{i}c\beta + c\epsilon_{i}\geq 0$} \\
		0 &\text{if $x^{'}_{i}c\beta + c\epsilon_{i} < 0$} 
		\end{cases}
		\]
		where $c > 0$. So you can see what I can do here. If I scale $\beta$ and $\epsilon_{i}$ up, that would not change the model. It just changes the random utility.  So this monotonic re-scaling transformation does not change the model. That tells you when we make an assumption about what distribution $\epsilon_{i}$ comes from, it is kind of like choosing a $c$. If we are requiring it form a standard normal, we are kind of requiring it not to be up and down too big. But what I say is it does not matter. That means the overall scale of $\beta$ is not that important. You should focus on the relative values of $\beta$. How each element of $\beta$ compares to each other. That is the important information. 
	\end{enumerate} 
\subsection{Marginal Effects}
	For the linear model with OLS 2 assumption, we have 
	\[	E(y_{i}|x_{i}) = x_{i}'\beta,	\]
	and therefore
	\[	\frac{\partial}{\partial x_{i}} E(y_{i}|x_{i}) = \beta_{i}.	\]
	$\beta$ has a nice interpretation model. \par 
	For the binary choice model, we have 
	\[		\frac{\partial}{\partial x_{i,j}} E(y_{i}|x_{i}) = 	\frac{\partial}{\partial x_{i,j}} Pr(y_{i}=1|x_{i}) = \phi(x_{i}'\beta) \cdot \beta_{j}	\]
	Also for another $x_{i,m}$, 
	\[		\frac{\partial}{\partial x_{i,m}} E(y_{i}|x_{i}) = 	\frac{\partial}{\partial x_{i,m}} Pr(y_{i}=1|x_{i}) = \phi(x_{i}'\beta) \cdot \beta_{m}.	\]
		$\beta$ does not have this nice interpretation, but this is kind of the point for the nonlinear model. \par 
		The blue lines are the success probability $\Phi(x_{i}'\beta) = Pr(y_{i}=1|x_{i})$. These probability changes with $x$ but not in a linear way. The marginal effects corresponds the derivative of the functions. The whole point of nonlinear model is they change with $x$ now.\par 
		But the \textbf{relative marginal effect} is more important. It is the ratio of the marginal effects for two different $x$s. We have 
		\[	\frac{	\frac{\partial}{\partial x_{i,j}} Pr(y_{i}=1|x_{i}) }{	\frac{\partial}{\partial x_{i,m}} Pr(y_{i}=1|x_{i})} = \frac{\beta_{j}}{\beta_{m}}.	\]
		That is to say, \emph{relative marginal effects are captured by the relative rate of coefficients. To change $x_{i}$'s, the marginal effect changes, but the relative marginal effect does not change}.
\subsection{Estimation $\beta$}
	There are two ways in the slides to use data to estimate $\beta$ and make confidence intervals. One approach is the \textbf{Nonlinear Least Squares} and another is \textbf{Maximum Likelihood}. The comment is Maximum Likelihood is better, more precise and more efficient. People will not use NLLS here. But let us talk about NLLS first, since it is useful in other nonlinear models, other than binary choices. We will be very fast here. I will finish it in only four minutes. It works in the exact analogy with what we seen in the linear case.\par 
	Before, we know that under orthogonal condition OLS 2, we have $E(y_{i}|x_{i}) = x_{i}'\beta$. We have $\beta$ defined as $\beta_{0} = E(x_{i}x_{i}')^{-1}E(x_{i}y_{i})$ solves the problem of ordinary least square 
	$\min E[(y_{i} - x_{i}'\beta)]$. And the least square estimator $\hat{\beta}$ solves $\min \frac{1}{n}\sum(y_{i} - x_{i}'\beta)$. \par 
	In the non-linear model(shown in the binary choice case), we have 
	\[	E(y_{i}|x_{i}) = \Phi(x_{i}'\beta).	\] 
	Same thing happens here. It is 
	\[	\beta_{NLLS} = \arg min ~ E[(y_{i}-\Phi(x_{i}'\beta))^{2}]	\]
	and
	\[	\hat{\beta}_{NLLS} = \arg min ~ \frac{1}{n}\sum_{i=1}^{n}[y_{i}-\Phi(x_{i}'\beta)]^{2}.	\]
	It can be shown that $\hat{\beta}_{NLLS}$ is consistent and has asymptotic normality property under Probit assumptions. We will not show them here. \par 
	For the maximum likelihood, if we have 
	\[	\beta_{ML} = \arg max E[y_{i}\ln \Phi(x_{i}'\beta) + (1-y_{i})\ln(1-\Phi(x_{i}'\beta))]	\] 
	Then probably our $\hat{\beta}_{ML}$ is a good estimator.\par 
	Does this $	\beta_{ML} $ actually solve this? The answer is yes. We will not show this.
\subsection{Probit Maximium Likelihood}
	Binary is a nice example of nonlinear model. All we are doing here can be carried over. I have been making a big deal is the orthogonal conditions in the models we have done so far. They are usually like $E[x_{i}|y_{i}-x_{'}\beta] = 0$. This mean is zero at the true value of $\beta$ and it will not be zero at other values of $\beta$. Essentially, we compute this mean and search for $\beta$, which gives me a value of zero. We do not have the expectation, so we replace it with an average. That is what least square does. The neat thing is we only made an assumption about the mean of a transformation of data and parameter. There are other methods people used called the \textbf{maximum likelihood}, which assumes we have a lot more information of these random variables. Their distributions, for example. This is huge difference. \par 
	Binary choice is very simple since $y_{i}$ only takes two values. That means if you know the conditional mean of $y_{i}$ on $x_{i}$, you know the conditional probability $Pr(y_{i}=1|x_{i})$ and the whole distribution. If $y_{i}$ takes three values, then knowing the mean would not be enough.\par 
	Basically, the maximum likelihood method is the probability that maximize the likelihood an outcome happens. It is the $p$ that most justifies the data you observe. Here is what we do.\par 
	Suppose we have a i.i.d. sequence of data $(y_{i}, x_{i})$ where $i = 1,2,3, \dots$. The likelihood really is the joint distribution of your data for whatever parameter you get. Now I am not going to look at the joint distribution of $y_{i}$ and $x_{i}$, I am going to look at the conditional distribution of $y_{i}$ given $x_{i}$. These conditional distributions are something we know. We have 
	\[	\begin{aligned}
			&Pr[Y_{1} = y_{i}, \dots, Y_{n}=y_{n} | X_{1}=x_{1}, \dots, X_{n}=x_{n}, \beta] \\
			&= \Pi_{i=1}^{n} Pr[Y_{i}=y_{i} | x_{i}=x_{i}, \beta] \\
			&=  \Pi_{i=1}^{n} \Phi(x_{i}'\beta)^{y_{i}} [1- \Phi(x_{i}'\beta)]^{1-y_{i}}
	\end{aligned}	\]
	where $y_{i}$ is either $1$ or $0$. The first equality holds since we have i.i.d. data. Notice that if $y_{i} = 1$, then we have $Pr[Y_{i}=y_{i} | x_{i}=x_{i}, \beta] = \Phi(x_{i}'\beta)$ and if $y_{i} = 0$, we have $Pr[Y_{i}=y_{i} | x_{i}=x_{i}, \beta] = 1- \Phi(x_{i}'\beta)$. So we used a little trick to have the second equality hold. Notice that if $y_{i} = 1$, then we have $\Phi(x_{i}'\beta)$; if $y_{i} = 0$, then we have $1- \Phi(x_{i}'\beta)$. \par 
	Now, if we have all the data, this is just a very complicated function of $\beta$.\par
	Then maximize $ \Pi_{i=1}^{n} \Phi(x_{i}'\beta)^{y_{i}} [1- \Phi(x_{i}'\beta)]^{1-y_{i}}$ w.r.t. $\beta$ gives us \textbf{the maximum likelihood estimator} or \textbf{the probit estimator} $\hat{\beta}_{ML}$. Notice that I can take a monotone transformation of the objection function and maximize that instead, so we can maximize 
	\[	\ln \Pi_{i=1}^{n} \Phi(x_{i}'\beta)^{y_{i}} [1- \Phi(x_{i}'\beta)]^{1-y_{i}}.	\]
	This gives us the same $\hat{\beta}_{ML}$. And I can also have 
	\[	\hat{\beta}_{ML} = \arg max \frac{1}{n} \sum_{i=1}^{n}[y_{i}\ln \Phi(x_{i}'\beta) + (1-y_{i})\ln(1-\Phi(x_{i}'\beta))].	\]
	Remember that we have 
	\[	\beta_{ML} = \arg max E[y_{i}\ln \Phi(x_{i}'\beta) + (1-y_{i})\ln(1-\Phi(x_{i}'\beta))],	\]
	so our $\hat{\beta}_{ML} $ is consistent and have asymptotic normality property.\par 
	Let us think about maximum likelihood in a more general setting. Notice since we have $z{i}$'s i.i.d., so the joint distribution is going to be the product of the marginal distributions. The likelihood function is just take the joint distribution function, and think about it as fixing a data set but varies $\theta$. It is now a function of $\theta$. And now maximize it w.r.t. $\theta$, or the log, or any monotone transformation. And we will have some properties familiar to you of these estimators. And we can have confidence intervals and other things.\par 
	Actually in the linear model, we could plug in $\epsilon_{i}$ as a normal distribution. Then our maximum likelihood estimator will be the OLS.
\subsection{Marginal Effects}
	I could just plug my estimation of $\beta$ here to get marginal effect. And that works. That means, it is a new estimate of a new parameter, and $g(\hat{\beta})$ is consistent if $\hat{\beta}$ is consistent. And if $\sqrt{n}(\hat{\beta}_{ML} - \beta_{0}) \stackrel{d}{\rightarrow} N(O,V_{ML})$, then $\sqrt{n}(g(\hat{\beta}_{ML}) - g(\beta_{0})) \stackrel{d}{\rightarrow} N(O,some variance)$. This works by the delta method.
\subsection{Logit}
	Logit uses instead the $logit$ distribution. There is one reason people like Logit is the log of the odds ratio is $x_{i}'\beta_{0}$. 
\end{document}